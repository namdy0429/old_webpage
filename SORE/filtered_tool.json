{
  "tf.reset_default_graph": {
    "tf.session": [
      {
        "title": "49943920",
        "h": "tf.reset_default_graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9408",
        "sentences": [
          "replace tf.interactivesession () with with tf.session (): statement.",
          "tf.reset_default_graph () won ' t free those resources while the session is active."
        ]
      },
      {
        "title": "53919276",
        "h": "tf.reset_default_graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9311999999999999",
        "sentences": [
          "tf.reset_default_graph will clear the default graph stack and resets the global default graph.",
          "this function applies only to the current thread.",
          "calling this function while a tf.session or tf.interactivesession is active will result in undefined behavior."
        ]
      },
      {
        "title": "44897259",
        "h": "tf.reset_default_graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9119999999999999",
        "sentences": [
          "you get an error because you use it in a session.",
          "from the tf.reset_default_graph () documentation : ",
          "calling this function while a tf.session or tf.interactivesession is active will result in undefined behavior.",
          "tf.reset_default_graph () can be helpful ( at least for me ) during the testing phase while i experiment in jupyter notebook."
        ]
      },
      {
        "title": "46894945",
        "h": "tf.reset_default_graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9114",
        "sentences": [
          "this error message is displayed when you call tf.reset_default_graph () in one of the following scenarios : ",
          "inside a with graph.as_default (): block .. ",
          "inside a with tf.session (): block .. ",
          "resetting the default graph in those scenarios would leave the system in an inconsistent state , so you should ensure to exit the block ( or close the tf.interactivesession ) before calling tf.reset_default_graph (). "
        ]
      }
    ]
  },
  "tf.nn.sigmoid_cross_entropy_with_logits": {
    "tf.nn.softmax_cross_entropy_with_logits": [
      {
        "title": "45459557",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.nn.sigmoid_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "i think that you should use tf.nn.sigmoid_cross_entropy_with_logits instead of tf.nn.softmax_cross_entropy_with_logits because you use sigmoid and 1 neuron in output layer."
        ]
      },
      {
        "title": "42229043",
        "h": "tf.nn.sigmoid_cross_entropy_with_logits",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9603999999999999",
        "sentences": [
          "in normal tensorflow multiclass classification ( classic mnist ) you will have 10 output units and you will use softmax at the end for computing losses i.e.",
          "\" tf.nn.softmax_cross_entropy_with_logits \". ",
          "\" tf.nn.sigmoid_cross_entropy_with_logits \" ",
          "ex : if your image has \" 2 \" & \" 4 \", then groundtruth will be [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ], i.e."
        ]
      },
      {
        "title": "39520251",
        "h": "tf.nn.sigmoid_cross_entropy_with_logits",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "if you have multiple correct labels , [ 1 0 1 0 ... 1 ] looks totally fine.",
          "the loss function used in denny ' s post is tf.nn.softmax_cross_entropy_with_logits , which is the loss function for a multi - class problem.",
          "in multi - label problem , you should use tf.nn.sigmoid_cross_entropy_with_logits : ",
          "computes sigmoid cross entropy given logits.",
          "the input to the loss function would be logits ( wx ) and targets ( labels ). "
        ]
      },
      {
        "title": "39783314",
        "h": "tf.nn.sigmoid_cross_entropy_with_logits",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.9207000000000001",
        "sentences": [
          "here is a comment in the relevant source code , about the function that implements tf.nn.softmax_cross_entropy_with_logits : ",
          "if the forced softmax hinders your computation , perhaps another api could help : tf.nn.sigmoid_cross_entropy_with_logits or maybe tf.nn.weighted_cross_entropy_with_logits."
        ]
      }
    ]
  },
  "tf.nn.softmax_cross_entropy_with_logits": {
    "tf.nn.sparse_softmax_cross_entropy_with_logits": [
      {
        "title": "36088396",
        "h": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "indices of columns in the predictions matrix ). ",
          "if your problem is a single - class problem , then i assume that your y_ tensor is a one - hot encoding of the true labels for your examples ( for example because you also pass them to an op like tf.nn.softmax_cross_entropy_with_logits (). ",
          "( also , consider using tf.nn.sparse_softmax_cross_entropy_with_logits () as your loss function , because it may be more efficient.",
          "). "
        ]
      },
      {
        "title": "39831223",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9503999999999999",
        "sentences": [
          "use tf.nn.sparse_softmax_cross_entropy_with_logits instead of tf.nn.softmax_cross_entropy_with_logits."
        ]
      },
      {
        "title": "64213624",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.9306",
        "sentences": [
          "actually tf.nn.softmax_cross_entropy_with_logits does not impose the restriction that labels must be on - hot encoded , so you can go ahead and use non - one - hot label vectors.",
          "you might be confusing this with tf.nn.sparse_softmax_cross_entropy_with_logits which does impose this restriction."
        ]
      },
      {
        "title": "60262947",
        "h": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "0.98",
        "prob": "0.9219839999999999",
        "sentences": [
          "hence , specifying the 2.0 compatible calls for all the functions , we discussed above , if we migrate from 1.x to 2.x , for the benefit of the community.",
          "functions in 1.x : ",
          "tf.nn.softmax.",
          "tf.nn.softmax_cross_entropy_with_logits.",
          "tf.nn.sparse_softmax_cross_entropy_with_logits.",
          "for more information about migration from 1.x to 2.x , please refer this migration guide."
        ]
      },
      {
        "title": "38101834",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.8648",
        "sentences": [
          "i would just like to add 2 things to accepted answer that you can also find in tf documentation.",
          "first : ",
          "tf.nn.softmax_cross_entropy_with_logits ",
          "if they are not , the computation of the gradient will be incorrect.",
          "second : ",
          "tf.nn.sparse_softmax_cross_entropy_with_logits "
        ]
      },
      {
        "title": "34243720",
        "h": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.97",
        "r_prob": "0.96",
        "prob": "0.8008319999999999",
        "sentences": [
          "in contrast , tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function ( but it does it all together in a more mathematically careful way ). ",
          "the output of tf.nn.softmax_cross_entropy_with_logits on a shape [ 2 , 5 ] tensor is of shape [ 2 , 1 ] ( the first dimension is treated as the batch ). "
        ]
      },
      {
        "title": "47145069",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.7742",
        "sentences": [
          "thanks to maosi chen , i found the issue.",
          "it was because the ",
          "tf.nn.sparse_softmax_cross_entropy_with_logits ",
          "requires labels to have one less dimension than logits.",
          "however , it was also possible to use ",
          "tf.nn.softmax_cross_entropy_with_logits "
        ]
      }
    ],
    "tf.nn.sigmoid_cross_entropy_with_logits": [
      {
        "title": "45459557",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.nn.sigmoid_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "i think that you should use tf.nn.sigmoid_cross_entropy_with_logits instead of tf.nn.softmax_cross_entropy_with_logits because you use sigmoid and 1 neuron in output layer."
        ]
      },
      {
        "title": "42229043",
        "h": "tf.nn.sigmoid_cross_entropy_with_logits",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9603999999999999",
        "sentences": [
          "in normal tensorflow multiclass classification ( classic mnist ) you will have 10 output units and you will use softmax at the end for computing losses i.e.",
          "\" tf.nn.softmax_cross_entropy_with_logits \". ",
          "\" tf.nn.sigmoid_cross_entropy_with_logits \" ",
          "ex : if your image has \" 2 \" & \" 4 \", then groundtruth will be [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 ], i.e."
        ]
      },
      {
        "title": "39520251",
        "h": "tf.nn.sigmoid_cross_entropy_with_logits",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "if you have multiple correct labels , [ 1 0 1 0 ... 1 ] looks totally fine.",
          "the loss function used in denny ' s post is tf.nn.softmax_cross_entropy_with_logits , which is the loss function for a multi - class problem.",
          "in multi - label problem , you should use tf.nn.sigmoid_cross_entropy_with_logits : ",
          "computes sigmoid cross entropy given logits.",
          "the input to the loss function would be logits ( wx ) and targets ( labels ). "
        ]
      },
      {
        "title": "39783314",
        "h": "tf.nn.sigmoid_cross_entropy_with_logits",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.9207000000000001",
        "sentences": [
          "here is a comment in the relevant source code , about the function that implements tf.nn.softmax_cross_entropy_with_logits : ",
          "if the forced softmax hinders your computation , perhaps another api could help : tf.nn.sigmoid_cross_entropy_with_logits or maybe tf.nn.weighted_cross_entropy_with_logits."
        ]
      }
    ]
  },
  "tf.read_file": {
    "tf.wholefilereader": [
      {
        "title": "53020118",
        "h": "tf.read_file",
        "t": "tf.wholefilereader",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.54",
        "r_prob": "0.99",
        "prob": "0.49183200000000005",
        "sentences": [
          "i found the solution by replacing the tf.wholefilereader () with tf.read_file (): "
        ]
      },
      {
        "title": "34345827",
        "h": "tf.wholefilereader",
        "t": "tf.read_file",
        "r": "S1",
        "h_prob": "0.37",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.3293",
        "sentences": [
          "use tf.read_file ( filename ) rather than tf.wholefilereader () to read your image files.",
          "tf.read_file () is a stateless op that consumes a single filename and produces a single string containing the contents of the file."
        ]
      },
      {
        "title": "42987992",
        "h": "tf.wholefilereader",
        "t": "tf.read_file",
        "r": "S1",
        "h_prob": "0.39",
        "t_prob": "0.84",
        "r_prob": "1.0",
        "prob": "0.3276",
        "sentences": [
          "since you are using tf.wholefilereader , you may be able to avoid the problem of synchronizing multiple queues by replacing it with the much simpler tf.read_file () op , as follows : "
        ]
      }
    ],
    "tf.decode_csv": [
      {
        "title": "35530400",
        "h": "tf.read_file",
        "t": "tf.decode_csv",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.38",
        "r_prob": "1.0",
        "prob": "0.3496",
        "sentences": [
          "the tf.read_file () op reads the entire contents of the given file into a single string , whereas tf.decode_csv () op expects each element of its input to be a single record ( i.e.",
          "one line ). "
        ]
      },
      {
        "title": "34401816",
        "h": "tf.decode_csv",
        "t": "tf.read_file",
        "r": "S1",
        "h_prob": "0.23",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.20700000000000002",
        "sentences": [
          "we ' ll use tf.decode_csv () to parse the text , tf.read_file () to load the jpeg data as a string , tf.image.decode_jpeg () to parse it into a dense tensor , and finally tf.train.batch () to build the parsed data into a batch of images."
        ]
      },
      {
        "title": "47373052",
        "h": "tf.read_file",
        "t": "tf.decode_csv",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.19",
        "r_prob": "1.0",
        "prob": "0.1881",
        "sentences": [
          "tf.read_file () needs a scalar input ( i.e ., just one string ), but the results of tf.decode_csv are coming back in a \" rank 1 \" context , i.e ., a 1 - d list."
        ]
      }
    ]
  },
  "convolutional neural networks": {
    "recurrent neural networks": [
      {
        "title": "55978869",
        "h": "convolutional neural networks",
        "t": "recurrent neural networks",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.46",
        "r_prob": "1.0",
        "prob": "0.4232",
        "sentences": [
          "for convolutional neural networks ( cnn ): ",
          "you can use them if your input is fixed length and has significant features.",
          "for recurrent neural networks : "
        ]
      },
      {
        "title": "45147778",
        "h": "convolutional neural networks",
        "t": "recurrent neural networks",
        "r": "S1",
        "h_prob": "0.65",
        "t_prob": "0.44",
        "r_prob": "1.0",
        "prob": "0.28600000000000003",
        "sentences": [
          "on other tasks , convolutional neural networks with pooling layers or recurrent neural networks can be helpful."
        ]
      },
      {
        "title": "45980285",
        "h": "convolutional neural networks",
        "t": "recurrent neural networks",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.25",
        "r_prob": "1.0",
        "prob": "0.2025",
        "sentences": [
          "to encode sequence to a fixed length vector you typically use recurrent neural networks ( rnns ) or convolutional neural networks ( cnns ). "
        ]
      }
    ]
  },
  "tf.expand_dims": {
    "tf.squeeze": [
      {
        "title": "35164907",
        "h": "tf.expand_dims",
        "t": "tf.squeeze",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.85",
        "r_prob": "1.0",
        "prob": "0.833",
        "sentences": [
          "there is no ' x ' or missing dimension ) you can use tf.transpose () to implement dimshuffle (). ",
          "tf.expand_dims () is used to add one or more size - 1 dimensions to a tensor.",
          "this handles the case where ' x ' is specified as part of the dimshuffle () pattern , but does not reorder the existing dimensions.",
          "tf.squeeze () is used to remove one or more size - 1 dimensions from a tensor.",
          "assuming that the input is a vector , your example ( dimshuffle ( 0 , ' x ')) can be expressed using tf.expand_dims () only : "
        ]
      },
      {
        "title": "35564506",
        "h": "tf.expand_dims",
        "t": "tf.squeeze",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.83",
        "r_prob": "1.0",
        "prob": "0.8217",
        "sentences": [
          "probably the most idiomatic way to do this is using tf.batch_matmul () operator ( in conjunction with tf.expand_dims () and tf.squeeze (): "
        ]
      },
      {
        "title": "55981692",
        "h": "tf.squeeze",
        "t": "tf.expand_dims",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.8190000000000001",
        "sentences": [
          "on gpu , tf.newaxis and tf.squeeze () are the fastest ones : ",
          "in tf2.0 tf.expand_dims () to add dimension and tf.squeeze () are the fastest ( cpu ): "
        ]
      }
    ]
  },
  "tf.squeeze": {
    "tf.expand_dims": [
      {
        "title": "35164907",
        "h": "tf.expand_dims",
        "t": "tf.squeeze",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.85",
        "r_prob": "1.0",
        "prob": "0.833",
        "sentences": [
          "there is no ' x ' or missing dimension ) you can use tf.transpose () to implement dimshuffle (). ",
          "tf.expand_dims () is used to add one or more size - 1 dimensions to a tensor.",
          "this handles the case where ' x ' is specified as part of the dimshuffle () pattern , but does not reorder the existing dimensions.",
          "tf.squeeze () is used to remove one or more size - 1 dimensions from a tensor.",
          "assuming that the input is a vector , your example ( dimshuffle ( 0 , ' x ')) can be expressed using tf.expand_dims () only : "
        ]
      },
      {
        "title": "35564506",
        "h": "tf.expand_dims",
        "t": "tf.squeeze",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.83",
        "r_prob": "1.0",
        "prob": "0.8217",
        "sentences": [
          "probably the most idiomatic way to do this is using tf.batch_matmul () operator ( in conjunction with tf.expand_dims () and tf.squeeze (): "
        ]
      },
      {
        "title": "55981692",
        "h": "tf.squeeze",
        "t": "tf.expand_dims",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.8190000000000001",
        "sentences": [
          "on gpu , tf.newaxis and tf.squeeze () are the fastest ones : ",
          "in tf2.0 tf.expand_dims () to add dimension and tf.squeeze () are the fastest ( cpu ): "
        ]
      }
    ]
  },
  "tf.import_graph_def": {
    "tf.train.import_meta_graph": [
      {
        "title": "50375518",
        "h": "tf.train.import_meta_graph",
        "t": "tf.import_graph_def",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "you can achieve that by serializing your graph and reimport it using tf.import_graph_def , which has an input_map argument used to plug - in inputs at the desired places.",
          "for example , if you load your graph from a.meta file , you can use tf.train.import_meta_graph which accepts the same input_map argument."
        ]
      },
      {
        "title": "50229276",
        "h": "tf.import_graph_def",
        "t": "tf.train.import_meta_graph",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.8439",
        "sentences": [
          "the tool import_pb_to_tensorboard.py uses tf.import_graph_def to import the graph and uses default name argument , which is \" import \" as documented.",
          "your code imports the graph through tf.train.import_meta_graph and uses default import_scope argument , which will not prefix imported tensor or operation name."
        ]
      },
      {
        "title": "55147060",
        "h": "tf.train.import_meta_graph",
        "t": "tf.import_graph_def",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.8148",
        "sentences": [
          "if you want to store a graph along with its metadata ( including the collections ), you should save a metagraphdef instead ( see tf.train.export_meta_graph / tf.train.import_meta_graph ). ",
          "in your v2 code , graph is none because tf.import_graph_def does not return anything , it just imports the nodes in the given graph definition into the current default graph."
        ]
      }
    ]
  },
  "tf.train.import_meta_graph": {
    "tf.import_graph_def": [
      {
        "title": "50375518",
        "h": "tf.train.import_meta_graph",
        "t": "tf.import_graph_def",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "you can achieve that by serializing your graph and reimport it using tf.import_graph_def , which has an input_map argument used to plug - in inputs at the desired places.",
          "for example , if you load your graph from a.meta file , you can use tf.train.import_meta_graph which accepts the same input_map argument."
        ]
      },
      {
        "title": "50229276",
        "h": "tf.import_graph_def",
        "t": "tf.train.import_meta_graph",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.8439",
        "sentences": [
          "the tool import_pb_to_tensorboard.py uses tf.import_graph_def to import the graph and uses default name argument , which is \" import \" as documented.",
          "your code imports the graph through tf.train.import_meta_graph and uses default import_scope argument , which will not prefix imported tensor or operation name."
        ]
      },
      {
        "title": "55147060",
        "h": "tf.train.import_meta_graph",
        "t": "tf.import_graph_def",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.8148",
        "sentences": [
          "if you want to store a graph along with its metadata ( including the collections ), you should save a metagraphdef instead ( see tf.train.export_meta_graph / tf.train.import_meta_graph ). ",
          "in your v2 code , graph is none because tf.import_graph_def does not return anything , it just imports the nodes in the given graph definition into the current default graph."
        ]
      }
    ]
  },
  "batch size": {
    "training": [
      {
        "title": "51810785",
        "h": "batch size",
        "t": "training",
        "r": "S2",
        "h_prob": "0.88",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.8184",
        "sentences": [
          "you need to have a larger batch size.",
          "see tradeoff batch size vs.number of iterations to train a neural network to get a better idea on choosing a more accurate value."
        ]
      },
      {
        "title": "50554516",
        "h": "batch size",
        "t": "training",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.782",
        "sentences": [
          "you set your batch size as 1 in your tensorflow pipeline during training but feeding in 500 batch size in your testing data.",
          "you can either set your training batch size 500 or testing batch size as 1."
        ]
      }
    ],
    "learning rate": [
      {
        "title": "48893698",
        "h": "learning rate",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.91",
        "r_prob": "1.0",
        "prob": "0.7735",
        "sentences": [
          "this should represent the dimension along which to compute the cosine distance and i think that the 0th dimension would be the wrong dimension ( as this likely should be the batch size.",
          "if the learning rate is too small then you may not actually be able to move enough in the right direction."
        ]
      },
      {
        "title": "43949154",
        "h": "learning rate",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.7387",
        "sentences": [
          "this happens a lot in practice when your learning rate is too high , i tend to start at 0.001 and move from there , 0.1 is on the very high side on most datasets , especially if you aren ' t dividing your loss by your batch size."
        ]
      },
      {
        "title": "63493234",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.92",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.7360000000000001",
        "sentences": [
          "if that doesn ' t help , i would try to use 1 as batch size , to see if at least within first runs there is a change.",
          "as well the learning rate might be a problem , try to play with its value and see if the accuracy changes."
        ]
      },
      {
        "title": "54708235",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.92",
        "t_prob": "0.74",
        "r_prob": "1.0",
        "prob": "0.6808000000000001",
        "sentences": [
          "use sigmoid activation instead of softmax.",
          "set a lower learning rate in adam ; to do that you need to create the optimizer separately like adam = adam ( 0.0001 ) and pass it in model.compile (..., optimizer = adam ). "
        ]
      },
      {
        "title": "41488871",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.92",
        "t_prob": "0.74",
        "r_prob": "0.99",
        "prob": "0.673992",
        "sentences": [
          "batch size , as it will largely affect the training time of future experiments .. ",
          "learning rate and batch size.",
          "learning rate and number of neurons.",
          "( source ). ",
          "for the learning rate with adam and rmsprop , i found values around 0.001 to be optimal for most problems."
        ]
      },
      {
        "title": "48753734",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.79",
        "t_prob": "0.86",
        "r_prob": "0.98",
        "prob": "0.665812",
        "sentences": [
          "the none dimension allows you to vary the batch size during training , instead of being limited to a size number such as 128."
        ]
      },
      {
        "title": "48433743",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.79",
        "t_prob": "0.89",
        "r_prob": "0.88",
        "prob": "0.6187280000000001",
        "sentences": [
          "lower your learning rate after validation error plateaus , this typically improves performance some each time you lower it."
        ]
      },
      {
        "title": "56916577",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.87",
        "t_prob": "0.71",
        "r_prob": "1.0",
        "prob": "0.6176999999999999",
        "sentences": [
          "lowered the learning rate and increased my batch size from 32 to 128 .. "
        ]
      },
      {
        "title": "64023626",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.89",
        "t_prob": "0.69",
        "r_prob": "1.0",
        "prob": "0.6141",
        "sentences": [
          "the learning rate was either too small ( if the error curve is flat or decreasing very slowly ) or too large ( if it oscillates around this point ) "
        ]
      },
      {
        "title": "49053478",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.86",
        "t_prob": "0.7",
        "r_prob": "1.0",
        "prob": "0.602",
        "sentences": [
          "in order to tackle this problem , you can reduce the learning rate."
        ]
      },
      {
        "title": "52813871",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.97",
        "t_prob": "0.62",
        "r_prob": "1.0",
        "prob": "0.6013999999999999",
        "sentences": [
          "if you use a constant learning rate : ( 1 ) with large value - then it will less probable to converge ; ( 2 ) with small value - it may take a long time to converge .. "
        ]
      },
      {
        "title": "42696199",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.79",
        "r_prob": "0.85",
        "prob": "0.56406",
        "sentences": [
          "maybe you are not using a high enough batch size.",
          "so each neurons \" part \" in the error doubles.",
          "by using the same learning rate the gradient updates double.",
          "therefore you have the same problem as if you had used a larger learning rate in the first place.",
          "by lowering the learning rate the updates are again in the range , which you had previously used."
        ]
      },
      {
        "title": "63625455",
        "h": "learning rate",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.61",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.5612",
        "sentences": [
          "increase the batch size.",
          "decrease the learning rate , you should use scheduler .. "
        ]
      },
      {
        "title": "48867709",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.63",
        "r_prob": "1.0",
        "prob": "0.5355",
        "sentences": [
          "this paper researches the relation of batch size and learning rate.",
          "instead of decaying the learning rate , they increase the batch size by the same factor.",
          "in short , if you use a bigger batch size , you can use a larger learning rate to reduce training time."
        ]
      },
      {
        "title": "49271865",
        "h": "learning rate",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.6",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.534",
        "sentences": [
          "hence , when you decide to change the mini - batch size , you can expect the same learning rate as before to still work well."
        ]
      },
      {
        "title": "49924566",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.82",
        "t_prob": "0.63",
        "r_prob": "1.0",
        "prob": "0.5166",
        "sentences": [
          "traditionally , the steps per epoch is calculated as train_length // batch_size , since this will use all of the data points , one batch size worth at a time.",
          ". "
        ]
      },
      {
        "title": "45668368",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.89",
        "t_prob": "0.58",
        "r_prob": "0.99",
        "prob": "0.511038",
        "sentences": [
          "this includes the optimizer ( which should include the learning rate and the batch size ). "
        ]
      },
      {
        "title": "62588661",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.73",
        "r_prob": "0.82",
        "prob": "0.5088099999999999",
        "sentences": [
          "b ) try to use learning rate scheduler after some epochs change the learning rate using scheduler it will decrease the loss and increase accuracy.",
          "d ) try to use different batch size ",
          "increase your dataset."
        ]
      },
      {
        "title": "40102907",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.8",
        "t_prob": "0.6",
        "r_prob": "1.0",
        "prob": "0.48",
        "sentences": [
          "try picking a smaller learning rate to reduce the amount of change per step."
        ]
      },
      {
        "title": "43805871",
        "h": "learning rate",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.55",
        "t_prob": "0.96",
        "r_prob": "0.87",
        "prob": "0.45936000000000005",
        "sentences": [
          "it seems like i have used too large learning rate.",
          "batch size - 64."
        ]
      }
    ],
    "training set": [
      {
        "title": "61032181",
        "h": "batch size",
        "t": "training set",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.85",
        "r_prob": "0.74",
        "prob": "0.5283599999999999",
        "sentences": [
          "second with a batch size of 50 , 000 all 50 , 000 samples reside in memory.if you are working with images for example that would take up an enormous amount of memory and probably lead to a resource exhaust error.",
          "divide your training set into groups of 1000 samples , so you will have 50 groups ( batches ). ",
          "so it is best to pick a moderate batch size."
        ]
      },
      {
        "title": "57255064",
        "h": "batch size",
        "t": "training set",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.88",
        "r_prob": "0.71",
        "prob": "0.5185839999999999",
        "sentences": [
          "for the training set , you use the number of samples of the training set and divide it for the training batch size , and for the validation set , you divide the number of samples in the validation set with the validation batch size."
        ]
      },
      {
        "title": "58356960",
        "h": "batch size",
        "t": "training set",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.78",
        "r_prob": "0.54",
        "prob": "0.35802000000000006",
        "sentences": [
          "you can edit the config file to change the values for batch size and number of steps.",
          "number of epochs trained = ( number of images in training set / batch size )* num_steps "
        ]
      }
    ],
    "deep learning": [
      {
        "title": "50784097",
        "h": "batch size",
        "t": "deep learning",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.92",
        "r_prob": "0.99",
        "prob": "0.755964",
        "sentences": [
          "batch size , in general , represents the size of the mini - batches constructed from the experimental dataset."
        ]
      },
      {
        "title": "47240662",
        "h": "batch size",
        "t": "deep learning",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.66",
        "r_prob": "0.99",
        "prob": "0.5292540000000001",
        "sentences": [
          "i have a deep learning fundamentals youtube playlist that you may find helpful.",
          "additionally , this deep learning with keras playlist may also be beneficial if you ' re wanting to focus more on coding after getting the basic concepts down."
        ]
      },
      {
        "title": "43429424",
        "h": "batch size",
        "t": "deep learning",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.9",
        "r_prob": "0.6",
        "prob": "0.4914",
        "sentences": [
          "2 ) increase your batch size , at least as same as classes number , "
        ]
      }
    ],
    "loss function": [
      {
        "title": "51032558",
        "h": "batch size",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.89",
        "t_prob": "0.84",
        "r_prob": "1.0",
        "prob": "0.7475999999999999",
        "sentences": [
          "i have simply put your loss function as it is.",
          "n is the batch size."
        ]
      },
      {
        "title": "57515658",
        "h": "batch size",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.92",
        "r_prob": "0.95",
        "prob": "0.55062",
        "sentences": [
          "similar to martino ' s answer , but will infer shape from input ( setting it to a fixed batch size did not work for me ). "
        ]
      },
      {
        "title": "48893698",
        "h": "batch size",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.72",
        "r_prob": "0.74",
        "prob": "0.484848",
        "sentences": [
          "if you really do have a reason for trying to generate a model that creates tries to match word2vec , then looking at your loss function here are a few suggestions.",
          "this should represent the dimension along which to compute the cosine distance and i think that the 0th dimension would be the wrong dimension ( as this likely should be the batch size."
        ]
      },
      {
        "title": "62585482",
        "h": "batch size",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.58",
        "t_prob": "0.88",
        "r_prob": "0.58",
        "prob": "0.29603199999999996",
        "sentences": [
          "the batch size has two purposes ( that i know of , there could be more ): "
        ]
      }
    ],
    "gpus": [
      {
        "title": "57694601",
        "h": "batch size",
        "t": "gpus",
        "r": "S2",
        "h_prob": "0.92",
        "t_prob": "0.73",
        "r_prob": "1.0",
        "prob": "0.6716",
        "sentences": [
          "as far as i can ( heuristically ) tell , when doing distributed training / evaluation , the number of elements in the dataset must be evenly divisible by the batch size and number of gpus."
        ]
      },
      {
        "title": "41763164",
        "h": "batch size",
        "t": "gpus",
        "r": "S2",
        "h_prob": "0.9",
        "t_prob": "0.51",
        "r_prob": "1.0",
        "prob": "0.459",
        "sentences": [
          "generally deep learning algorithms are ran on gpus which has limited memory and thus a limited number of input data samples ( in the algorithm commonly defined as batch size ) could be loaded at a time.",
          "another probable benefit of large batch size is : in multi - class classification problems , if the number of classes are large , a larger batch size makes algorithm generalize better ( technically avoids over - fitting ) over the different classes ( while doing this a standard technique is to keep uniform distribution of classes in a batch ). "
        ]
      },
      {
        "title": "51343044",
        "h": "batch size",
        "t": "gpus",
        "r": "S2",
        "h_prob": "0.82",
        "t_prob": "0.59",
        "r_prob": "0.73",
        "prob": "0.35317399999999993",
        "sentences": [
          "now , as your dataset got smaller , your relative batch size can be larger.",
          "running models explicitly on multiple gpus will require you to set your algorithm in that fashion."
        ]
      },
      {
        "title": "34278070",
        "h": "gpus",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.62",
        "t_prob": "0.78",
        "r_prob": "0.66",
        "prob": "0.319176",
        "sentences": [
          "increase the batch size by a factor of n , where n is the number of gpus."
        ]
      },
      {
        "title": "63637806",
        "h": "batch size",
        "t": "gpus",
        "r": "S2",
        "h_prob": "0.73",
        "t_prob": "0.29",
        "r_prob": "1.0",
        "prob": "0.21169999999999997",
        "sentences": [
          "if you still want to train f - rcnn with batch size > 1 , you can do so with multiple gpus."
        ]
      }
    ],
    "broadcasting": [
      {
        "title": "34220963",
        "h": "batch size",
        "t": "broadcasting",
        "r": "S2",
        "h_prob": "0.94",
        "t_prob": "0.71",
        "r_prob": "1.0",
        "prob": "0.6673999999999999",
        "sentences": [
          "the tf.mul () operator supports numpy - style broadcasting , which would allow you to simplify and optimize the code slightly.",
          "(* this would work equally if output_img were a 4 - d b x m x n x c ( for any number of channels c ) or 3 - d m x n x c tensor , due to the way broadcasting works .) "
        ]
      },
      {
        "title": "60531920",
        "h": "batch size",
        "t": "broadcasting",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.8",
        "r_prob": "0.93",
        "prob": "0.6324000000000001",
        "sentences": [
          "notice that it ' s very important that you understand where your batch size is , and that a layer cannot have weights with sizes based on the batch size ( unless you define your inputs with batch_shape or batch_input_shape instead of shape -- this will force you to use a fixed batch size in the model ). ",
          "because of this , we will not use diagonals , but a broadcasting trick with elementwise multiplication : "
        ]
      },
      {
        "title": "48082967",
        "h": "batch size",
        "t": "broadcasting",
        "r": "S2",
        "h_prob": "0.9",
        "t_prob": "0.95",
        "r_prob": "0.68",
        "prob": "0.5814",
        "sentences": [
          "this is an invalid multiplication , since the dimensions don ' t agree and broadcasting can ' t be applied."
        ]
      }
    ],
    "convergence": [
      {
        "title": "61556937",
        "h": "batch size",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.72",
        "r_prob": "1.0",
        "prob": "0.6552",
        "sentences": [
          "change your batch size.",
          "generally the idea is the bigger the faster the convergence."
        ]
      },
      {
        "title": "43768367",
        "h": "batch size",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.73",
        "r_prob": "0.99",
        "prob": "0.6142949999999999",
        "sentences": [
          "when you have a large batch size , you can have a better estimate of the gradients and vice - versa for small batch size.",
          "as far as i know , there is no fool - prof way of knowing the optimal batch size.",
          "regarding speed , my guess is that gpu is always going to win even if the batch size 20 times smaller.",
          "if you observe that batch size is affecting your validation accuracy and convergence , then you may think about shifting onto cpu."
        ]
      },
      {
        "title": "61304937",
        "h": "batch size",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.82",
        "t_prob": "0.66",
        "r_prob": "1.0",
        "prob": "0.5412",
        "sentences": [
          "yes , if you can go for the as large batch size as you can.",
          "high batch size almost always results in faster convergence , short training time."
        ]
      },
      {
        "title": "40102907",
        "h": "batch size",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.8",
        "t_prob": "0.58",
        "r_prob": "0.9",
        "prob": "0.41759999999999997",
        "sentences": [
          "consequently you may also need to increase the number of steps by some factor , depending on your model and data , to get to the same convergence."
        ]
      }
    ],
    "batch normalization": [
      {
        "title": "50241169",
        "h": "batch normalization",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.7",
        "t_prob": "0.9",
        "r_prob": "0.99",
        "prob": "0.6237",
        "sentences": [
          "increase batch size ",
          "batch normalization between layers."
        ]
      },
      {
        "title": "61476847",
        "h": "batch size",
        "t": "batch normalization",
        "r": "S2",
        "h_prob": "0.87",
        "t_prob": "0.69",
        "r_prob": "1.0",
        "prob": "0.6003",
        "sentences": [
          "the first things that pop into mind are early stopping callbacks and change the batch size."
        ]
      },
      {
        "title": "57489798",
        "h": "batch normalization",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.6",
        "t_prob": "0.87",
        "r_prob": "1.0",
        "prob": "0.522",
        "sentences": [
          "specifically , \" the effect of batch normalization is dependent on the mini - batch size and it is not obvious how to apply it to recurrent networks \" ( from the paper ba , et al."
        ]
      },
      {
        "title": "48034744",
        "h": "batch normalization",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.58",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.4698",
        "sentences": [
          "large batch size can break validation performance when batch normalization is used.",
          "see this."
        ]
      },
      {
        "title": "63020188",
        "h": "batch size",
        "t": "batch normalization",
        "r": "S2",
        "h_prob": "0.68",
        "t_prob": "0.6",
        "r_prob": "0.97",
        "prob": "0.39576",
        "sentences": [
          "indeed , using a very large batch size can harm generalization as there is less variation in batch statistics , decreasing regularization."
        ]
      }
    ],
    "gradient descent": [
      {
        "title": "38840883",
        "h": "batch size",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.82",
        "t_prob": "0.68",
        "r_prob": "1.0",
        "prob": "0.5576",
        "sentences": [
          "there is a batch size , because it uses mini - batch gradient descent."
        ]
      },
      {
        "title": "38047478",
        "h": "batch size",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.72",
        "r_prob": "0.91",
        "prob": "0.550368",
        "sentences": [
          "switching from gradient descent to stochastic gradient descent you need to keep some things in mind.",
          "it may be worth looking further into differences between gradient descent and stochastic gradient descent."
        ]
      },
      {
        "title": "44471919",
        "h": "gradient descent",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.59",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.48379999999999995",
        "sentences": [
          "it is just a way to train models faster ( mini - batch gradient descent ) "
        ]
      },
      {
        "title": "44543550",
        "h": "batch size",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.74",
        "r_prob": "0.6",
        "prob": "0.37295999999999996",
        "sentences": [
          "usually , this loop is based on the gradient descent algorithm.",
          "there are many variations on gradient descent ( batch , stochastic , mini - batch ) as well as other algorithms for optimizing the learning parameters ( e.g ., l - bfgs ). "
        ]
      }
    ]
  },
  "tf.estimator.estimator": {
    "tf.contrib.learn.estimator": [
      {
        "title": "47254759",
        "h": "tf.contrib.learn.estimator",
        "t": "tf.estimator.estimator",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "afaik , you cannot incorporate l1 or l2 regularizations on provided estimators , like the subclasses of tf.estimator.estimator and tf.contrib.learn.estimator.",
          "nonetheless , you could create customized estimators by using tf.layers api , explained here : https :// www.tensorflow.org / extend / estimators."
        ]
      },
      {
        "title": "46230705",
        "h": "tf.contrib.learn.estimator",
        "t": "tf.estimator.estimator",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.9507959999999999",
        "sentences": [
          "evaluating every n steps , early stopping using a metric , etc .) ",
          "one more thing — to use this hook you ' ll need to update from a tf.contrib.learn.estimator ( which only accepts monitors ) to the more full - fledged and official tf.estimator.estimator ( which only accepts hooks ). "
        ]
      },
      {
        "title": "47249767",
        "h": "tf.estimator.estimator",
        "t": "tf.contrib.learn.estimator",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9408",
        "sentences": [
          "first up , you should stop using tf.contrib.learn.estimator in favor of tf.estimator.estimator , because contrib is an experimental module , and classes that have graduated to the core api ( such es estimator ) automatically get deprecated."
        ]
      },
      {
        "title": "45186929",
        "h": "tf.contrib.learn.estimator",
        "t": "tf.estimator.estimator",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.8832",
        "sentences": [
          "it seems that tf.estimator.estimator together with a model function that returns tf.estimator.estimatorspec is the most current one that is used in the newer examples and the one to be used in new code.",
          "my guess now is that the tf.contrib.learn.estimator is an early prototype that got replaced by the tf.estimator.estimator."
        ]
      }
    ]
  },
  "tf.all_variables": {
    "tf.train.saver": [
      {
        "title": "39134460",
        "h": "tf.all_variables",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "instead it provides a way to associate a graphdef with the weights stored in one or more checkpoint files , written by a tf.train.saver.",
          "in you training program , write out a checkpoint using a tf.train.saver.",
          "if you want to get the value of each variable , you can ( for example ) find the variable in tf.all_variables () collection and pass it to sess.run () to get its value.",
          "you could also filter tf.all_variables () to find the particular weights and biases that you ' re trying to extract from the model."
        ]
      },
      {
        "title": "43098373",
        "h": "tf.all_variables",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "the default behavior of tf.train.saver is to save ( or restore ) every variable in tf.all_variables () ( in addition to any other \" saveable objects \") using their name property as a key.",
          "you can override the mapping between variables in the checkpoint and tf.variable objects by passing a var_list argument to the tf.train.saver constructor as follows : "
        ]
      },
      {
        "title": "37039218",
        "h": "tf.all_variables",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.9114",
        "sentences": [
          "saver by default uses the list in ops.graphkeys.variables collection ( accessible through tf.all_variables ()), and if you restored from graphdef rather than using python api to build your model , that collection is empty.",
          "you could specify the list of variables manually in tf.train.saver ( var_list =[' myvariable1 : 0 ', ' myvariable2 : 0 ',...]). "
        ]
      },
      {
        "title": "35675700",
        "h": "tf.all_variables",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "0.87",
        "prob": "0.852687",
        "sentences": [
          "when , in cifar10_eval.py a tf.train.saver is created , it uses tf.all_variables (), which includes the implicitly - created variable from the tf.nn.string_input_producer (). "
        ]
      },
      {
        "title": "34458812",
        "h": "tf.all_variables",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "1.0",
        "r_prob": "0.99",
        "prob": "0.8217",
        "sentences": [
          "when you construct your inference graph , you should be able to construct a tf.train.saver () with no arguments , and it will construct the appropriate save and restore ops for you.",
          "the result of tf.all_variables ()) must be a subset of the variables in the training graph , and ( ii ) the corresponding variables must have exactly the same names."
        ]
      }
    ]
  },
  "reboo": {},
  "states": {},
  "tf.initialize_all_variables": {
    "tf.global_variables_initializer": [
      {
        "title": "45273797",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "tf.initialize_all_variables () is depricated.",
          "use tf.global_variables_initializer () instead."
        ]
      },
      {
        "title": "41489484",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "in the newer version of tensorflow : ",
          "tf.initialize_all_variables () is deprecated.",
          "they mention that you have to use : ",
          "tf.global_variables_initializer () "
        ]
      },
      {
        "title": "44942630",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9211999999999999",
        "sentences": [
          "btw : use tf.global_variables_initializer rather than tf.initialize_all_variables.",
          "edit : "
        ]
      },
      {
        "title": "51612509",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9021",
        "sentences": [
          "tf.initialize_all_variables is depreciated , please use tf.global_variables_initializer as recommended here : https :// www.tensorflow.org / api_docs / python / tf / initialize_all_variables "
        ]
      },
      {
        "title": "48111682",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.8835999999999999",
        "sentences": [
          "if you look at the docs , you will see that they do the same , but tf.initialize_all_variables is now deprecated in favour of tf.global_variables_initializer."
        ]
      },
      {
        "title": "43444670",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.97",
        "r_prob": "0.94",
        "prob": "0.8388559999999999",
        "sentences": [
          "tf.initialize_all_variables renamed to tf.global_variables_initializer ",
          "instructions for updating : use tf.global_variables_initializer instead.",
          "if you will call it , you will get a warning as well.",
          "so you should always use tf.global_variables_initializer () "
        ]
      },
      {
        "title": "41439546",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.8366",
        "sentences": [
          "unfortunately , you forgot to read an important line in the documentation of tf.initialize_all_variables.",
          "it will be removed after 2017 - 03 - 02.",
          "instructions for updating : use tf.global_variables_initializer instead."
        ]
      }
    ]
  },
  "tf.estimator.export.build_parsing_serving_input_receiver_fn": {},
  "tf.estimator.export.build_raw_serving_input_receiver_fn": {},
  "tf.set_random_seed": {},
  "tf.global_variables_initializer": {
    "tf.variables_initializer": [
      {
        "title": "48505953",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "looking at the documentation the init = tf.global_variables_initializer () is the same as init = tf.variables_initializer ( tf.global_variables ()) ",
          "instead , the vanilla gradient descent optimizer tf.train.gradientdescentoptimizer does not depend on any variables.",
          "if you would place the init = tf.global_variables_initializer () before the tf.train.adamoptimizer like ",
          "so tf.global_variables_initializer () does not see the needed variables of adam , when placing init = tf.global_variables_initializer () before the adam definition tf.train.adamoptimizer."
        ]
      },
      {
        "title": "43583960",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "in your case the error even explains what variable was not initialized : attempting to use uninitialized value variable_1.",
          "one of the tf tutorials explains a lot about variables , their creation / initialization / saving / loading ",
          "basically to initialize the variable you have 3 options : ",
          "initialize all global variables with tf.global_variables_initializer (). ",
          "initialize variables you care about with tf.variables_initializer ( list_of_vars ). "
        ]
      },
      {
        "title": "43773554",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "failedpreconditionerror : attempting to use uninitialized value is one of the most frequent errors related to tensorflow.",
          "in your case the error even explains what variable was not initialized : attempting to use uninitialized value variable_1.",
          "one of the tf tutorials explains a lot about variables , their creation / initialization / saving / loading ",
          "basically to initialize the variable you have 3 options : ",
          "initialize all global variables with tf.global_variables_initializer (). ",
          "initialize variables you care about with tf.variables_initializer ( list_of_vars ). "
        ]
      },
      {
        "title": "54401883",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "the easiest way is initializing all variables at once using : tf.global_variables_initializer () ",
          "you use sess.run ( init ) to run the initializer , without fetching any value.",
          "to initialize only a subset of variables , you use tf.variables_initializer () listing the variables : "
        ]
      },
      {
        "title": "47271906",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "0.95",
        "prob": "0.9123799999999999",
        "sentences": [
          "the tf.global_variables_initializer just initializes all variables that tf.global_variables () would list.",
          "this actually makes much sense in a distributed environment where the graph might be located in different computing nodes in a cluster.",
          "in such a case , tf.global_variables_initializer () which is just an alias for tf.variables_initializer ( tf.global_variables ()) would initialize all the variables in all the computing nodes , where the graph is placed."
        ]
      },
      {
        "title": "52725027",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.99",
        "r_prob": "0.95",
        "prob": "0.9122849999999999",
        "sentences": [
          "tf.global_variables_initializer makes an initialization op for all the global variables created up to that point."
        ]
      },
      {
        "title": "45976536",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9009",
        "sentences": [
          "if var_list is empty , however , the function still returns an op that can be run.",
          "that op just has no effect.",
          "the load script has no global varibale , and since tf.global_variables_initializer () is equivalent to tf.variables_initializer ( tf.global_variables ()), the operation is a no - op."
        ]
      }
    ],
    "tf.initialize_all_variables": [
      {
        "title": "45273797",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "tf.initialize_all_variables () is depricated.",
          "use tf.global_variables_initializer () instead."
        ]
      },
      {
        "title": "41489484",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "in the newer version of tensorflow : ",
          "tf.initialize_all_variables () is deprecated.",
          "they mention that you have to use : ",
          "tf.global_variables_initializer () "
        ]
      },
      {
        "title": "44942630",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9211999999999999",
        "sentences": [
          "btw : use tf.global_variables_initializer rather than tf.initialize_all_variables.",
          "edit : "
        ]
      },
      {
        "title": "51612509",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9021",
        "sentences": [
          "tf.initialize_all_variables is depreciated , please use tf.global_variables_initializer as recommended here : https :// www.tensorflow.org / api_docs / python / tf / initialize_all_variables "
        ]
      },
      {
        "title": "48111682",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.8835999999999999",
        "sentences": [
          "if you look at the docs , you will see that they do the same , but tf.initialize_all_variables is now deprecated in favour of tf.global_variables_initializer."
        ]
      },
      {
        "title": "43444670",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.97",
        "r_prob": "0.94",
        "prob": "0.8388559999999999",
        "sentences": [
          "tf.initialize_all_variables renamed to tf.global_variables_initializer ",
          "instructions for updating : use tf.global_variables_initializer instead.",
          "if you will call it , you will get a warning as well.",
          "so you should always use tf.global_variables_initializer () "
        ]
      },
      {
        "title": "41439546",
        "h": "tf.initialize_all_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.8366",
        "sentences": [
          "unfortunately , you forgot to read an important line in the documentation of tf.initialize_all_variables.",
          "it will be removed after 2017 - 03 - 02.",
          "instructions for updating : use tf.global_variables_initializer instead."
        ]
      }
    ],
    "tf.global_variables": [
      {
        "title": "47271906",
        "h": "tf.global_variables_initializer",
        "t": "tf.global_variables",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "the tf.global_variables_initializer just initializes all variables that tf.global_variables () would list.",
          "this actually makes much sense in a distributed environment where the graph might be located in different computing nodes in a cluster.",
          "in such a case , tf.global_variables_initializer () which is just an alias for tf.variables_initializer ( tf.global_variables ()) would initialize all the variables in all the computing nodes , where the graph is placed."
        ]
      },
      {
        "title": "49785891",
        "h": "tf.global_variables_initializer",
        "t": "tf.global_variables",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9603999999999999",
        "sentences": [
          "as documented : ",
          "tf.global_variables_initializer () is just a shortcut for variables_initializer ( global_variables ()) ",
          "therefore , call tf.global_variables () will give you a list of initialized variables."
        ]
      },
      {
        "title": "48505953",
        "h": "tf.global_variables_initializer",
        "t": "tf.global_variables",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.92",
        "r_prob": "0.98",
        "prob": "0.892584",
        "sentences": [
          "looking at the documentation the init = tf.global_variables_initializer () is the same as init = tf.variables_initializer ( tf.global_variables ()) ",
          "instead , the vanilla gradient descent optimizer tf.train.gradientdescentoptimizer does not depend on any variables.",
          "if you would place the init = tf.global_variables_initializer () before the tf.train.adamoptimizer like ",
          "so tf.global_variables_initializer () does not see the needed variables of adam , when placing init = tf.global_variables_initializer () before the adam definition tf.train.adamoptimizer."
        ]
      },
      {
        "title": "45976536",
        "h": "tf.global_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.91",
        "r_prob": "0.93",
        "prob": "0.6770400000000001",
        "sentences": [
          "if var_list is empty , however , the function still returns an op that can be run.",
          "that op just has no effect.",
          "the load script has no global varibale , and since tf.global_variables_initializer () is equivalent to tf.variables_initializer ( tf.global_variables ()), the operation is a no - op."
        ]
      }
    ],
    "tf.session": [
      {
        "title": "45139678",
        "h": "tf.global_variables_initializer",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "try adding init = tf.global_variables_initializer () before with tf.session () as sess :, this should fix the \" uninitialized value \" error.",
          "tf.global_variables_initializer () has to be called after your graph has been defined.",
          "always define init as your last operation before with tf.session () ... to make sure you don ' t miss anything in the initialization."
        ]
      },
      {
        "title": "44435910",
        "h": "tf.global_variables_initializer",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9602999999999999",
        "sentences": [
          "you have to initialize the variables.",
          "try moving tf.global_variables_initializer () inside tf.session () as sess : block and run it as tf.global_variables_initializer (). run () "
        ]
      },
      {
        "title": "53886128",
        "h": "tf.session",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "0.94",
        "prob": "0.921294",
        "sentences": [
          "tf.session () initiates a tensorflow graph object in which tensors are processed through operations ( or ops ). ",
          "these have to be initiated once the session is created.",
          "hence we call tf.global_variables_initializer (). run () ",
          "a graph contains tensors and operations.",
          "in other words , graph provides a schema whereas a session processes a graph to compute values ( tensors ). "
        ]
      },
      {
        "title": "51537951",
        "h": "tf.global_variables_initializer",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "1.0",
        "r_prob": "1.0",
        "prob": "0.91",
        "sentences": [
          "and with this follwing code works as expected : with tf.session () as sess : sess.run ( tf.global_variables_initializer ()) image , label = sess.run ( make_batch ( 1 )) "
        ]
      },
      {
        "title": "54997427",
        "h": "tf.session",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.86",
        "r_prob": "0.99",
        "prob": "0.8003159999999999",
        "sentences": [
          "option 2 : use tf.session () instead of the ' with ' block but add the line sess.run ( tf.global_variables_initializer ()): "
        ]
      },
      {
        "title": "61988339",
        "h": "tf.global_variables_initializer",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.81",
        "r_prob": "0.96",
        "prob": "0.723168",
        "sentences": [
          "tried using tensorflow 2 and everything fails.",
          "no more tf.session (), no more tf.global_variables_initializer (), etc.",
          "so no working tensorflow in ubuntu , you have to use docker."
        ]
      },
      {
        "title": "50047139",
        "h": "tf.session",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.88",
        "r_prob": "0.99",
        "prob": "0.7143839999999999",
        "sentences": [
          "executing functions which define the graph again will just build another subgraph ( this probably also goes for your function which defines the placeholder and variables ). ",
          "the tf.global_variables_initializer operation should also only be executed once.",
          "so in the notebook after initializing the graph exactly once you can only call functions which wrap tensorflow graph evaluation code , not graph building code dynamically without resetting the kernel.",
          "examples for such methods which only evaluate an existing graph are session.run , other tf.session methods or similar evaluation methods like tensor.eval."
        ]
      }
    ],
    "tf.local_variables_initializer": [
      {
        "title": "41489484",
        "h": "tf.local_variables_initializer",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9603999999999999",
        "sentences": [
          "they mention that you have to use : ",
          "tf.global_variables_initializer () ",
          "what is happening here is that there is nothing in the queue for it to read , hence it says requested 1 current 0.",
          "just add this : ",
          "tf.local_variables_initializer () ",
          "i have pushed the update."
        ]
      },
      {
        "title": "51315836",
        "h": "tf.global_variables_initializer",
        "t": "tf.local_variables_initializer",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "before executing your variables using a tensorflow session object , all the variables must be initialized.",
          "the method tf.global_variables_initializer (), initializes all the global trainable variables in the graph that are local to the machine.",
          "whereas , the method tf.local_variables_initializer initializes all the variables shared across a distributed environment."
        ]
      },
      {
        "title": "54817501",
        "h": "tf.local_variables_initializer",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.9309999999999999",
        "sentences": [
          "seems like it used to be tf.global_variables_initializer (), but now tf.local_variables_initializer () works , at least for me."
        ]
      },
      {
        "title": "55797959",
        "h": "tf.local_variables_initializer",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.855",
        "sentences": [
          "run both : ",
          "sess.run ( tf.global_variables_initializer ()) ",
          "sess.run ( tf.local_variables_initializer ()) "
        ]
      }
    ],
    "tf.variable": [
      {
        "title": "44434099",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9309999999999999",
        "sentences": [
          "a more complete description is given here.",
          "only after running tf.global_variables_initializer () in a session will your variables hold the values you told them to hold when you declare them ( tf.variable ( tf.zeros (...)), tf.variable ( tf.random_normal (...)),...). ",
          "from the tf doc : ",
          "calling tf.variable () adds several ops to the graph : "
        ]
      },
      {
        "title": "48760159",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.91",
        "r_prob": "0.95",
        "prob": "0.786695",
        "sentences": [
          "you can define xx as a tf.variable instead , giving it a default value ( which will be used whenever xx is not fed with another value ). ",
          "a few things to notice : ",
          "don ' t forget to initialize the values for xx , by using , e.g ., tf.global_variables_initializer ().. "
        ]
      },
      {
        "title": "43773554",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.97",
        "r_prob": "0.79",
        "prob": "0.758637",
        "sentences": [
          "failedpreconditionerror : attempting to use uninitialized value is one of the most frequent errors related to tensorflow.",
          "this exception is most commonly raised when running an operation that reads a tf.variable before it has been initialized.",
          "in your case the error even explains what variable was not initialized : attempting to use uninitialized value variable_1.",
          "one of the tf tutorials explains a lot about variables , their creation / initialization / saving / loading ",
          "basically to initialize the variable you have 3 options : ",
          "initialize all global variables with tf.global_variables_initializer (). "
        ]
      },
      {
        "title": "43583960",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.97",
        "r_prob": "0.79",
        "prob": "0.758637",
        "sentences": [
          "this exception is most commonly raised when running an operation that reads a tf.variable before it has been initialized.",
          "in your case the error even explains what variable was not initialized : attempting to use uninitialized value variable_1.",
          "one of the tf tutorials explains a lot about variables , their creation / initialization / saving / loading ",
          "basically to initialize the variable you have 3 options : ",
          "initialize all global variables with tf.global_variables_initializer (). "
        ]
      },
      {
        "title": "45897756",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.71",
        "r_prob": "1.0",
        "prob": "0.6887",
        "sentences": [
          "however , now i ' m facing a new problem when trying to use the network copy.",
          "the new variables in the copy are not initialized , and trying to run tf.global_variables_initializer () does not help.",
          "or , if that weren ' t possible , at least a reliable way to get the initializer ops without having to find the tf.variable objects of the original network."
        ]
      },
      {
        "title": "57441807",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.68",
        "r_prob": "0.99",
        "prob": "0.659736",
        "sentences": [
          "therefore , all of the tf.variable in your tf graph will be initialized randomly.",
          "for example , in the code bellow , the tf.variable will be initialized with values from a normal distribution.",
          "you initialized those tf.variable with a the values you wish.",
          "as for the distinction between tf.global_variables_initializer and tf.constant_initializer , they are something completely different : ",
          "tf.global_variables_initializer is an operation that you execute to initialize all variables in your graph.",
          "tf.constant_initializer on the other hand , is just an initializer that you pass to the tf.variable of your graph.",
          "then , when a tf.session runs the operation tf.global_variables_initializer , the tf graph will use the tf.constant_initializer to initialize the corresponding tf.variable with the constant values provided."
        ]
      },
      {
        "title": "47271489",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.65",
        "r_prob": "0.99",
        "prob": "0.61776",
        "sentences": [
          "btw , if you want to initialize only a single tensor ( for e.g.",
          "tf.variable ) that hasn ' t been initialized using tf.global_variables_initializer (), then you can use your_tensor.initializer in the sess.run () as in the following example : "
        ]
      },
      {
        "title": "55372251",
        "h": "tf.variable",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.96",
        "r_prob": "0.8",
        "prob": "0.59904",
        "sentences": [
          "you just need to modify the code a little bit.",
          "the value of tf.variable should not be tf.placeholder , otherwise it will cause your initialization error when running sess.run ( tf.global_variables_initializer ()). "
        ]
      },
      {
        "title": "57438488",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.61",
        "r_prob": "1.0",
        "prob": "0.5978",
        "sentences": [
          "apparently there is , after running tf.global_variables_initializer () the variables got reinitialized.",
          "thus , the assertion fails.",
          "can a tf.variable be used in multiple sessions ? "
        ]
      },
      {
        "title": "45003763",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.69",
        "r_prob": "0.81",
        "prob": "0.48065399999999997",
        "sentences": [
          "the most obvious difference between the tf.variable and the tf.placeholder is that ",
          "initialization of the variables is done with sess.run ( tf.global_variables_initializer ()). ",
          "placeholder is a function , variable is a class ( hence an uppercase ). ",
          "when you use tf in a distributed environment , variables are stored in a special place ( parameter server ) and are shared between the workers .. "
        ]
      },
      {
        "title": "43693704",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.69",
        "r_prob": "0.81",
        "prob": "0.48065399999999997",
        "sentences": [
          "the most obvious difference between the tf.variable and the tf.placeholder is that ",
          "initialization of the variables is done with sess.run ( tf.global_variables_initializer ()). ",
          "placeholder is a function , variable is a class ( hence an uppercase ). ",
          "when you use tf in a distributed environment , variables are stored in a special place ( parameter server ) and are shared between the workers .. "
        ]
      }
    ],
    "tf.placeholder": [
      {
        "title": "45366164",
        "h": "tf.placeholder",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.8091",
        "sentences": [
          "it ' s never a requirement unless you are using a declared tf.variable or tf.placeholder from within your tensorflow session run.",
          "personally , i always make it a habit of running tf.global_variables_initializer (). "
        ]
      },
      {
        "title": "43693704",
        "h": "tf.global_variables_initializer",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.7654",
        "sentences": [
          "the most obvious difference between the tf.variable and the tf.placeholder is that ",
          "initialization of the variables is done with sess.run ( tf.global_variables_initializer ()). ",
          "placeholder is a function , variable is a class ( hence an uppercase ). ",
          "when you use tf in a distributed environment , variables are stored in a special place ( parameter server ) and are shared between the workers .. "
        ]
      },
      {
        "title": "45003763",
        "h": "tf.global_variables_initializer",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.7654",
        "sentences": [
          "the most obvious difference between the tf.variable and the tf.placeholder is that ",
          "initialization of the variables is done with sess.run ( tf.global_variables_initializer ()). ",
          "placeholder is a function , variable is a class ( hence an uppercase ). ",
          "when you use tf in a distributed environment , variables are stored in a special place ( parameter server ) and are shared between the workers .. "
        ]
      },
      {
        "title": "55372251",
        "h": "tf.global_variables_initializer",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.72",
        "r_prob": "1.0",
        "prob": "0.6911999999999999",
        "sentences": [
          "you just need to modify the code a little bit.",
          "the value of tf.variable should not be tf.placeholder , otherwise it will cause your initialization error when running sess.run ( tf.global_variables_initializer ()). ",
          "you can use tf.stack instead of it."
        ]
      }
    ]
  },
  "tf.scatter_nd_add": {},
  "tf.tensor_scatter_nd_add": {},
  "tf.train.slice_input_producer": {
    "tf.train.batch": [
      {
        "title": "41864611",
        "h": "tf.train.slice_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.873",
        "sentences": [
          "assuming you can get a list of filenames , image_paths and a numpy array of labels labels , you can bind them together and operate on individual examples with tf.train.slice_input_producer then batch them together using tf.train.batch."
        ]
      },
      {
        "title": "48991201",
        "h": "tf.train.slice_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.8526",
        "sentences": [
          "tf.train.batch simply groups upstream samples into batches , and nothing more.",
          "for example , if your training data fits into a tensor , you could use tf.train.slice_input_producer to produce samples.",
          "this function has arguments for shuffling and epochs."
        ]
      },
      {
        "title": "46053671",
        "h": "tf.train.slice_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.99",
        "r_prob": "0.95",
        "prob": "0.8182349999999999",
        "sentences": [
          "you can use tf.train.batch , but you also have to use a queue for filenames such as tf.train.slice_input_producer "
        ]
      },
      {
        "title": "41866982",
        "h": "tf.train.slice_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7968",
        "sentences": [
          "use tf.train.slice_input_producer to get a tensor for a single example ;. ",
          "load images from filenames ;. ",
          "batch them together using tf.train.batch to group them up .. "
        ]
      },
      {
        "title": "44350073",
        "h": "tf.train.slice_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.96",
        "r_prob": "0.54",
        "prob": "0.47174400000000005",
        "sentences": [
          "set capacity = 55000 for tf.train.slice_input_producer.",
          "( 55000 is the size of mnist training set in my case ).",
          "set num_threads = 5 for tf.train.batch .. ",
          "set capacity = 500 for tf.train.batch .. ",
          "this was possible because titan x has enough memory space to preload entire dataset.",
          ". "
        ]
      }
    ]
  },
  "tf.scatter_sub": {},
  "tf.scatter_add": {
    "tf.scatter_update": [
      {
        "title": "41455713",
        "h": "tf.scatter_add",
        "t": "tf.scatter_update",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.7084",
        "sentences": [
          "you can try tf.scatter_update or tf.scatter_add to update tensor value according to indices."
        ]
      },
      {
        "title": "41418052",
        "h": "tf.scatter_update",
        "t": "tf.scatter_add",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.76",
        "r_prob": "1.0",
        "prob": "0.7068000000000001",
        "sentences": [
          "how about tf.scatter_update ( ref , indices , updates ) or tf.scatter_add ( ref , indices , updates )? "
        ]
      },
      {
        "title": "37584068",
        "h": "tf.scatter_add",
        "t": "tf.scatter_update",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.65",
        "r_prob": "1.0",
        "prob": "0.43550000000000005",
        "sentences": [
          "tensorflow variable objects have limited support for updating slices , using the tf.scatter_update (), tf.scatter_add (), and tf.scatter_sub () ops.",
          "to update a single row of the variable , you can use tf.scatter_update (). ",
          "to chain multiple updates , you can use the mutable updated_state tensor that is returned from tf.scatter_update (): "
        ]
      }
    ]
  },
  "tf.control_flow_ops.cond": {},
  "tf.control_flo": {},
  "tf.convert_to_tensor_or_sparse_tensor": {},
  "tf.nest.map_structure": {},
  "tf.variablescope": {},
  "tf.graph_util.import_graph_def": {},
  "tf.float": {
    "tf.int32": [
      {
        "title": "47374305",
        "h": "tf.int32",
        "t": "tf.float",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.68",
        "r_prob": "0.98",
        "prob": "0.639744",
        "sentences": [
          "you ' re trying to multiple tensors with a tf.int32 ( dt_int32 ) datatype on the gpu."
        ]
      },
      {
        "title": "43853600",
        "h": "tf.float",
        "t": "tf.int32",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.6324000000000001",
        "sentences": [
          "another issue is the different data type between your matrix ( tf.int32 ) and the tensorarray ( tf.float32 ), based on your code you ' re multiplying the matrix ints by 2 and writing the result into the array so it should be int32 as well."
        ]
      },
      {
        "title": "52577242",
        "h": "tf.int32",
        "t": "tf.float",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.73",
        "r_prob": "0.97",
        "prob": "0.5806419999999999",
        "sentences": [
          "p , a = tf.get_session_tensor ( h.handle , tf.float32 ) ",
          "p , a = tf.get_session_tensor ( h.handle , tf.int32 ) "
        ]
      }
    ]
  },
  "exploding gradient problem": {},
  "tf.train.input_producer": {},
  "tf.keras.metrics.meaniou": {},
  "tf.keras.metrics.metric": {},
  "tf.contrib.layers.flatten": {},
  "tf.layers": {
    "tf.keras.layers": [
      {
        "title": "56147622",
        "h": "tf.layers",
        "t": "tf.keras.layers",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.9211999999999999",
        "sentences": [
          "this is the low level api."
        ]
      },
      {
        "title": "54718798",
        "h": "tf.keras.layers",
        "t": "tf.layers",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.8464",
        "sentences": [
          "since tensorflow 1.12 , tf.layers are merely wrappers around tf.keras.layers.",
          "a few examples : ",
          "convolutional tf.layers just inherit from the convolutional tf.keras.layers , see source code here : ",
          "the same is true for all core tf.layers , e.g.",
          "tf.keras is becoming the de - facto high - level api for tensorflow , therefore tf.layers are now just wrappers around tf.keras.layers."
        ]
      },
      {
        "title": "58538279",
        "h": "tf.layers",
        "t": "tf.keras.layers",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.97",
        "r_prob": "0.85",
        "prob": "0.750295",
        "sentences": [
          "so i faced the same error but discovered that my version of tensorflow ( which is 2.0 ) moved layers from the tf package ( tf.layers ) to tf.keras.",
          "an easy fix would be to replace tf.layers with tf.keras.layers "
        ]
      },
      {
        "title": "55413120",
        "h": "tf.keras.layers",
        "t": "tf.layers",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.6596000000000001",
        "sentences": [
          "as per official docs , tf.layers are wrappers around tf.keras.layers.",
          "convolutional layers in layers api inherit from tf.keras.layers."
        ]
      },
      {
        "title": "53534547",
        "h": "tf.keras.layers",
        "t": "tf.layers",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.9",
        "r_prob": "0.6",
        "prob": "0.5075999999999999",
        "sentences": [
          "the core functionality corresponding to tf.contrib.layers is in tf.layers.",
          "if your goal is to prepare your code for tf 2.0 , consider that tf.contrib will be removed entirely ( either split from tf or integrated into it ) and that tf.layers too will be removed and the high - level api will reside under tf.keras.",
          "so to best prepare for tf 2.0 you should start using tf.keras.layers instead."
        ]
      }
    ],
    "tf.contrib.layers": [
      {
        "title": "47862519",
        "h": "tf.contrib.layers",
        "t": "tf.layers",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.94",
        "prob": "0.8935639999999999",
        "sentences": [
          "just to mention that : ",
          "there is no need to extract weights and biases just to save them.",
          "for tf.layers or tf.contrib.layers , if trainable is set to true , the weights and biases are added to graphkeys.trainable_variables , which is a subset of graphkeys.global_variables."
        ]
      },
      {
        "title": "55518654",
        "h": "tf.contrib.layers",
        "t": "tf.layers",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.8633",
        "sentences": [
          "this not the case of tf.conrib.layers.",
          "even tf.layers ( which was distilled from tf.contrib.layers ) will be no longer supported."
        ]
      },
      {
        "title": "44805926",
        "h": "tf.contrib.layers",
        "t": "tf.layers",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.85",
        "r_prob": "1.0",
        "prob": "0.816",
        "sentences": [
          "apis in this namespace are allowed to change rapidly between versions , whereas the others usually can ' t without a new major version.",
          "in particular , the functions in tf.contrib.layers are not identical to those found in tf.layers , although some of them might be replicated with different names."
        ]
      }
    ]
  },
  "tf.train.latest_checkpoint": {
    "tf.train.import_meta_graph": [
      {
        "title": "42897970",
        "h": "tf.train.import_meta_graph",
        "t": "tf.train.latest_checkpoint",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "for example , in your case , before you did : saver = tf.train.import_meta_graph (' model.ckpt.meta ') saver.restore ( sess , tf.train.latest_checkpoint ('./')) your graph had a variable called v1."
        ]
      },
      {
        "title": "47707275",
        "h": "tf.train.import_meta_graph",
        "t": "tf.train.latest_checkpoint",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9306",
        "sentences": [
          "three suggestions : ",
          "check the path when restoring the model ",
          "saver = tf.train.import_meta_graph ( model_path ) ",
          "check the path when restoring the checkpoint ",
          "saver.restore ( sess , tf.train.latest_checkpoint ( cur_dir )) "
        ]
      },
      {
        "title": "47709570",
        "h": "tf.train.import_meta_graph",
        "t": "tf.train.latest_checkpoint",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.9207000000000001",
        "sentences": [
          "the key is that it doesn ' t need to call tf.train.import_meta_graph () if it has already uses saver.restore ( sess , tf.train.latest_checkpoint ('./')). "
        ]
      }
    ]
  },
  "tf.contrib.metrics.streaming_auc": {},
  "tf.local_variables_initializer": {
    "tf.global_variables_initializer": [
      {
        "title": "41489484",
        "h": "tf.local_variables_initializer",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9603999999999999",
        "sentences": [
          "they mention that you have to use : ",
          "tf.global_variables_initializer () ",
          "what is happening here is that there is nothing in the queue for it to read , hence it says requested 1 current 0.",
          "just add this : ",
          "tf.local_variables_initializer () ",
          "i have pushed the update."
        ]
      },
      {
        "title": "51315836",
        "h": "tf.global_variables_initializer",
        "t": "tf.local_variables_initializer",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "before executing your variables using a tensorflow session object , all the variables must be initialized.",
          "the method tf.global_variables_initializer (), initializes all the global trainable variables in the graph that are local to the machine.",
          "whereas , the method tf.local_variables_initializer initializes all the variables shared across a distributed environment."
        ]
      },
      {
        "title": "54817501",
        "h": "tf.local_variables_initializer",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.9309999999999999",
        "sentences": [
          "seems like it used to be tf.global_variables_initializer (), but now tf.local_variables_initializer () works , at least for me."
        ]
      },
      {
        "title": "55797959",
        "h": "tf.local_variables_initializer",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.855",
        "sentences": [
          "run both : ",
          "sess.run ( tf.global_variables_initializer ()) ",
          "sess.run ( tf.local_variables_initializer ()) "
        ]
      }
    ]
  },
  "recurrent neural networks": {
    "convolutional neural networks": [
      {
        "title": "55978869",
        "h": "convolutional neural networks",
        "t": "recurrent neural networks",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.46",
        "r_prob": "1.0",
        "prob": "0.4232",
        "sentences": [
          "for convolutional neural networks ( cnn ): ",
          "you can use them if your input is fixed length and has significant features.",
          "for recurrent neural networks : "
        ]
      },
      {
        "title": "45147778",
        "h": "convolutional neural networks",
        "t": "recurrent neural networks",
        "r": "S1",
        "h_prob": "0.65",
        "t_prob": "0.44",
        "r_prob": "1.0",
        "prob": "0.28600000000000003",
        "sentences": [
          "on other tasks , convolutional neural networks with pooling layers or recurrent neural networks can be helpful."
        ]
      },
      {
        "title": "45980285",
        "h": "convolutional neural networks",
        "t": "recurrent neural networks",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.25",
        "r_prob": "1.0",
        "prob": "0.2025",
        "sentences": [
          "to encode sequence to a fixed length vector you typically use recurrent neural networks ( rnns ) or convolutional neural networks ( cnns ). "
        ]
      }
    ]
  },
  "tf.parse_single_example": {
    "tf.parse_example": [
      {
        "title": "48654046",
        "h": "tf.parse_single_example",
        "t": "tf.parse_example",
        "r": "S1",
        "h_prob": "0.49",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.46549999999999997",
        "sentences": [
          "encoding each frame as a separate feature makes it difficult to select frames dynamically , because the signature of tf.parse_example () ( and tf.parse_single_example ()) requires that the set of parsed feature names be fixed at graph construction time."
        ]
      },
      {
        "title": "45800039",
        "h": "tf.parse_example",
        "t": "tf.parse_single_example",
        "r": "S1",
        "h_prob": "0.53",
        "t_prob": "0.84",
        "r_prob": "1.0",
        "prob": "0.4452",
        "sentences": [
          "the shape error \" shape must be rank 1 but is rank 0 \" indicates that the tf.parse_example () op expects a vector ( rank 1 tensor ) as input , rather than a scalar.",
          "there are at least two possible solutions : ",
          "use the tf.parse_single_example () op , which expects a scalar input , instead .. ",
          "reshape the value returned by reader.read () into a vector , for example using tf.expand_dims ( examples_serialized , 0 ).. "
        ]
      },
      {
        "title": "49219963",
        "h": "tf.parse_example",
        "t": "tf.parse_single_example",
        "r": "S1",
        "h_prob": "0.5",
        "t_prob": "0.72",
        "r_prob": "1.0",
        "prob": "0.36",
        "sentences": [
          "the following appears to work : no errors are raised , at least.",
          "tf.parse_example ([ serialized ], ...) is used instead of tf.parse_single_example ( serialized , ...). "
        ]
      },
      {
        "title": "39376833",
        "h": "tf.parse_single_example",
        "t": "tf.parse_example",
        "r": "S1",
        "h_prob": "0.51",
        "t_prob": "0.57",
        "r_prob": "1.0",
        "prob": "0.29069999999999996",
        "sentences": [
          "and to use tf.parse_example ( which is faster than tf.parse_single_example ) you need to first batch the examples like that : "
        ]
      },
      {
        "title": "47400330",
        "h": "tf.parse_example",
        "t": "tf.parse_single_example",
        "r": "S1",
        "h_prob": "0.43",
        "t_prob": "0.63",
        "r_prob": "1.0",
        "prob": "0.2709",
        "sentences": [
          "tf.parse_example operates on a batch (\" rank 1 \"), and decode_png expects a single image ( a scalar string , \" rank 0 \"). ",
          "i ' d either use tf.parse_single_example or add a reshape to scalar ( shape =[]) before using decode_png."
        ]
      }
    ]
  },
  "tf.sequence": {},
  "tf.transpose": {
    "tf.matmul": [
      {
        "title": "39383544",
        "h": "tf.matmul",
        "t": "tf.transpose",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.82",
        "r_prob": "0.98",
        "prob": "0.7553839999999999",
        "sentences": [
          "try to transpose your output , i.e.",
          "change tf.nn.softmax ( tf.matmul ( x , w ) + b )) to tf.nn.softmax ( tf.transpose ( tf.matmul ( x , w ) + b ))). "
        ]
      },
      {
        "title": "50132258",
        "h": "tf.transpose",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.74",
        "t_prob": "0.97",
        "r_prob": "0.97",
        "prob": "0.6962659999999999",
        "sentences": [
          "a one - dimensional array , and you are trying to multiply it with a two - dimensional array w1 of shape ( 2 , 3 ), which is not possible for matrix multiplication , as number of columns of first parameter must be equal to number of rows in second parameter.",
          "this will create a two - dimensional constant tensor of shape ( 2 , 1 ). ",
          "and , then multiply it as , ",
          "a = tf.matmul ( tf.transpose ( x ), w1 ) ",
          "tf.transpose () is used to create transpose of array x with shape ( 2 , 1 ) to shape ( 1 , 2 ). "
        ]
      },
      {
        "title": "44099690",
        "h": "tf.transpose",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.6499",
        "sentences": [
          "they will be updated by optimizer during training.",
          "we can use tf.matmul ( embed , tf.transpose ( nce_weights )) + nce_biases to get final output score."
        ]
      },
      {
        "title": "41941688",
        "h": "tf.transpose",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.63",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.5922",
        "sentences": [
          "you can use tf.matmul and tf.transpose "
        ]
      }
    ],
    "tf.reshape": [
      {
        "title": "56833142",
        "h": "tf.reshape",
        "t": "tf.transpose",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.6",
        "r_prob": "1.0",
        "prob": "0.576",
        "sentences": [
          "anyway , you can get what you need with tf.concat , tf.reshape , and tf.transpose.",
          "so you may need to use tf.transpose , interleave using concat and reshape , then transpose again to reorder the dimensions."
        ]
      },
      {
        "title": "40899688",
        "h": "tf.transpose",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.44",
        "t_prob": "0.94",
        "r_prob": "0.88",
        "prob": "0.36396799999999996",
        "sentences": [
          "tf.reshape ( a , shape =[- 1 ]) will \" unroll \" tensor a into vector using row - major order.",
          "if you want different order , you could tf.transpose first "
        ]
      },
      {
        "title": "57721028",
        "h": "tf.transpose",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.37",
        "t_prob": "0.6",
        "r_prob": "0.98",
        "prob": "0.21756",
        "sentences": [
          "you can make use of tf.transpose to shift your axis from nhwc to nchw ",
          "you may even make use of tf.reshape "
        ]
      }
    ]
  },
  "tf.train.adadeltaoptimizer": {},
  "tf.substr": {},
  "tf.abs": {},
  "tf.__path__": {},
  "tf.__": {},
  "learning rate": {
    "loss function": [
      {
        "title": "60954965",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.89",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.8277000000000001",
        "sentences": [
          "such a messy loss trajectory would usually mean that the learning rate is too high for the given smoothness of the loss function.",
          "an alternative interpretation is that the loss function is not at all predictive of the success at the given task."
        ]
      },
      {
        "title": "44949185",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.93",
        "r_prob": "0.99",
        "prob": "0.7457670000000001",
        "sentences": [
          "if your model is not converging it means that the optimizer is stuck in a local minima in your loss function.",
          "another strategy employed often is the learning rate decay , which reduces your learning rate by a factor every several epochs."
        ]
      },
      {
        "title": "47692350",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.6992",
        "sentences": [
          "note that your loss function is ",
          "l = \\ sum ( wx + b - y )^ 2 ",
          "now , your loss is diverging because learning rate is more than inverse of hessian which there will be roughly 1 /( 2 * 2900 ). "
        ]
      },
      {
        "title": "35106509",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.85",
        "r_prob": "1.0",
        "prob": "0.6885",
        "sentences": [
          "if you are getting nan values , it is probably because your learning rate is high relative to your loss function."
        ]
      },
      {
        "title": "49943047",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.7",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.651",
        "sentences": [
          "you can keep an eye on loss function and do early stopping if you think you can adjust to better learning rate."
        ]
      },
      {
        "title": "40434284",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.69",
        "t_prob": "0.92",
        "r_prob": "0.96",
        "prob": "0.6094080000000001",
        "sentences": [
          "too high of a learning rate."
        ]
      },
      {
        "title": "55838335",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.72",
        "t_prob": "0.9",
        "r_prob": "0.93",
        "prob": "0.6026400000000001",
        "sentences": [
          "i have tried to increase layers , play with the learning rate , changing the loss function , changing the optimizer , scaling the data , normalizing the data , but nothing helped me to solve this problem."
        ]
      },
      {
        "title": "52049396",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.73",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.5985999999999999",
        "sentences": [
          "your loss function looks ok , although i would just use : tf.reduce_sum ( tf.square ( heatmaps - heat_ground_truth , 2 ) , name =' heat_loss '). ",
          "then use the same loss function."
        ]
      },
      {
        "title": "46482301",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.8",
        "r_prob": "0.91",
        "prob": "0.5678400000000001",
        "sentences": [
          "one major problem with your estimator is the loss function.",
          "since you use tf.reduce_sum , the loss grows with the number of samples , which you have to compensate by using a smaller learning rate."
        ]
      },
      {
        "title": "63493601",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.8",
        "t_prob": "0.91",
        "r_prob": "0.78",
        "prob": "0.5678400000000001",
        "sentences": [
          "compile defines the loss function , the optimizer and the metrics.",
          "you need a compiled model to train ( because training uses the loss function and the optimizer ). "
        ]
      },
      {
        "title": "54031459",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.73",
        "r_prob": "0.99",
        "prob": "0.549252",
        "sentences": [
          "choose the right loss function binary crossentropy might lead your network in the direction of optimizing for all labels , now if you have an unbalanced amount of labels in your image , it might draw your network to just give back either white , gray or black image predictions.",
          "reduce learning rate if your learning rate is too high you might converge in non - sufficient optima , which also tend to optimize for gray , black or white predictions only."
        ]
      },
      {
        "title": "48893698",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.72",
        "r_prob": "0.87",
        "prob": "0.53244",
        "sentences": [
          "if you really do have a reason for trying to generate a model that creates tries to match word2vec , then looking at your loss function here are a few suggestions.",
          "if the learning rate is too small then you may not actually be able to move enough in the right direction."
        ]
      },
      {
        "title": "49924566",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.9",
        "r_prob": "0.86",
        "prob": "0.48762000000000005",
        "sentences": [
          "imagine here you are over - correcting the jumping - around and it ' s not jumping around enough to further minimize the loss function .. ",
          "when to reduce epochs "
        ]
      },
      {
        "title": "47996024",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.88",
        "r_prob": "0.64",
        "prob": "0.47872000000000003",
        "sentences": [
          "compile defines the loss function , the optimizer and the metrics.",
          "you need a compiled model to train ( because training uses the loss function and the optimizer ). ",
          "loss function.",
          "optimizer / learning rate.",
          "metrics."
        ]
      },
      {
        "title": "36577144",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.69",
        "t_prob": "0.68",
        "r_prob": "1.0",
        "prob": "0.4692",
        "sentences": [
          "the extra layer made the gradients too unstable , and that lead to the loss function quickly devolving to nan.",
          "also , decreasing the learning rate may help."
        ]
      }
    ],
    "batch size": [
      {
        "title": "61556937",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.91",
        "r_prob": "0.98",
        "prob": "0.8115380000000001",
        "sentences": [
          "change your batch size.",
          "change your learning rate or use the callbacks api of tensorflow for learning decay .. "
        ]
      },
      {
        "title": "33899251",
        "h": "learning rate",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.86",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.7998000000000001",
        "sentences": [
          "increase it to something like 2000 at a minimum ( running 20 times through your dataset ). ",
          "that ' s the issue , your learning rate was too high for the loss function you were using."
        ]
      },
      {
        "title": "48893698",
        "h": "learning rate",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.91",
        "r_prob": "1.0",
        "prob": "0.7735",
        "sentences": [
          "this should represent the dimension along which to compute the cosine distance and i think that the 0th dimension would be the wrong dimension ( as this likely should be the batch size.",
          "if the learning rate is too small then you may not actually be able to move enough in the right direction."
        ]
      },
      {
        "title": "43949154",
        "h": "learning rate",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.7387",
        "sentences": [
          "this happens a lot in practice when your learning rate is too high , i tend to start at 0.001 and move from there , 0.1 is on the very high side on most datasets , especially if you aren ' t dividing your loss by your batch size."
        ]
      },
      {
        "title": "63493234",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.92",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.7360000000000001",
        "sentences": [
          "if that doesn ' t help , i would try to use 1 as batch size , to see if at least within first runs there is a change.",
          "as well the learning rate might be a problem , try to play with its value and see if the accuracy changes."
        ]
      },
      {
        "title": "54708235",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.92",
        "t_prob": "0.74",
        "r_prob": "1.0",
        "prob": "0.6808000000000001",
        "sentences": [
          "use sigmoid activation instead of softmax.",
          "set a lower learning rate in adam ; to do that you need to create the optimizer separately like adam = adam ( 0.0001 ) and pass it in model.compile (..., optimizer = adam ). "
        ]
      },
      {
        "title": "41488871",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.92",
        "t_prob": "0.74",
        "r_prob": "0.99",
        "prob": "0.673992",
        "sentences": [
          "batch size , as it will largely affect the training time of future experiments .. ",
          "learning rate and batch size.",
          "learning rate and number of neurons.",
          "( source ). ",
          "for the learning rate with adam and rmsprop , i found values around 0.001 to be optimal for most problems."
        ]
      },
      {
        "title": "48753734",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.79",
        "t_prob": "0.86",
        "r_prob": "0.98",
        "prob": "0.665812",
        "sentences": [
          "the none dimension allows you to vary the batch size during training , instead of being limited to a size number such as 128."
        ]
      },
      {
        "title": "48433743",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.79",
        "t_prob": "0.89",
        "r_prob": "0.88",
        "prob": "0.6187280000000001",
        "sentences": [
          "lower your learning rate after validation error plateaus , this typically improves performance some each time you lower it."
        ]
      },
      {
        "title": "56916577",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.87",
        "t_prob": "0.71",
        "r_prob": "1.0",
        "prob": "0.6176999999999999",
        "sentences": [
          "lowered the learning rate and increased my batch size from 32 to 128 .. "
        ]
      },
      {
        "title": "64023626",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.89",
        "t_prob": "0.69",
        "r_prob": "1.0",
        "prob": "0.6141",
        "sentences": [
          "the learning rate was either too small ( if the error curve is flat or decreasing very slowly ) or too large ( if it oscillates around this point ) "
        ]
      },
      {
        "title": "49053478",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.86",
        "t_prob": "0.7",
        "r_prob": "1.0",
        "prob": "0.602",
        "sentences": [
          "in order to tackle this problem , you can reduce the learning rate."
        ]
      },
      {
        "title": "52813871",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.97",
        "t_prob": "0.62",
        "r_prob": "1.0",
        "prob": "0.6013999999999999",
        "sentences": [
          "if you use a constant learning rate : ( 1 ) with large value - then it will less probable to converge ; ( 2 ) with small value - it may take a long time to converge .. "
        ]
      },
      {
        "title": "42696199",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.79",
        "r_prob": "0.85",
        "prob": "0.56406",
        "sentences": [
          "maybe you are not using a high enough batch size.",
          "so each neurons \" part \" in the error doubles.",
          "by using the same learning rate the gradient updates double.",
          "therefore you have the same problem as if you had used a larger learning rate in the first place.",
          "by lowering the learning rate the updates are again in the range , which you had previously used."
        ]
      },
      {
        "title": "63625455",
        "h": "learning rate",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.61",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.5612",
        "sentences": [
          "increase the batch size.",
          "decrease the learning rate , you should use scheduler .. "
        ]
      },
      {
        "title": "48867709",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.63",
        "r_prob": "1.0",
        "prob": "0.5355",
        "sentences": [
          "this paper researches the relation of batch size and learning rate.",
          "instead of decaying the learning rate , they increase the batch size by the same factor.",
          "in short , if you use a bigger batch size , you can use a larger learning rate to reduce training time."
        ]
      },
      {
        "title": "49271865",
        "h": "learning rate",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.6",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.534",
        "sentences": [
          "hence , when you decide to change the mini - batch size , you can expect the same learning rate as before to still work well."
        ]
      },
      {
        "title": "49924566",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.82",
        "t_prob": "0.63",
        "r_prob": "1.0",
        "prob": "0.5166",
        "sentences": [
          "traditionally , the steps per epoch is calculated as train_length // batch_size , since this will use all of the data points , one batch size worth at a time.",
          ". "
        ]
      },
      {
        "title": "45668368",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.89",
        "t_prob": "0.58",
        "r_prob": "0.99",
        "prob": "0.511038",
        "sentences": [
          "this includes the optimizer ( which should include the learning rate and the batch size ). "
        ]
      },
      {
        "title": "62588661",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.73",
        "r_prob": "0.82",
        "prob": "0.5088099999999999",
        "sentences": [
          "b ) try to use learning rate scheduler after some epochs change the learning rate using scheduler it will decrease the loss and increase accuracy.",
          "d ) try to use different batch size ",
          "increase your dataset."
        ]
      },
      {
        "title": "40102907",
        "h": "batch size",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.8",
        "t_prob": "0.6",
        "r_prob": "1.0",
        "prob": "0.48",
        "sentences": [
          "try picking a smaller learning rate to reduce the amount of change per step."
        ]
      },
      {
        "title": "43805871",
        "h": "learning rate",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.55",
        "t_prob": "0.96",
        "r_prob": "0.87",
        "prob": "0.45936000000000005",
        "sentences": [
          "it seems like i have used too large learning rate.",
          "batch size - 64."
        ]
      }
    ],
    "activation function": [
      {
        "title": "62817087",
        "h": "learning rate",
        "t": "activation function",
        "r": "S2",
        "h_prob": "0.75",
        "t_prob": "0.78",
        "r_prob": "1.0",
        "prob": "0.585",
        "sentences": [
          "i would use relu instead of sigmoid as the activation function.",
          "try a smaller learning rate.",
          "actually i find i get the best results using a variable learning rate."
        ]
      },
      {
        "title": "63242757",
        "h": "learning rate",
        "t": "activation function",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.57",
        "r_prob": "1.0",
        "prob": "0.48449999999999993",
        "sentences": [
          "you have the wrong activation function.",
          "for multi - class problems , use ' softmax '.. ",
          "your optimizer ' s learning rate is a little too high , so the step is too high and jumping all over the cost function."
        ]
      },
      {
        "title": "54031459",
        "h": "learning rate",
        "t": "activation function",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.72",
        "r_prob": "0.76",
        "prob": "0.415872",
        "sentences": [
          "reduce learning rate if your learning rate is too high you might converge in non - sufficient optima , which also tend to optimize for gray , black or white predictions only.",
          "the activation function causes the values to vanish ! "
        ]
      }
    ],
    "neural network": [
      {
        "title": "45696327",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.9",
        "t_prob": "0.85",
        "r_prob": "1.0",
        "prob": "0.765",
        "sentences": [
          "i have faced a similar issue with the weight histograms in my neural network."
        ]
      },
      {
        "title": "46388899",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.86",
        "r_prob": "1.0",
        "prob": "0.6708000000000001",
        "sentences": [
          "your learning rate is too big ( try starting with 1e - 3 ). "
        ]
      },
      {
        "title": "46628573",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.71",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.6673999999999999",
        "sentences": [
          "if you only have 65 / 70 % accuracy on your training data that is really poor and indicates your neural network is not converging properly."
        ]
      },
      {
        "title": "63242757",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.74",
        "r_prob": "0.98",
        "prob": "0.61642",
        "sentences": [
          "your optimizer ' s learning rate is a little too high , so the step is too high and jumping all over the cost function."
        ]
      },
      {
        "title": "64227877",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.72",
        "t_prob": "0.74",
        "r_prob": "1.0",
        "prob": "0.5327999999999999",
        "sentences": [
          "this tends to be application - specific and not every problem can benefit from retraining the whole neural network.",
          "my advice is to keep lowering learning rate until it starts improving instead of damaging the weights but this may lead to such a small learning rate that it will be of no practical use."
        ]
      },
      {
        "title": "60393907",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.65",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.5265000000000001",
        "sentences": [
          "your neural network is ridiculously small , and that is paired with a ridiculously small learning rate."
        ]
      },
      {
        "title": "38575943",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.59",
        "t_prob": "0.74",
        "r_prob": "0.94",
        "prob": "0.410404",
        "sentences": [
          "use a smaller learning rate ( 1e - 3 to 1e - 5 ). ",
          "use more layers.",
          "follow the xor neural network architecture."
        ]
      }
    ],
    "neural networks": [
      {
        "title": "50843787",
        "h": "learning rate",
        "t": "neural networks",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.83",
        "r_prob": "1.0",
        "prob": "0.6971999999999999",
        "sentences": [
          "reference : dropout : a simple way to prevent neural networks from overfitting "
        ]
      },
      {
        "title": "47916386",
        "h": "learning rate",
        "t": "neural networks",
        "r": "S2",
        "h_prob": "0.79",
        "t_prob": "0.75",
        "r_prob": "0.99",
        "prob": "0.5865750000000001",
        "sentences": [
          "dropout : a simple way to prevent neural networks from overfitting."
        ]
      },
      {
        "title": "51188348",
        "h": "learning rate",
        "t": "neural networks",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.92",
        "r_prob": "0.54",
        "prob": "0.4024080000000001",
        "sentences": [
          "though some times this large percentage of rate argument is feasible , some times it hinders the learning rate of neural networks."
        ]
      }
    ],
    "convergence": [
      {
        "title": "62044177",
        "h": "learning rate",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.86",
        "r_prob": "0.96",
        "prob": "0.685248",
        "sentences": [
          "most probably , it has to do with momentum for convergence for this data.",
          "it is very slow , maybe you ' ll need to train for more epochs with higher learning rate."
        ]
      },
      {
        "title": "61556937",
        "h": "learning rate",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.72",
        "r_prob": "1.0",
        "prob": "0.6552",
        "sentences": [
          "generally the idea is the bigger the faster the convergence.",
          "change your learning rate or use the callbacks api of tensorflow for learning decay .. "
        ]
      },
      {
        "title": "45944700",
        "h": "learning rate",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.87",
        "t_prob": "0.71",
        "r_prob": "1.0",
        "prob": "0.6176999999999999",
        "sentences": [
          "there is a chance that your learning rate is too big too."
        ]
      },
      {
        "title": "61294931",
        "h": "learning rate",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.68",
        "t_prob": "0.58",
        "r_prob": "0.91",
        "prob": "0.35890400000000006",
        "sentences": [
          "this sounds related to the learning rate.",
          "you ' ll get faster convergence if your original learning rate was too low."
        ]
      },
      {
        "title": "40102907",
        "h": "learning rate",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.6",
        "t_prob": "0.58",
        "r_prob": "1.0",
        "prob": "0.348",
        "sentences": [
          "try picking a smaller learning rate to reduce the amount of change per step.",
          "consequently you may also need to increase the number of steps by some factor , depending on your model and data , to get to the same convergence."
        ]
      },
      {
        "title": "64050327",
        "h": "convergence",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.55",
        "t_prob": "0.62",
        "r_prob": "1.0",
        "prob": "0.341",
        "sentences": [
          "this allows you to use a larger learning rate initially and have it decrease as needed so convergence will be faster."
        ]
      }
    ],
    "rmsprop": [
      {
        "title": "54579638",
        "h": "learning rate",
        "t": "rmsprop",
        "r": "S2",
        "h_prob": "0.73",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.6716",
        "sentences": [
          "you can do this by creating your own optimizer with a different learning rate.",
          "the rmsprop optimizer defaults to a learning rate of 0.001.",
          "if your images are in [ 0 , 1 ] then i suggest trying a higher learning rate , maybe 0.1."
        ]
      },
      {
        "title": "52356306",
        "h": "learning rate",
        "t": "rmsprop",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.84",
        "r_prob": "1.0",
        "prob": "0.6552",
        "sentences": [
          "you could also set a large initial learning rate and anneal it over the course of several training epochs by employing keras ' s learningratescheduler callback and defining a custom learning rate schedule ( for sgd ). "
        ]
      },
      {
        "title": "41488871",
        "h": "rmsprop",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.74",
        "r_prob": "0.99",
        "prob": "0.6153839999999999",
        "sentences": [
          "learning rate and batch size.",
          "learning rate and number of neurons.",
          "for the learning rate with adam and rmsprop , i found values around 0.001 to be optimal for most problems."
        ]
      }
    ],
    "machine learning": [
      {
        "title": "46878756",
        "h": "learning rate",
        "t": "machine learning",
        "r": "S2",
        "h_prob": "0.89",
        "t_prob": "0.93",
        "r_prob": "0.77",
        "prob": "0.6373290000000001",
        "sentences": [
          "in machine learning , placeholders are usually used for nodes that hold data , because we may want to run the same graph again and again , in a loop , with different parts of our dataset.",
          "people also use placeholders for parameters that change during training , like the learning rate."
        ]
      },
      {
        "title": "39315068",
        "h": "machine learning",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.74",
        "r_prob": "0.87",
        "prob": "0.54723",
        "sentences": [
          "picking a good learning rate is often the first challenge when implementing or using a machine learning algorithm.",
          "getting increased loss values instead of converging to a minimum is usually a sign that learning rate is too high."
        ]
      },
      {
        "title": "46844258",
        "h": "machine learning",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.64",
        "r_prob": "0.53",
        "prob": "0.26457600000000003",
        "sentences": [
          "just a note : since in every machine learning framework dropout is implemented in its \" inverted \" version , you should have to lower your learning rate in order to overcome the \" boost \" that the dropout probability gives to the learning rate."
        ]
      }
    ],
    "gradient descent": [
      {
        "title": "56360009",
        "h": "learning rate",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.7",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.63",
        "sentences": [
          "the learning rate is analogous with the size of the step."
        ]
      },
      {
        "title": "57284025",
        "h": "learning rate",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.7",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.63",
        "sentences": [
          "the learning rate is analogous with the size of the step."
        ]
      },
      {
        "title": "53549555",
        "h": "learning rate",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.74",
        "t_prob": "0.82",
        "r_prob": "0.98",
        "prob": "0.594664",
        "sentences": [
          "this is because gradient descent is an iterative technique."
        ]
      },
      {
        "title": "50717404",
        "h": "gradient descent",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.71",
        "r_prob": "0.9",
        "prob": "0.48563999999999996",
        "sentences": [
          "your initial learning rate for the gradient descent is just too big for it to converge toward a minima ( see for instance this other thread about gradient descent and learning rate values : \" gradient descent explodes if learning rate is too large \"). "
        ]
      },
      {
        "title": "35050158",
        "h": "learning rate",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.72",
        "r_prob": "0.99",
        "prob": "0.449064",
        "sentences": [
          "if you use online gradient descent , it can be as simple as using a larger / smaller learning rate when seeing imbalanced examples."
        ]
      },
      {
        "title": "39315068",
        "h": "gradient descent",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.79",
        "t_prob": "0.74",
        "r_prob": "0.58",
        "prob": "0.339068",
        "sentences": [
          "you are overflowing float32 because the learning rate is too high for your problem , and instead of converging the weight variable ( w ) is oscillating towards larger and larger magnitudes on each step of gradient descent."
        ]
      },
      {
        "title": "33719722",
        "h": "learning rate",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.6",
        "t_prob": "0.74",
        "r_prob": "0.71",
        "prob": "0.31523999999999996",
        "sentences": [
          "he already suggested to decrease the learning rate.",
          "gradient descent is the most basic algorithm."
        ]
      },
      {
        "title": "41442953",
        "h": "learning rate",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.9",
        "r_prob": "0.52",
        "prob": "0.29484000000000005",
        "sentences": [
          "so yes , it does not make much sense to use exponential decay on adamoptimizer but on gradient descent or momentum optimizer."
        ]
      },
      {
        "title": "46655987",
        "h": "gradient descent",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.66",
        "r_prob": "0.64",
        "prob": "0.266112",
        "sentences": [
          "for what concerns adagrad , let me remember what a standard gradient descent update step looks like : ",
          "often , people use a particular scheduling for the learning rate eta because they need a larger eta in the initial phase of learning , while a smaller eta in the final phase ( when you are very close to the minimum and you want to avoid oscillating around it ). "
        ]
      }
    ],
    "minimum": [
      {
        "title": "53536568",
        "h": "learning rate",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.68",
        "r_prob": "0.99",
        "prob": "0.5452920000000001",
        "sentences": [
          "likely your learning rate is too high.",
          "when the learning rate is too high , the network takes large leaps when changing the weights , and this can cause it to overshoot the local minimum it ' s approaching."
        ]
      },
      {
        "title": "33899251",
        "h": "minimum",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.86",
        "r_prob": "0.97",
        "prob": "0.525546",
        "sentences": [
          "increase it to something like 2000 at a minimum ( running 20 times through your dataset ). ",
          "that ' s the issue , your learning rate was too high for the loss function you were using."
        ]
      },
      {
        "title": "39234296",
        "h": "learning rate",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.8",
        "t_prob": "0.71",
        "r_prob": "0.78",
        "prob": "0.44304",
        "sentences": [
          "then the learning rate is decayed.",
          "to me it sounds like the learning rate was too high initially , and it got stuck in a local minimum afterwards.",
          "decaying the learning rate at that point , once it ' s already stuck in a local minimum , is not going to help it escape that minimum."
        ]
      },
      {
        "title": "51119754",
        "h": "learning rate",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.72",
        "t_prob": "0.57",
        "r_prob": "1.0",
        "prob": "0.41039999999999993",
        "sentences": [
          "and you set the learning rate is too large ( you can set lr = 0.01 or lr = 0.001 ), will be near the minimum point of shock.this is my code : cifar - 10 "
        ]
      },
      {
        "title": "64023626",
        "h": "minimum",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.69",
        "r_prob": "0.85",
        "prob": "0.36949499999999996",
        "sentences": [
          "the network is able to jump out of the previous local minimum.",
          "this indicates that it was previously stuck here , i.e.",
          "the learning rate was either too small ( if the error curve is flat or decreasing very slowly ) or too large ( if it oscillates around this point ) "
        ]
      },
      {
        "title": "50591136",
        "h": "learning rate",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.63",
        "r_prob": "0.7",
        "prob": "0.36603",
        "sentences": [
          "note that improvement from there is not guaranteed , because the model may have reached the local minimum , which may be global.",
          "there is no point to resume a model in order to search for another local minimum , unless you intent to increase the learning rate in a controlled fashion and nudge the model into a possibly better minimum not far away."
        ]
      },
      {
        "title": "35122970",
        "h": "learning rate",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.58",
        "r_prob": "0.7",
        "prob": "0.30855999999999995",
        "sentences": [
          "in practice this means your optimization will move towards a local minimum , but after getting close enough , will jump around the solution with steps proportional to the learning rate.",
          "use learning rate schedule to gradually decrease learning rate."
        ]
      }
    ],
    "learning rates": [
      {
        "title": "47242482",
        "h": "learning rates",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.74",
        "t_prob": "0.74",
        "r_prob": "0.93",
        "prob": "0.509268",
        "sentences": [
          "after playing around with different learning rates ( and compensating with more epochs ), i got the following : "
        ]
      },
      {
        "title": "37432424",
        "h": "learning rates",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.52",
        "t_prob": "0.87",
        "r_prob": "0.91",
        "prob": "0.41168400000000005",
        "sentences": [
          "vanilla sgd needs ( accepts ) individual adaption of the learning rate."
        ]
      },
      {
        "title": "59864516",
        "h": "learning rate",
        "t": "learning rates",
        "r": "S2",
        "h_prob": "0.67",
        "t_prob": "0.6",
        "r_prob": "0.99",
        "prob": "0.39798",
        "sentences": [
          "refer to this answer for more details on adaptive learning rates."
        ]
      },
      {
        "title": "58434402",
        "h": "learning rate",
        "t": "learning rates",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.59",
        "r_prob": "0.82",
        "prob": "0.30479399999999995",
        "sentences": [
          "1 ) you can use adaptive learning rate ( exponential decay or step dependent may work for you ) furthermore , you can try extreme high learning rates when your model goes into local minimum."
        ]
      },
      {
        "title": "54119442",
        "h": "learning rate",
        "t": "learning rates",
        "r": "S2",
        "h_prob": "0.62",
        "t_prob": "0.71",
        "r_prob": "0.64",
        "prob": "0.281728",
        "sentences": [
          "following the comment by @ gerges dib , i tried out different learning rates in increasing order.",
          "it looks like the plateau was caused by the optimizer ' s learning rate being too low."
        ]
      },
      {
        "title": "60787567",
        "h": "learning rates",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.51",
        "t_prob": "0.83",
        "r_prob": "0.52",
        "prob": "0.220116",
        "sentences": [
          "using big learning rates at the end of training can cause plateauing or convergence issues.",
          "in the image , you can see that for learning rate = 0.1 it reaches high accuracy very fast but then plateaus and drops in accuracy.",
          "for a learning rate = 0.001 , it reaches high accuracy slower but is continuously increasing."
        ]
      }
    ],
    "learning rate decay": [
      {
        "title": "44949185",
        "h": "learning rate decay",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.56",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.45360000000000006",
        "sentences": [
          "another strategy employed often is the learning rate decay , which reduces your learning rate by a factor every several epochs."
        ]
      },
      {
        "title": "58637810",
        "h": "learning rate decay",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.44",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.396",
        "sentences": [
          "in such case , you can either lower down the learning rate or use learning rate decay as well."
        ]
      },
      {
        "title": "51669193",
        "h": "learning rate",
        "t": "learning rate decay",
        "r": "S2",
        "h_prob": "0.77",
        "t_prob": "0.35",
        "r_prob": "1.0",
        "prob": "0.26949999999999996",
        "sentences": [
          "the sudden step down is caused by the learning rate decay happening at 40k steps ( you can find this parameter in hyper_parameters.py ). "
        ]
      },
      {
        "title": "51648360",
        "h": "learning rate",
        "t": "learning rate decay",
        "r": "S2",
        "h_prob": "0.77",
        "t_prob": "0.35",
        "r_prob": "0.99",
        "prob": "0.26680499999999996",
        "sentences": [
          "as discussed in the comments of the question , this seems to be a problem of the high learning rate decay.",
          "essentially , with every episode you multiply your learning rate by some factor j , which means that your learning rate after n episodes / epochs will be equal to lr = initial_lr * j ^ n.",
          "in your case specifically , a higher value for the decay ( i.e."
        ]
      },
      {
        "title": "64149336",
        "h": "learning rate",
        "t": "learning rate decay",
        "r": "S2",
        "h_prob": "0.61",
        "t_prob": "0.39",
        "r_prob": "0.96",
        "prob": "0.228384",
        "sentences": [
          "a similar implementation of adaptive learning rate decay ( think keras ' functions to decay learning rate on plateaus ) is right next door to the code for afo .. "
        ]
      },
      {
        "title": "43949035",
        "h": "learning rate",
        "t": "learning rate decay",
        "r": "S2",
        "h_prob": "0.77",
        "t_prob": "0.32",
        "r_prob": "0.89",
        "prob": "0.21929600000000002",
        "sentences": [
          "additionally , one needs to be careful with learning rate decay here."
        ]
      }
    ]
  },
  "convergence": {
    "learning rate": [
      {
        "title": "62044177",
        "h": "learning rate",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.86",
        "r_prob": "0.96",
        "prob": "0.685248",
        "sentences": [
          "most probably , it has to do with momentum for convergence for this data.",
          "it is very slow , maybe you ' ll need to train for more epochs with higher learning rate."
        ]
      },
      {
        "title": "61556937",
        "h": "learning rate",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.72",
        "r_prob": "1.0",
        "prob": "0.6552",
        "sentences": [
          "generally the idea is the bigger the faster the convergence.",
          "change your learning rate or use the callbacks api of tensorflow for learning decay .. "
        ]
      },
      {
        "title": "45944700",
        "h": "learning rate",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.87",
        "t_prob": "0.71",
        "r_prob": "1.0",
        "prob": "0.6176999999999999",
        "sentences": [
          "there is a chance that your learning rate is too big too."
        ]
      },
      {
        "title": "61294931",
        "h": "learning rate",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.68",
        "t_prob": "0.58",
        "r_prob": "0.91",
        "prob": "0.35890400000000006",
        "sentences": [
          "this sounds related to the learning rate.",
          "you ' ll get faster convergence if your original learning rate was too low."
        ]
      },
      {
        "title": "40102907",
        "h": "learning rate",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.6",
        "t_prob": "0.58",
        "r_prob": "1.0",
        "prob": "0.348",
        "sentences": [
          "try picking a smaller learning rate to reduce the amount of change per step.",
          "consequently you may also need to increase the number of steps by some factor , depending on your model and data , to get to the same convergence."
        ]
      },
      {
        "title": "64050327",
        "h": "convergence",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.55",
        "t_prob": "0.62",
        "r_prob": "1.0",
        "prob": "0.341",
        "sentences": [
          "this allows you to use a larger learning rate initially and have it decrease as needed so convergence will be faster."
        ]
      }
    ],
    "batch size": [
      {
        "title": "61556937",
        "h": "batch size",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.72",
        "r_prob": "1.0",
        "prob": "0.6552",
        "sentences": [
          "change your batch size.",
          "generally the idea is the bigger the faster the convergence."
        ]
      },
      {
        "title": "43768367",
        "h": "batch size",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.73",
        "r_prob": "0.99",
        "prob": "0.6142949999999999",
        "sentences": [
          "when you have a large batch size , you can have a better estimate of the gradients and vice - versa for small batch size.",
          "as far as i know , there is no fool - prof way of knowing the optimal batch size.",
          "regarding speed , my guess is that gpu is always going to win even if the batch size 20 times smaller.",
          "if you observe that batch size is affecting your validation accuracy and convergence , then you may think about shifting onto cpu."
        ]
      },
      {
        "title": "61304937",
        "h": "batch size",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.82",
        "t_prob": "0.66",
        "r_prob": "1.0",
        "prob": "0.5412",
        "sentences": [
          "yes , if you can go for the as large batch size as you can.",
          "high batch size almost always results in faster convergence , short training time."
        ]
      },
      {
        "title": "40102907",
        "h": "batch size",
        "t": "convergence",
        "r": "S2",
        "h_prob": "0.8",
        "t_prob": "0.58",
        "r_prob": "0.9",
        "prob": "0.41759999999999997",
        "sentences": [
          "consequently you may also need to increase the number of steps by some factor , depending on your model and data , to get to the same convergence."
        ]
      }
    ]
  },
  "tf.parse_example": {
    "tf.parse_single_example": [
      {
        "title": "48654046",
        "h": "tf.parse_single_example",
        "t": "tf.parse_example",
        "r": "S1",
        "h_prob": "0.49",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.46549999999999997",
        "sentences": [
          "encoding each frame as a separate feature makes it difficult to select frames dynamically , because the signature of tf.parse_example () ( and tf.parse_single_example ()) requires that the set of parsed feature names be fixed at graph construction time."
        ]
      },
      {
        "title": "45800039",
        "h": "tf.parse_example",
        "t": "tf.parse_single_example",
        "r": "S1",
        "h_prob": "0.53",
        "t_prob": "0.84",
        "r_prob": "1.0",
        "prob": "0.4452",
        "sentences": [
          "the shape error \" shape must be rank 1 but is rank 0 \" indicates that the tf.parse_example () op expects a vector ( rank 1 tensor ) as input , rather than a scalar.",
          "there are at least two possible solutions : ",
          "use the tf.parse_single_example () op , which expects a scalar input , instead .. ",
          "reshape the value returned by reader.read () into a vector , for example using tf.expand_dims ( examples_serialized , 0 ).. "
        ]
      },
      {
        "title": "49219963",
        "h": "tf.parse_example",
        "t": "tf.parse_single_example",
        "r": "S1",
        "h_prob": "0.5",
        "t_prob": "0.72",
        "r_prob": "1.0",
        "prob": "0.36",
        "sentences": [
          "the following appears to work : no errors are raised , at least.",
          "tf.parse_example ([ serialized ], ...) is used instead of tf.parse_single_example ( serialized , ...). "
        ]
      },
      {
        "title": "39376833",
        "h": "tf.parse_single_example",
        "t": "tf.parse_example",
        "r": "S1",
        "h_prob": "0.51",
        "t_prob": "0.57",
        "r_prob": "1.0",
        "prob": "0.29069999999999996",
        "sentences": [
          "and to use tf.parse_example ( which is faster than tf.parse_single_example ) you need to first batch the examples like that : "
        ]
      },
      {
        "title": "47400330",
        "h": "tf.parse_example",
        "t": "tf.parse_single_example",
        "r": "S1",
        "h_prob": "0.43",
        "t_prob": "0.63",
        "r_prob": "1.0",
        "prob": "0.2709",
        "sentences": [
          "tf.parse_example operates on a batch (\" rank 1 \"), and decode_png expects a single image ( a scalar string , \" rank 0 \"). ",
          "i ' d either use tf.parse_single_example or add a reshape to scalar ( shape =[]) before using decode_png."
        ]
      }
    ]
  },
  "tf.check_numerics": {},
  "tf.control_dependencies": {
    "tf.identity": [
      {
        "title": "37980704",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.8811",
        "sentences": [
          "there is no such function in the tensorflow api.",
          "instead you can use with tf.control_dependencies (): and tf.identity () to achieve the intended effect : "
        ]
      },
      {
        "title": "37863686",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.8118",
        "sentences": [
          "an alternative approach is to use with tf.control_dependencies ( ops ): blocks , where ops is a list of operations ( such as assignments ) that must run before the operations in the with block.",
          "the typical idiom to force a read is to use tf.identity ( var.ref ()), so the example would look something like : "
        ]
      },
      {
        "title": "49881987",
        "h": "tf.identity",
        "t": "tf.control_dependencies",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.75",
        "r_prob": "0.98",
        "prob": "0.7202999999999999",
        "sentences": [
          "when you do y = tf.identity ( x ) you are creating a new tensor , tensor_2 , which value will be the same as tensor_1.",
          "but it is a different node in your graph , so the value from tensor_1 to tensor_2 has to move.",
          "that is why the with tf.control_dependencies ([ x_plus_1 ]) does something in your second code but nothing in the first one.",
          "to sum up y = x makes the variable y to point to the same object in x , but y = tf.identity ( x ) creates a new object with the content of x."
        ]
      },
      {
        "title": "55480139",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.69",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.6692999999999999",
        "sentences": [
          "tf.control_dependencies only affects new operations created within the context.",
          "the simplest solution is to use a tf.identity operation that will produce the same result but will have the control dependencies : "
        ]
      },
      {
        "title": "44896587",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.65",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.624",
        "sentences": [
          "res3 = sess.run ( op3 ) ",
          "with dependencies ",
          "with tf.control_dependencies ([ op1 ]): ",
          "op2_after = tf.identity ( op1 ) ",
          "op3_after = tf.identity ( op1 ) "
        ]
      },
      {
        "title": "35704144",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.74",
        "t_prob": "0.83",
        "r_prob": "1.0",
        "prob": "0.6142",
        "sentences": [
          "the with tf.control_dependencies ([ op ]) block enforces control dependency on op to other ops created within with block.",
          "in your case , x * x is created outside , and the tf.identity just gets the old value."
        ]
      },
      {
        "title": "51938568",
        "h": "tf.identity",
        "t": "tf.control_dependencies",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.57",
        "r_prob": "1.0",
        "prob": "0.4673999999999999",
        "sentences": [
          "another option would be to use tf.control_dependencies which is a way to \" force \" tf to compute specific ops when it is computing other ones.",
          "we use tf.identity as a noop just to have something to wrap with control_dependencies."
        ]
      },
      {
        "title": "47252914",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.57",
        "t_prob": "0.88",
        "r_prob": "0.89",
        "prob": "0.44642399999999993",
        "sentences": [
          "now tensorflow needs to resolve the y variable.",
          "y variable is tf.identity ( t ). ",
          "tf.identity ( t ) must be executed after reset = tf.assign ( w , 0 ).. ",
          "tf.identity ( t ) references t.after executing reset , we have to resolve t , evaluate it and then exexute y .. ",
          "thus : t = tf.identity ( w ) -> only after the execution of update "
        ]
      },
      {
        "title": "44246730",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.62",
        "t_prob": "0.52",
        "r_prob": "1.0",
        "prob": "0.3224",
        "sentences": [
          "the tf.print ops can be run in any order , the strict ordering is only on the tf.identity ops.",
          "instead one has to use with tf.control_dependencies instead ( as already suggested by the op ): "
        ]
      }
    ],
    "tf.cond": [
      {
        "title": "45056367",
        "h": "tf.control_dependencies",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.7360000000000001",
        "sentences": [
          "then , if you want to return 0 when a condition is met you can ' t use python if statement ( that ' s not computed insede the tensorflow graph ) but you have to use tf.cond ( computation inside the graph ). ",
          "to force the evaluation of tf.cond after the p and r update , you can use tf.control_dependencies "
        ]
      },
      {
        "title": "57659781",
        "h": "tf.control_dependencies",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.96",
        "r_prob": "0.97",
        "prob": "0.6797759999999999",
        "sentences": [
          "interesting question.",
          "to execute a tf operation just in the first epoch , one could use tf.cond and tf.control_dependencies to check / update the value of a boolean tensor."
        ]
      },
      {
        "title": "50645142",
        "h": "tf.control_dependencies",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.55",
        "t_prob": "0.85",
        "r_prob": "0.99",
        "prob": "0.46282500000000004",
        "sentences": [
          "both optimizers compute the variable updates from the out computed with the values that the variables had before calling session.run.",
          "optimize the ( possibly weighted ) sum of both losses ( tf.train.adamoptimzer ( 0.001 ). minimize ( loss_one + loss_two )).. ",
          "use tf.control_dependencies to make sure that one optimization step always takes place after the other.",
          "however this means that using the second optimizer will always require using the first one ( could be work around , maybe with tf.cond , but it ' s more of a hassle ).. "
        ]
      }
    ]
  },
  "tf.scatter_update": {
    "tf.scatter_add": [
      {
        "title": "41455713",
        "h": "tf.scatter_add",
        "t": "tf.scatter_update",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.7084",
        "sentences": [
          "you can try tf.scatter_update or tf.scatter_add to update tensor value according to indices."
        ]
      },
      {
        "title": "41418052",
        "h": "tf.scatter_update",
        "t": "tf.scatter_add",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.76",
        "r_prob": "1.0",
        "prob": "0.7068000000000001",
        "sentences": [
          "how about tf.scatter_update ( ref , indices , updates ) or tf.scatter_add ( ref , indices , updates )? "
        ]
      },
      {
        "title": "37584068",
        "h": "tf.scatter_add",
        "t": "tf.scatter_update",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.65",
        "r_prob": "1.0",
        "prob": "0.43550000000000005",
        "sentences": [
          "tensorflow variable objects have limited support for updating slices , using the tf.scatter_update (), tf.scatter_add (), and tf.scatter_sub () ops.",
          "to update a single row of the variable , you can use tf.scatter_update (). ",
          "to chain multiple updates , you can use the mutable updated_state tensor that is returned from tf.scatter_update (): "
        ]
      }
    ]
  },
  "tf.nn.top_k": {},
  "tf.merge_all_summ": {},
  "tf.placeholder": {
    "tf.constant": [
      {
        "title": "43536220",
        "h": "tf.constant",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "tf.constant () and tf.placeholder () are nodes in the graph ( ops or operations ). "
        ]
      },
      {
        "title": "42906762",
        "h": "tf.constant",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.99",
        "r_prob": "0.99",
        "prob": "0.940896",
        "sentences": [
          "in particular , each embedding array is converted to a tf.constant () tensor , which will be quite large ( approximately 328mb by my estimate ). ",
          "the best way to avoid this is to load the variables from the previous model directly into your new model using a tf.train.saver.",
          "an alternative way to solve your problem would be to pre - create a tf.placeholder () op for assigning a value to each variable."
        ]
      },
      {
        "title": "35688187",
        "h": "tf.constant",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.94",
        "r_prob": "0.99",
        "prob": "0.874764",
        "sentences": [
          "simply create w as a tf.constant () that takes embedding as its value : ",
          "create w as a tf.variable and initialize it from the numpy array via a tf.placeholder (): "
        ]
      },
      {
        "title": "58430178",
        "h": "tf.placeholder",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.78",
        "r_prob": "0.95",
        "prob": "0.71136",
        "sentences": [
          "what you are missing is sentences_list should be passed through tf.constant or tf.placeholder depends on how you want to use it.",
          "for tf.constant use : x = tf.constant ( sentences_list ) "
        ]
      },
      {
        "title": "42633142",
        "h": "tf.constant",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.7",
        "r_prob": "1.0",
        "prob": "0.693",
        "sentences": [
          "tensorflow will apply constant propagation so that ( computed ) values that are the same in every execution of a subgraph will only be computed once.",
          "in your example , the entire expression is a constant , so tensorflow will replace it with a single tf.constant () value corresponding to the result ( 1.6 ). ",
          "if in your example x were a tf.placeholder (), the remainder of the computation could be compiled into a single kernel with one input and one output."
        ]
      },
      {
        "title": "54372304",
        "h": "tf.placeholder",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.49",
        "r_prob": "0.97",
        "prob": "0.423017",
        "sentences": [
          "like you said , this won ' t work for a varlenfeature.",
          "instead of using tf.constant try using tf.placeholder for a a fixedlenfeature and tf.sparse_placeholder for a varlenfeature."
        ]
      },
      {
        "title": "57959532",
        "h": "tf.constant",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.56",
        "t_prob": "0.97",
        "r_prob": "0.58",
        "prob": "0.315056",
        "sentences": [
          "you will not be able to do this for a ' tf.constant ()', as it is a constant variable and does not support having its values changed.",
          "if you want to change values within tensorflow data structures it is best to either pass values to a tf.placeholder or use a tf.variable."
        ]
      },
      {
        "title": "47257999",
        "h": "tf.placeholder",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.62",
        "r_prob": "0.53",
        "prob": "0.30559800000000004",
        "sentences": [
          "you defined keep_prob as a tf.constant , but then trying to feed the value into it.",
          "replace keep_prob = tf.constant ( 1.0 ) with keep_prob = tf.placeholder ( tf.float32 ,[]) or keep_prob = tf.placeholder_with_default ( 1.0 ,[]) "
        ]
      }
    ],
    "tf.variable": [
      {
        "title": "44371483",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9408",
        "sentences": [
          "the tensors v1 : 0 and v2 : 0 were created from tf.placeholder () ops , whereas only tf.variable objects are added to the \" variables \" ( or \" trainable_variables \") collections.",
          "there is no general collection to which tf.placeholder () ops are added , so your options are : ",
          "add the tf.placeholder () ops to a collection ( using tf.add_to_collection () when constructing the original graph.",
          "use [ x for x in tf.get_default_graph (). get_operations () if x.type == \" placeholderv2 \"] to get a list of placeholder ops after you import the metagraph."
        ]
      },
      {
        "title": "43536220",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.9405",
        "sentences": [
          "tf.constant () and tf.placeholder () are nodes in the graph ( ops or operations ). ",
          "on the other hand tf.variable () is a class."
        ]
      },
      {
        "title": "40533137",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9216",
        "sentences": [
          "to change the value that your tensorflow program uses in the loop , you have two main choices : ( 1 ) using a tf.placeholder () to feed in a value , or ( 2 ) using a tf.variable to store the value between steps , and tf.variable.assign () to update it."
        ]
      },
      {
        "title": "46369727",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.94",
        "r_prob": "0.98",
        "prob": "0.8659279999999999",
        "sentences": [
          "actually , it ' s pretty straightforward.",
          "a tf.variable constructor has an argument trainable.",
          "when you specify this kind of dependency : loss = sigmod ( theta * x ), a tf.variable is created under the hood and of course it ' s trainable.",
          "a tf.placeholder only adds a node to the graph."
        ]
      },
      {
        "title": "35749825",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "0.91",
        "prob": "0.856128",
        "sentences": [
          "you have to change your placeholder is_train = tf.placeholder ( tf.int32 ) for a tf.variable : is_train = tf.variable ( true , name =' training '). "
        ]
      },
      {
        "title": "36240769",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.816",
        "sentences": [
          "unfortunately you can ' t currently use the feed / tf.placeholder () mechanism to pass the result of one tensorflow graph to another."
        ]
      },
      {
        "title": "46566863",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.99",
        "r_prob": "0.86",
        "prob": "0.791802",
        "sentences": [
          "in your first code snippet , you basically define y = tf.placeholder ( tf.float32 ) + tf.variable ([ 1.0 , 1.0 , 1.0 ], tf.float32 ). ",
          "in your second example , you define y = tf.placeholder ( tf.float32 ) + tf.assign ( tf.variable ([ 1.0 , 1.0 , 1.0 ], tf.float32 ), [ 4.0 , 4.0 , 4.0 ]). ",
          "so , no matter which value you assign to c , the computation graph contains the assign operation and will always assign [ 4.0 , 4.0 , 4.0 ] to it before computing the sum."
        ]
      },
      {
        "title": "40649185",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.92",
        "r_prob": "0.99",
        "prob": "0.7377480000000001",
        "sentences": [
          "subclasses of tf.train.optimizer ) operate on tf.variable objects because they need to be able to assign new values to those objects , and in tensorflow only variables support an assign operation.",
          "if you use a tf.placeholder (), there ' s nothing to update , because the value of a placeholder is immutable within each step.",
          "instead of feeding a tf.placeholder (), you could first assign a fed - in value to a variable and then optimize with respect to it : ",
          "you could use the lower - level tf.gradients () function to get the gradient of the loss with respect to a placeholder in a single step.",
          "ps.",
          "the code in your question , where you define a tf.variable ( tf.placeholder (...), ...) is just defining a variable whose initial value is fed by the placeholder."
        ]
      },
      {
        "title": "57098377",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.81",
        "r_prob": "0.99",
        "prob": "0.7056720000000001",
        "sentences": [
          "maybe you should take a look at this question : what ' s the difference between tf.placeholder and tf.variable ? "
        ]
      },
      {
        "title": "57959532",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.88",
        "r_prob": "0.81",
        "prob": "0.691416",
        "sentences": [
          "if you want to change values within tensorflow data structures it is best to either pass values to a tf.placeholder or use a tf.variable."
        ]
      },
      {
        "title": "45016895",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.84",
        "r_prob": "0.99",
        "prob": "0.640332",
        "sentences": [
          "other approach to do same is to define variables tf.variable and in this case you have to provide an initial value when you declare it.",
          "conclusion : ",
          "use tf.variable for trainable variables such as weights ( w ) and biases ( b ) for your model or when initial values are required in general.",
          "tf.placeholder allows you to create operations and build computation graph , without needing the data."
        ]
      },
      {
        "title": "42718577",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.95",
        "r_prob": "0.7",
        "prob": "0.60515",
        "sentences": [
          "in second case there ' s a feed list , first step 1 and 2 will be added , next 3 and 4 ( a and b ). ",
          "relevant reads : ",
          "tf.placeholder doc.",
          ". ",
          "tf.variable doc.",
          ". ",
          "variable vs placeholder .. "
        ]
      },
      {
        "title": "43693704",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.69",
        "r_prob": "0.98",
        "prob": "0.601818",
        "sentences": [
          "the most obvious difference between the tf.variable and the tf.placeholder is that ",
          "placeholder is a function , variable is a class ( hence an uppercase ). ",
          "when you use tf in a distributed environment , variables are stored in a special place ( parameter server ) and are shared between the workers .. "
        ]
      },
      {
        "title": "45003763",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.69",
        "r_prob": "0.98",
        "prob": "0.601818",
        "sentences": [
          "the most obvious difference between the tf.variable and the tf.placeholder is that ",
          "placeholder is a function , variable is a class ( hence an uppercase ). ",
          "when you use tf in a distributed environment , variables are stored in a special place ( parameter server ) and are shared between the workers .. "
        ]
      },
      {
        "title": "45366164",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.84",
        "r_prob": "0.82",
        "prob": "0.599256",
        "sentences": [
          "it ' s never a requirement unless you are using a declared tf.variable or tf.placeholder from within your tensorflow session run."
        ]
      },
      {
        "title": "42718405",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.74",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.5846",
        "sentences": [
          "a little background : ",
          "above the you just initialized x and y using tf.placeholder (), and b and w as tf.variable (). ",
          "is it ok to keep passing the same x and y in every time ? ",
          "for every step x and y will be different i.e."
        ]
      },
      {
        "title": "55372251",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.72",
        "r_prob": "0.99",
        "prob": "0.555984",
        "sentences": [
          "you just need to modify the code a little bit.",
          "the value of tf.variable should not be tf.placeholder , otherwise it will cause your initialization error when running sess.run ( tf.global_variables_initializer ()). "
        ]
      },
      {
        "title": "35904439",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.73",
        "r_prob": "0.88",
        "prob": "0.5267679999999999",
        "sentences": [
          "the easiest way to achieve this would be to create a tf.variable of the appropriate type and shape by initializing it from a tf.placeholder (), then use the feed mechanism to pass in the value."
        ]
      },
      {
        "title": "35688187",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.66",
        "r_prob": "0.76",
        "prob": "0.471504",
        "sentences": [
          "since embedding can be very large , you should only use this approach for toy examples.",
          "create w as a tf.variable and initialize it from the numpy array via a tf.placeholder (): ",
          "this avoid storing a copy of embedding in the graph , but it does require enough memory to keep two copies of the matrix in memory at once ( one for the numpy array , and one for the tf.variable ). "
        ]
      },
      {
        "title": "42991444",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.63",
        "r_prob": "0.83",
        "prob": "0.44446499999999994",
        "sentences": [
          "it ' s unclear to me why you needed to change x to a tf.variable when you are continuing to feed a value for it.",
          "there are two workarounds ( not counting the case where you could just revert x to being tf.placeholder () as in the working code ): ",
          "this latter version would allow you to perform gradient descent on the contents of the image ( which might be why you ' d want to store it in a variable in the first place ). "
        ]
      },
      {
        "title": "51694027",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.65",
        "r_prob": "1.0",
        "prob": "0.44200000000000006",
        "sentences": [
          "define a placeholder lookup_ids = tf.placeholder ([ 10 ]). ",
          "define a embedding layer embeddings = tf.variable ([ 100 , 10 ],...). "
        ]
      },
      {
        "title": "36703529",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.64",
        "t_prob": "0.76",
        "r_prob": "0.9",
        "prob": "0.43776",
        "sentences": [
          "the difference is that with tf.variable you have to provide an initial value when you declare it.",
          "with tf.placeholder you don ' t have to provide an initial value and you can specify it at run time with the feed_dict argument inside session.run "
        ]
      }
    ],
    "tf.int32": [
      {
        "title": "35749825",
        "h": "tf.int32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.98",
        "prob": "0.931588",
        "sentences": [
          "you have to change your placeholder is_train = tf.placeholder ( tf.int32 ) for a tf.variable : is_train = tf.variable ( true , name =' training '). "
        ]
      },
      {
        "title": "38453485",
        "h": "tf.int32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.99",
        "r_prob": "0.98",
        "prob": "0.911988",
        "sentences": [
          "a = tf.placeholder ( tf.int32 , shape =[ none , 100 ]) a is a placeholder , not a variable."
        ]
      },
      {
        "title": "40274013",
        "h": "tf.int32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9117999999999999",
        "sentences": [
          "tf.placeholder ( tf.int32 , shape =[ none , none , seq_len ]) ( replacing seq_len with none if appropriate ). "
        ]
      },
      {
        "title": "39886448",
        "h": "tf.int32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.98",
        "r_prob": "0.98",
        "prob": "0.854756",
        "sentences": [
          "use ",
          "tf.placeholder ( tf.int32 ) "
        ]
      },
      {
        "title": "34428867",
        "h": "tf.int32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.96",
        "r_prob": "0.98",
        "prob": "0.84672",
        "sentences": [
          "the main issue is that you ' re using a single tensor for inputs and outputs , as in : inputs = tf.placeholder ( tf.int32 , [ batch_size , num_steps ]). ",
          "in tensorflow the rnn functions take a list of tensors ( because num_steps can vary in some models ). ",
          "so you should construct inputs like this : inputs = [ tf.placeholder ( tf.int32 , [ batch_size , 1 ]) for _ in xrange ( num_steps )] "
        ]
      },
      {
        "title": "44891572",
        "h": "tf.int32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.97",
        "r_prob": "0.97",
        "prob": "0.837401",
        "sentences": [
          "labels expects the input to be [ batch_size , num_classes ] but you are feeding it [ batch_size ]. ",
          "change to labels = tf.placeholder ( tf.int32 , [ none ]) and use tf.one_hot ( labels , num_classes ) when you pass it to the tf.nn.softmax_cross_entropy_with_logits () function."
        ]
      }
    ],
    "tf.tensor": [
      {
        "title": "36240769",
        "h": "tf.placeholder",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.931392",
        "sentences": [
          "tl ; dr : you can ' t feed a tf.tensor object ( viz.",
          "unfortunately you can ' t currently use the feed / tf.placeholder () mechanism to pass the result of one tensorflow graph to another."
        ]
      },
      {
        "title": "51228411",
        "h": "tf.tensor",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.765",
        "sentences": [
          "the reason this was occuring is that every time you called model you were actually creating a bunch of tf.tensor objects , which you were attempting to add to the graph.",
          "you must create a tf.placeholder in your model graph , and load the value you want your model to process onto that placeholder."
        ]
      },
      {
        "title": "35375029",
        "h": "tf.tensor",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.87",
        "r_prob": "0.82",
        "prob": "0.6777299999999999",
        "sentences": [
          "the reason for the error is that tf.reshape () expects a value that is convertible to a tf.tensor as its second argument.",
          "tensorflow will automatically convert a list of python numbers to a tf.tensor but will not automatically convert a mixed list of numbers and tensors ( such as a tf.placeholder ())— instead raising the somewhat unintuitive error message you saw."
        ]
      },
      {
        "title": "42585542",
        "h": "tf.placeholder",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.59",
        "r_prob": "0.8",
        "prob": "0.45311999999999997",
        "sentences": [
          "as you suspect , there are some downsides to using variable - shaped tf.placeholder () ops to represent input in a tensorflow model : ",
          "for example , a call to tf.shape ( x ) returns a tf.tensor containing the true dynamic shape of a tensor x.",
          "as an extreme case , the xla compiler requires that all input tensor shapes be fully defined before code is generated , so that it can generate much more efficient kernel code where array bounds ( etc .) ",
          "xla recompiles the kernel code for each combination of input shapes , so using fixed - size tensors will avoid the recompilation overhead.",
          "however , the underlying memory allocators ( the bfc allocator for gpu memory , and tcmalloc or jemalloc for cpu memory ) tend to perform better if they have a static distribution of allocation requests ( since the requests can be satisfied from buffers that were recently freed ). "
        ]
      }
    ],
    "tf.float32": [
      {
        "title": "46892382",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.912285",
        "sentences": [
          "your placeholder : 0 is tfkids = tf.placeholder ( tf.float32 , [ pop_size , dna_size ]). ",
          "your kids variable , instead , has shape = ( 10 ). "
        ]
      },
      {
        "title": "35382604",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "0.95",
        "prob": "0.9030699999999999",
        "sentences": [
          "it sounds like you have defined input_y — which i am assuming is a tf.placeholder ()— as having type tf.int32.",
          "either change this to tf.float32 or add a cast : tf.cast ( input_y , tf.float32 ) or tf.to_float ( input_y ). "
        ]
      },
      {
        "title": "54201739",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.9025",
        "sentences": [
          "from your code : ",
          "x_state = tf.placeholder ( tf.float32 , shape =[ none , input_height , input_width , input_channels ]) ",
          "if you want to feed arrays in nchw format , change x_state to tf.placeholder ( tf.float32 , shape =[ none , input_channels , input_height , input_width ]) "
        ]
      },
      {
        "title": "42970364",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.902286",
        "sentences": [
          "load your test image from disk into a numpy array , vectorize and reshape it to be of size [ 1 , 784 ] because this is the shape of your input placeholder defined here : x = tf.placeholder ( tf.float32 , shape =[ none , 784 ]). "
        ]
      },
      {
        "title": "46566863",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "0.9",
        "prob": "0.87318",
        "sentences": [
          "in your first code snippet , you basically define y = tf.placeholder ( tf.float32 ) + tf.variable ([ 1.0 , 1.0 , 1.0 ], tf.float32 ). ",
          "in your second example , you define y = tf.placeholder ( tf.float32 ) + tf.assign ( tf.variable ([ 1.0 , 1.0 , 1.0 ], tf.float32 ), [ 4.0 , 4.0 , 4.0 ]). ",
          "so , no matter which value you assign to c , the computation graph contains the assign operation and will always assign [ 4.0 , 4.0 , 4.0 ] to it before computing the sum."
        ]
      },
      {
        "title": "43817958",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.8648",
        "sentences": [
          "please carefully check the datatype you feed \" x_train / y_train \" and the tensor \" x / y_label \" you defined by ' tf.placeholder (...)' ",
          "and the reason is x_train in my code is \" np.float64 \", but what i defined by tf.placeholder () is tf.float32."
        ]
      },
      {
        "title": "43606151",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.96",
        "r_prob": "0.96",
        "prob": "0.857088",
        "sentences": [
          "one probable reason could be the lack of numpy like versatility in tensorflow yet.",
          "define a placeholder in your graph , ",
          "derivative = tf.placeholder ( tf.float32 ,[ none , num_features ]) "
        ]
      },
      {
        "title": "42933116",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.96",
        "r_prob": "0.98",
        "prob": "0.856128",
        "sentences": [
          "you ' ve correctly observed the problem.",
          "the keep_prob = tf.placeholder ( tf.float32 ) tensor is unconnected to the graph that you import with tf.train.import_meta_graph (), so feeding that tensor has no effect on the inference."
        ]
      },
      {
        "title": "44276422",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.96",
        "r_prob": "0.99",
        "prob": "0.85536",
        "sentences": [
          "i.e.",
          "i created keep_prob = tf.placeholder ( tf.float32 ,) in the training file.",
          "i created another keep_prob = tf.placeholder ( tf.float32 ,) in testing file , thinking it would be the same , but it was not.",
          "i modified my code in the training file by adding the label : keep_prob = tf.placeholder ( tf.float32 , name =\" keep_prob \") "
        ]
      },
      {
        "title": "50498669",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.98",
        "r_prob": "0.94",
        "prob": "0.838292",
        "sentences": [
          "in a tensorflow model you can define a placeholder such as x = tf.placeholder ( tf.float32 ), then you will use x in your model."
        ]
      },
      {
        "title": "51872436",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.95",
        "r_prob": "0.97",
        "prob": "0.81092",
        "sentences": [
          "most of objects in tensorflow can be found with a string.",
          "when you invoke tf.placeholder ( tf.float32 ), tensorflow will do the following : ",
          "create a node with the placeholder op.",
          "add this node to default graph.",
          "you can set a name for any node , say tf.placeholder ( tf.float32 , name =' myplaceholder '), if you don ' t specify a node name , tensorflow will generate one , you can use print x.op to see the name of the op."
        ]
      },
      {
        "title": "42325639",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.8066519999999999",
        "sentences": [
          "let ' s create one : ",
          "x = tf.placeholder ( tf.float32 , [ none , 784 ]), ",
          "b = tf.variable ( tf.zeros ([ 10 ])) "
        ]
      },
      {
        "title": "48038945",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.93",
        "r_prob": "0.99",
        "prob": "0.7825949999999999",
        "sentences": [
          "x = tf.placeholder ( tf.float32 , [ none , 784 ]) "
        ]
      },
      {
        "title": "40881670",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.96",
        "r_prob": "0.92",
        "prob": "0.7507199999999999",
        "sentences": [
          "in the graph , i ' d suggest to move keep_prob = tf.placeholder ( tf.float32 ) outside of the model function to make it global."
        ]
      },
      {
        "title": "48263700",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.8",
        "r_prob": "0.98",
        "prob": "0.6115200000000001",
        "sentences": [
          "so change this line of code y_ = tf.placeholder ( tf.float32 , [ none , 1 ]). ",
          "replace this 1 with number of output classes."
        ]
      }
    ],
    "tf.matmul": [
      {
        "title": "42309513",
        "h": "tf.matmul",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.93",
        "r_prob": "0.99",
        "prob": "0.902286",
        "sentences": [
          "the main advantage of constraining the shape of a tf.placeholder () is that the ( possibly partial ) shape can be used to infer the shapes of tensors that are derived from it.",
          "... you would get a runtime error from the matmul kernel.",
          "by contrast , if you constrained the shapes in the tf.placeholder () calls as follows : ",
          "... you ' d get an error when you call tf.matmul (). ",
          "shape inference can also improve the performance at runtime."
        ]
      },
      {
        "title": "42537878",
        "h": "tf.placeholder",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.84",
        "r_prob": "0.99",
        "prob": "0.66528",
        "sentences": [
          "to fix this problem , use a different python variable name for the placeholder y and result of tf.matmul ( x , w ) + b."
        ]
      },
      {
        "title": "44735282",
        "h": "tf.matmul",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.69",
        "t_prob": "0.89",
        "r_prob": "0.99",
        "prob": "0.607959",
        "sentences": [
          "to be more detailed , in tensorflow , all the tensors or operations you define ( e.g.",
          "tf.placeholder or tf.matmul ) are defined in the tf.graph () youre working on.",
          "you might want to store them in python variable , as you did by doingx = tf.placeholder ` but that ' s not mandatory."
        ]
      }
    ],
    "tf.global_variables_initializer": [
      {
        "title": "45366164",
        "h": "tf.placeholder",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.8091",
        "sentences": [
          "it ' s never a requirement unless you are using a declared tf.variable or tf.placeholder from within your tensorflow session run.",
          "personally , i always make it a habit of running tf.global_variables_initializer (). "
        ]
      },
      {
        "title": "43693704",
        "h": "tf.global_variables_initializer",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.7654",
        "sentences": [
          "the most obvious difference between the tf.variable and the tf.placeholder is that ",
          "initialization of the variables is done with sess.run ( tf.global_variables_initializer ()). ",
          "placeholder is a function , variable is a class ( hence an uppercase ). ",
          "when you use tf in a distributed environment , variables are stored in a special place ( parameter server ) and are shared between the workers .. "
        ]
      },
      {
        "title": "45003763",
        "h": "tf.global_variables_initializer",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.7654",
        "sentences": [
          "the most obvious difference between the tf.variable and the tf.placeholder is that ",
          "initialization of the variables is done with sess.run ( tf.global_variables_initializer ()). ",
          "placeholder is a function , variable is a class ( hence an uppercase ). ",
          "when you use tf in a distributed environment , variables are stored in a special place ( parameter server ) and are shared between the workers .. "
        ]
      },
      {
        "title": "55372251",
        "h": "tf.global_variables_initializer",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.72",
        "r_prob": "1.0",
        "prob": "0.6911999999999999",
        "sentences": [
          "you just need to modify the code a little bit.",
          "the value of tf.variable should not be tf.placeholder , otherwise it will cause your initialization error when running sess.run ( tf.global_variables_initializer ()). ",
          "you can use tf.stack instead of it."
        ]
      }
    ],
    "tf.session": [
      {
        "title": "37635555",
        "h": "tf.session",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.83",
        "r_prob": "0.98",
        "prob": "0.7320599999999999",
        "sentences": [
          "if you want to add some python it should be separated from the graph and added via placeholders ( tf.placeholder ). ",
          "on the other side , in the tf.session () part , you will call the graph results through sess.run (), providing the feed_dict values ( python objects , mostly arrays ). ",
          "in the first part , you define the tensorflow graph ( compiled by tensorflow ). ",
          "this is where all the heavy calculus should be .. ",
          "in the second part , the code you run is in python but calls the tensorflow graph through sess.run (), which is run by tensorflow ( and should contain most of the calculus ). "
        ]
      },
      {
        "title": "60509694",
        "h": "tf.placeholder",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.94",
        "r_prob": "0.73",
        "prob": "0.5764079999999999",
        "sentences": [
          "your creating a new graph for each iteration (= calling xcross ). ",
          "you should redefine xcross so that it takes a tf.placeholder as input and define it outside the loop , even outside the with tf.session as sess :. "
        ]
      },
      {
        "title": "51929310",
        "h": "tf.session",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.97",
        "r_prob": "0.56",
        "prob": "0.5106080000000001",
        "sentences": [
          "they should only ever be called once and then ran repeatedly using a tf.session.",
          "you can use a tf.placeholder for the input ( file name ). "
        ]
      }
    ]
  },
  "tf.metrics.accuracy": {},
  "tf.keras.models.sequential": {},
  "tf.keras.layers.dense": {},
  "tf.scatter_nd_update": {},
  "tf.while_loop": {
    "tf.tensorarray": [
      {
        "title": "55680074",
        "h": "tf.while_loop",
        "t": "tf.tensorarray",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.931095",
        "sentences": [
          "take a look at this example that does what you want using tf.while_loop () with tf.tensorarray and tf.slice () function : "
        ]
      },
      {
        "title": "43592281",
        "h": "tf.while_loop",
        "t": "tf.tensorarray",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.7426",
        "sentences": [
          "while the tf.map_fn () does use tf.tensorarray objects internally , and a tf.tensorarray can hold objects of different size , this program won ' t work as - is because tf.map_fn () converts its tf.tensorarray result back to a tf.tensor by stacking the elements together , and it is this operation that fails.",
          "you can however implement the tf.tensorarray - based using the lower - lever tf.while_loop () op instead : "
        ]
      },
      {
        "title": "55886502",
        "h": "tf.while_loop",
        "t": "tf.tensorarray",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.78",
        "r_prob": "1.0",
        "prob": "0.7098000000000001",
        "sentences": [
          "you could do something more complicated with a tf.while_loop and tf.tensorarray , but i suspect there would be an overhead involved that would make things more expensive for small problems ( and the code complexity would be non - trivial ). "
        ]
      },
      {
        "title": "54906367",
        "h": "tf.tensorarray",
        "t": "tf.while_loop",
        "r": "S1",
        "h_prob": "0.76",
        "t_prob": "0.86",
        "r_prob": "0.72",
        "prob": "0.47059199999999995",
        "sentences": [
          "use tf.tensorarray : ",
          "or tf.scan ( which is implemented using tf.while_loop and tf.tensorarray ): "
        ]
      }
    ],
    "tf.cond": [
      {
        "title": "46701916",
        "h": "tf.while_loop",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.9216899999999999",
        "sentences": [
          "first of all , dead tensors are an implementation detail of tensorflow ' s control flow constructs : tf.cond () and tf.while_loop (). ",
          "let ' s consider the simpler tf.cond ( pred , true_fn , false_fn ) case.",
          "if pred is true , the dead tensor is sent along output_false ( and vice versa ) the tf.cond () implementation is set up so that the ops in true_fn depend on the output_true and the ops in the false_fn depend on output_false.",
          "this dead - tensor propagation ensures that only the ops in the appropriate branch will execute.",
          "how does tf.cond () stop a dead tensor from propagating all the way to the output ? ",
          "a second special op , called a merge op handles dead inputs differently.",
          "a merge op has two or more inputs , and it expects to get a dead input for all except one of the inputs ; it then forwards the not - dead input to its output.",
          "tf.cond () uses merge ops to combine the results from the true_fn and false_fn , and so the results of the taken branch are returned as the output of the overall tf.cond () subgraph."
        ]
      },
      {
        "title": "48528363",
        "h": "tf.while_loop",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.89",
        "r_prob": "0.99",
        "prob": "0.872289",
        "sentences": [
          "each stateful operation in a graph ( that is not in a tf.while_loop () or tf.cond ()) will execute exactly once per session.run () call."
        ]
      },
      {
        "title": "51509387",
        "h": "tf.while_loop",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.86",
        "r_prob": "0.99",
        "prob": "0.7577459999999999",
        "sentences": [
          "you cannot have a variable initializer inside a conditional.",
          "this is using tf.while_loop but it is the same as tf.cond for this purposes.",
          "so , one might expect that the if the condition is false , the branch returning a will not be executed ( since it is not needed ). "
        ]
      },
      {
        "title": "48333302",
        "h": "tf.while_loop",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.738",
        "sentences": [
          "this \" answer \" is an implementation of muskrat ' s tf.slice suggestion with the details of tf.while_loop worked out ( with help from how to use tf.while_loop () in tensorflow and https :// www.tensorflow.org / api_docs / python / tf / while_loop ). ",
          "disadvantages : ",
          "tf.while_loop is treacherous.",
          "the missing documentation of tf.while_loop is that tensors outside the body of the loop are only evaluated once , even if inner ops depend on them.",
          "presumably this could be accomplished with tf.cond statements and the appropriate flags passed in via feed_dict."
        ]
      },
      {
        "title": "61521741",
        "h": "tf.while_loop",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.75",
        "r_prob": "0.95",
        "prob": "0.66975",
        "sentences": [
          "this tf issue and so question was about how to dynamically catch an exception , so basically implementing try : ... except : ... in the tf graph.",
          "other tf functionality which introduce control structure are : ",
          "tf.while_loop.",
          "tf.cond.",
          "tf.cond is the answer to your question how you conditionally execute code.",
          "a bool scalar."
        ]
      },
      {
        "title": "46523259",
        "h": "tf.while_loop",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.73",
        "r_prob": "0.93",
        "prob": "0.6313770000000001",
        "sentences": [
          "if / else blocks , for , while and other python stuff should be replaced with appropriate tensorflow operations like tf.while_loop , tf.cond and so on.",
          "thus , when you are calling sess.run () you get python object ( more precisely , numpy one ). ",
          "you could either evaluate z with another sess.run () and then switch to regular python operations , or properly use tf.cond and create a subgraph based on the values of cost and z which are both tensors."
        ]
      }
    ],
    "tf.scan": [
      {
        "title": "46735740",
        "h": "tf.while_loop",
        "t": "tf.scan",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.92",
        "r_prob": "0.99",
        "prob": "0.892584",
        "sentences": [
          "the model._step () method will only be called once per model object constructed.",
          "the tf.scan () function , like the tf.while_loop () function it wraps , will call their given function ( s ) only once to build a graph with a loop in it , and then the same graph will be used for each iteration of the loop."
        ]
      },
      {
        "title": "42960494",
        "h": "tf.while_loop",
        "t": "tf.scan",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.85",
        "r_prob": "0.99",
        "prob": "0.7321049999999999",
        "sentences": [
          "as a worst - case fallback , tf.scan or tf.while_loop with a tensorarray may be somewhat faster."
        ]
      },
      {
        "title": "37551496",
        "h": "tf.while_loop",
        "t": "tf.scan",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.73",
        "r_prob": "1.0",
        "prob": "0.7153999999999999",
        "sentences": [
          "tensorflow does support cyclic computation graphs.",
          "the tf.while_loop () function allows you to specify a while loop with arbitrary subgraphs for the condition and the body of the loop , and the runtime will execute the loop in parallel.",
          "the tf.scan () function is a higher - level api that is similar to theano ' s theano.scan () function."
        ]
      },
      {
        "title": "54906367",
        "h": "tf.while_loop",
        "t": "tf.scan",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.67",
        "r_prob": "1.0",
        "prob": "0.5762",
        "sentences": [
          "or tf.scan ( which is implemented using tf.while_loop and tf.tensorarray ): "
        ]
      }
    ],
    "tf.slice": [
      {
        "title": "55680074",
        "h": "tf.while_loop",
        "t": "tf.slice",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "0.89",
        "prob": "0.872289",
        "sentences": [
          "take a look at this example that does what you want using tf.while_loop () with tf.tensorarray and tf.slice () function : "
        ]
      },
      {
        "title": "51686804",
        "h": "tf.slice",
        "t": "tf.while_loop",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.84",
        "r_prob": "0.98",
        "prob": "0.765576",
        "sentences": [
          "you are not passing any structure / tensor to receive the values of your tf.slice (...). ",
          "your lambda b should have a signature such as lambda i , res : i + 1 , ... ",
          "tensors edited through a tf.while_loop should have a fixed shape.",
          "note : regarding your particular application ( collecting all pairs of neighbor columns ), this could be done without a tf.while_loop : "
        ]
      },
      {
        "title": "48333302",
        "h": "tf.while_loop",
        "t": "tf.slice",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.86",
        "r_prob": "0.99",
        "prob": "0.6981479999999999",
        "sentences": [
          "this \" answer \" is an implementation of muskrat ' s tf.slice suggestion with the details of tf.while_loop worked out ( with help from how to use tf.while_loop () in tensorflow and https :// www.tensorflow.org / api_docs / python / tf / while_loop ). ",
          "disadvantages : ",
          "tf.while_loop is treacherous.",
          "the missing documentation of tf.while_loop is that tensors outside the body of the loop are only evaluated once , even if inner ops depend on them."
        ]
      }
    ],
    "tf.map_fn": [
      {
        "title": "43592281",
        "h": "tf.while_loop",
        "t": "tf.map_fn",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.98",
        "r_prob": "0.9",
        "prob": "0.8290799999999999",
        "sentences": [
          "while the tf.map_fn () does use tf.tensorarray objects internally , and a tf.tensorarray can hold objects of different size , this program won ' t work as - is because tf.map_fn () converts its tf.tensorarray result back to a tf.tensor by stacking the elements together , and it is this operation that fails.",
          "you can however implement the tf.tensorarray - based using the lower - lever tf.while_loop () op instead : "
        ]
      },
      {
        "title": "49717913",
        "h": "tf.map_fn",
        "t": "tf.while_loop",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.84",
        "r_prob": "0.95",
        "prob": "0.77406",
        "sentences": [
          "you can use tf.while_loop to execute an arbitrary tensorflow operation repeatedly , until some stopping condition occurs.",
          "the stopping condition is itself specified as an op.",
          "note that tf.while_loop should be used with care , as its iterations will run in parallel by default.",
          "for this reason , tf.while_loop and tf.map_fn are most commonly used when the stopping condition is not known at graph - building time.",
          "you might still want to use tf.while_loop instead of a python loop if there is a fixed but very large number of iterations , because there is a nontrivial memory cost per op."
        ]
      },
      {
        "title": "62407166",
        "h": "tf.while_loop",
        "t": "tf.map_fn",
        "r": "S1",
        "h_prob": "0.66",
        "t_prob": "0.96",
        "r_prob": "0.86",
        "prob": "0.544896",
        "sentences": [
          "there are several issues in the code , you shouldn ' t use tf.variable objects for this , those tf.map_fn are avoidable and tf.cond must always have two branches.",
          "one caveat with this function is that the tf.while_loop tries to find good h and w values for all images in the batch , but if it fails to sample a good pair of values in the 100 loop iterations even for just one of the images , then if will not apply the erasing to any image."
        ]
      },
      {
        "title": "48876666",
        "h": "tf.map_fn",
        "t": "tf.while_loop",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.85",
        "r_prob": "0.65",
        "prob": "0.51935",
        "sentences": [
          "both approaches use functional primitives : tf.map_fn and tf.foldl.",
          "these primitives are built upon tf.while_loop and tensorarray."
        ]
      }
    ],
    "tf.gradients": [
      {
        "title": "45748708",
        "h": "tf.while_loop",
        "t": "tf.gradients",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.8036",
        "sentences": [
          "the tf.gradients () function is not thread - safe.",
          "in particular , it seems that using tf.gradients () on a graph that contains control flow operations ( such as tf.while_loop ()) is more likely to run into problems if you run it concurrently.",
          "the function performs no i / o and does not call any native methods that release python ' s gil , so the execution would most likely be serialized."
        ]
      },
      {
        "title": "49705763",
        "h": "tf.while_loop",
        "t": "tf.gradients",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.7654",
        "sentences": [
          "you can ' t ever call tf.gradients inside tf.while_loop in tensorflow based on this and this , i found this out the hard way when i was trying to create conjugate gradient descent entirely into the tensorflow graph.",
          "but if i understand your model correctly , you could make your own version of an rnncell and wrap it in a tf.dynamic_rnn , but the actual cell implementation will be a little complex since you need to evaluate a condition dynamically at runtime."
        ]
      },
      {
        "title": "57677851",
        "h": "tf.while_loop",
        "t": "tf.gradients",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.91",
        "r_prob": "1.0",
        "prob": "0.7462",
        "sentences": [
          "in effect , allowing gradients on integer tensors was causing incorrectness in tf.while_loop , and there was no satisfactory way to resolve them without this change."
        ]
      }
    ]
  },
  "tf.contrib.learn.read_keyed_batch_examples": {},
  "tf.contrib.learn.read_batch_features": {},
  "tf.square": {
    "tf.reduce_sum": [
      {
        "title": "46831346",
        "h": "tf.reduce_sum",
        "t": "tf.square",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.96",
        "r_prob": "0.96",
        "prob": "0.903168",
        "sentences": [
          "it is because you sum before taking the mean , so you get the squared error and not its mean.",
          "change tf.reduce_mean ( tf.reduce_sum ( tf.square ( tf.subtract ( y , y_ )))) to tf.reduce_mean (( tf.square ( tf.subtract ( y , y_ ))) "
        ]
      },
      {
        "title": "44400232",
        "h": "tf.square",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.99",
        "r_prob": "0.95",
        "prob": "0.8464499999999999",
        "sentences": [
          "just a little bit of linear algebra and you have your solution : ",
          "which means that you need to do : tf.reduce_sum ( tf.square ( a ), axis = 0 ) "
        ]
      },
      {
        "title": "48805042",
        "h": "tf.reduce_sum",
        "t": "tf.square",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.85",
        "r_prob": "0.97",
        "prob": "0.8080099999999999",
        "sentences": [
          "however , you might want to consider changing your loss from tf.sqrt ( tf.reduce_sum ( tf.square ( imgs - decoder_op ))) to tf.reduce_sum ( tf.square ( imgs - decoder_op )). "
        ]
      },
      {
        "title": "58390952",
        "h": "tf.square",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.75",
        "t_prob": "0.94",
        "r_prob": "0.94",
        "prob": "0.6627",
        "sentences": [
          "model.add_loss ( lambda : penalty * tf.reduce_sum ( tf.square ( layer.trainable_variables [ 0 ]) "
        ]
      },
      {
        "title": "55572669",
        "h": "tf.square",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.84",
        "r_prob": "0.7",
        "prob": "0.5585999999999999",
        "sentences": [
          "we thus get , which is : tf.square ( tf.sub ( target_out [ layer ], cont_out )). ",
          "this is why we sum all the difference into a single scalar using tf.reduce_sum."
        ]
      }
    ],
    "tf.reduce_mean": [
      {
        "title": "48836872",
        "h": "tf.square",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.93",
        "prob": "0.884058",
        "sentences": [
          "the - operator is called on var inside variable_summaries : ",
          "stddev = tf.sqrt ( tf.reduce_mean ( tf.square ( var - mean ))) "
        ]
      },
      {
        "title": "57379271",
        "h": "tf.reduce_mean",
        "t": "tf.square",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.874",
        "sentences": [
          "hence the steps error = output - y , error_sq = tf.square ( error ) and loss = tf.reduce_mean ( error_sq , axis =- 1 ) might be resulting in nan."
        ]
      },
      {
        "title": "41001210",
        "h": "tf.reduce_mean",
        "t": "tf.square",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.96",
        "r_prob": "0.9",
        "prob": "0.79488",
        "sentences": [
          "following modified code works.",
          "the main problem was loss function , which should be loss = 0.5 * tf.reduce_mean ( tf.square ( tf.transpose ( logits ) - tftrainy )) "
        ]
      },
      {
        "title": "63342681",
        "h": "tf.square",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.81",
        "r_prob": "0.96",
        "prob": "0.668736",
        "sentences": [
          "further , i found tf.reduce_mean , k.mean , tf.square , tf.exp etc.",
          "implemented in a loss funtion cause the same error."
        ]
      },
      {
        "title": "42474169",
        "h": "tf.square",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.94",
        "r_prob": "0.68",
        "prob": "0.626416",
        "sentences": [
          "so you are subtracting a row from a column at this line : loss = tf.reduce_mean ( tf.square ( tf.matmul ( _a , _x ) - _b )). ",
          "by default , reduction operations in numpy and tensorflow reduce along all dimensions , so you keep getting a single number regardless dimensions of the input array."
        ]
      }
    ]
  },
  "tf.stop_gradient": {},
  "batch normalization": {
    "batch size": [
      {
        "title": "50241169",
        "h": "batch normalization",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.7",
        "t_prob": "0.9",
        "r_prob": "0.99",
        "prob": "0.6237",
        "sentences": [
          "increase batch size ",
          "batch normalization between layers."
        ]
      },
      {
        "title": "61476847",
        "h": "batch size",
        "t": "batch normalization",
        "r": "S2",
        "h_prob": "0.87",
        "t_prob": "0.69",
        "r_prob": "1.0",
        "prob": "0.6003",
        "sentences": [
          "the first things that pop into mind are early stopping callbacks and change the batch size."
        ]
      },
      {
        "title": "57489798",
        "h": "batch normalization",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.6",
        "t_prob": "0.87",
        "r_prob": "1.0",
        "prob": "0.522",
        "sentences": [
          "specifically , \" the effect of batch normalization is dependent on the mini - batch size and it is not obvious how to apply it to recurrent networks \" ( from the paper ba , et al."
        ]
      },
      {
        "title": "48034744",
        "h": "batch normalization",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.58",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.4698",
        "sentences": [
          "large batch size can break validation performance when batch normalization is used.",
          "see this."
        ]
      },
      {
        "title": "63020188",
        "h": "batch size",
        "t": "batch normalization",
        "r": "S2",
        "h_prob": "0.68",
        "t_prob": "0.6",
        "r_prob": "0.97",
        "prob": "0.39576",
        "sentences": [
          "indeed , using a very large batch size can harm generalization as there is less variation in batch statistics , decreasing regularization."
        ]
      }
    ],
    "batch norm": [
      {
        "title": "51157850",
        "h": "batch normalization",
        "t": "batch norm",
        "r": "S2",
        "h_prob": "0.72",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.5832",
        "sentences": [
          "you can use this to extract fairly easily the variables from layers that used batch norm.",
          "now that you know which layers used batch norm , for every such layer , you can extract its weights w , bias b , batch norm variance v , mean m , gamma and beta parameters."
        ]
      },
      {
        "title": "46620505",
        "h": "batch normalization",
        "t": "batch norm",
        "r": "S2",
        "h_prob": "0.72",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.576",
        "sentences": [
          "fused batch norm combines the multiple operations needed to do batch normalization into a single kernel.",
          "batch norm is an expensive process that for some models makes up a large percentage of the operation time.",
          "using fused batch norm can result in a 12 %- 30 % speedup."
        ]
      },
      {
        "title": "60861291",
        "h": "batch normalization",
        "t": "batch norm",
        "r": "S2",
        "h_prob": "0.74",
        "t_prob": "0.7",
        "r_prob": "0.99",
        "prob": "0.51282",
        "sentences": [
          "certain ops , such as batch normalization , are disabled on prediction - this can make a big difference with certain architectures , although it generally isn ' t supposed to if you ' re using batch norm correctly.",
          ". "
        ]
      },
      {
        "title": "44002219",
        "h": "batch normalization",
        "t": "batch norm",
        "r": "S2",
        "h_prob": "0.73",
        "t_prob": "0.83",
        "r_prob": "0.75",
        "prob": "0.45442499999999997",
        "sentences": [
          "the non - fused batch norm does computations using several individual ops.",
          "fused batch norm combines the individual operations into a single kernel , which runs faster.",
          "fused batch norm combines the multiple operations needed to do batch normalization into a single kernel.",
          "batch norm is an expensive process that for some models makes up a large percentage of the operation time.",
          "using fused batch norm can result in a 12 %- 30 % speedup."
        ]
      }
    ]
  },
  "tf.data": {
    "tf.keras": [
      {
        "title": "63074542",
        "h": "tf.keras",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9503999999999999",
        "sentences": [
          "if x is a tf.data dataset , and ' steps_per_epoch ' is none , the epoch will run until the input dataset is exhausted."
        ]
      },
      {
        "title": "54117754",
        "h": "tf.keras",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.8246699999999999",
        "sentences": [
          "tf.keras ( https :// www.tensorflow.org / guide / keras ) implements the keras api specification within tensorflow.",
          "in addition , the tf.keras api is optimized to work well with other tensorflow modules : you can pass a tf.data dataset to the.fit () method of a tf.keras model , for instance , or convert a tf.keras model to a tensorflow estimator with tf.keras.estimator.model_to_estimator."
        ]
      },
      {
        "title": "49970811",
        "h": "tf.keras",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.75",
        "r_prob": "0.94",
        "prob": "0.6345",
        "sentences": [
          "tf.keras ( formerly tf.contrib.keras ) is an implementation of keras 2 implemented exclusively with / for tensorflow.",
          "as a rule of thumb , if your code use any tensorflow - specific code , say anything in tf.data.",
          "* for providing inputs or tf.summary.",
          "* for visualization in tensorboard , it is simpler to just use tf.keras.",
          "if you don ' t care much about being framework - agnostic but don ' t use tensorflow - specific code , i would probably advise to go with tf.keras and start using tensorflow - specific code , esp.",
          "tf.data which is a game - changer in my opinion.",
          "i attended a talk by chollet on tf2 ( couldn ' t find a recording online ) in which he basically said that support for frameworks other than tf would eventually drop and future developments of keras would happen exclusively in tf.keras.",
          "so today , my answer would be to use tf.keras by default , and keep keras for legacy projects that would be hard to migrate -- that is the future - proof choice for keras."
        ]
      }
    ],
    "tf.data.dataset": [
      {
        "title": "62187234",
        "h": "tf.data",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.96",
        "prob": "0.8937599999999999",
        "sentences": [
          "tf.data does not use tf.placeholders under the hood.",
          "tf.data.dataset uses the same mechanisms as other tensorflow ops."
        ]
      },
      {
        "title": "48738199",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.9",
        "r_prob": "0.98",
        "prob": "0.87318",
        "sentences": [
          "the more recent tf.data library provides a tf.data.dataset.cache method to cache an entire dataset into memory or into a file."
        ]
      },
      {
        "title": "52417770",
        "h": "tf.data",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.97",
        "r_prob": "0.94",
        "prob": "0.8206199999999999",
        "sentences": [
          "the tf.data api ( tensorflow 1.4 onwards ) is great for things like this.",
          "the pipeline will looks something like the following : ",
          "create an initial tf.data.dataset object that iterates over all examples.",
          "you can also generate the tfrecords using lower level operations.",
          "load images via tf.data.dataset.map and tf.py_func ( tion ). ",
          "alternatively you can load the image files from filenames inside tf.data.dataset.map as below.",
          "if you need more custom loading functions , also check out tf.py_func."
        ]
      },
      {
        "title": "47996856",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.81",
        "r_prob": "0.96",
        "prob": "0.7698240000000001",
        "sentences": [
          "the tf.data.dataset.from_generator () method provides a way to convert python generators into tf.tensor objects that evaluate to each successive element from the generator.",
          "you can use the tf.data api to convert the generator first to a tf.data.dataset , then to a tf.data.iterator , and finally to a tuple of tf.tensor objects.",
          "see the tf.data programmer ' s guide for more ideas."
        ]
      },
      {
        "title": "64211418",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.752",
        "sentences": [
          "the tf.data.dataset is the tf2 way of setting this up.",
          "at a high - level , the tf.data api sets up a stream of examples."
        ]
      },
      {
        "title": "64119127",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.75",
        "r_prob": "0.97",
        "prob": "0.7202249999999999",
        "sentences": [
          "you don ' t need your own tfds.load , it simply returns a tf.data.dataset object , which you can easily build yourself.",
          "note : do not confuse tfds ( this library ) with tf.data ( tensorflow api to build efficient data pipelines ). ",
          "tfds is a high level wrapper around tf.data.",
          "if you ' re not familiar with this api , we encourage you to read the official tf.data guide first.",
          "read more about tf.data.dataset."
        ]
      },
      {
        "title": "56340136",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.74",
        "r_prob": "0.95",
        "prob": "0.6748799999999999",
        "sentences": [
          "you can pass tf.data.dataset iterators instead of numpy data.",
          "more information on how to work with tf.data you can find here."
        ]
      },
      {
        "title": "58231402",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.64",
        "r_prob": "0.98",
        "prob": "0.620928",
        "sentences": [
          "alongside custom defined python generators , you can wrap the imagedatagenerator from keras inside tf.data.",
          "therefore , one can still use the typical keras imagedatagenerator , you just need to wrap it into a tf.data.dataset like above."
        ]
      },
      {
        "title": "48817892",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.66",
        "r_prob": "1.0",
        "prob": "0.5874",
        "sentences": [
          "you should be able to use tf.data.dataset.shuffle to achieve what you want.",
          "add enough shuffle so that a batch of patches is diverse ( all the patches come from different images ). ",
          "don ' t load too many big images in cache.",
          "you can achieve all that using the tf.data api by doing the following steps : "
        ]
      },
      {
        "title": "47946271",
        "h": "tf.data",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.97",
        "r_prob": "0.58",
        "prob": "0.5175919999999999",
        "sentences": [
          "if you implement the same pipeline using the tf.data.dataset api and using queues , the performance of the dataset version should be better than the queue - based version.",
          "adding dataset.prefetch ( 1 ) to the end of your pipeline will give you most of the benefit of prefetching , but you might need to tune this further.",
          "in a dataset pipeline , this would be equivalent to dataset.repeat ( num_epochs ). shuffle ( n ). "
        ]
      },
      {
        "title": "37343690",
        "h": "tf.data",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.89",
        "r_prob": "0.59",
        "prob": "0.43058199999999996",
        "sentences": [
          "new answer ( with tf.data ) and with labels.",
          "with the introduction of tf.data in r1.4 , we can create a batch of images without placeholders and without queues.",
          "create a list containing the filenames of the images and a corresponding list of labels.",
          "create a tf.data.dataset reading these filenames and labels.",
          "preprocess the data.",
          "create an iterator from the tf.data.dataset which will yield the next batch.",
          "create a list of filenames ( ex : the paths to your images ). "
        ]
      }
    ]
  },
  "tf.keras.backend.clear_session": {},
  "tf.nn.bias_add": {},
  "tf.add": {
    "tf.matmul": [
      {
        "title": "36520509",
        "h": "tf.add",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "the most basic way to write a linear activation in tensorflow is using tf.matmul () and tf.add () ( or the + operator ). "
        ]
      },
      {
        "title": "35711410",
        "h": "tf.matmul",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9023999999999999",
        "sentences": [
          "see the tensorflow api for tf.nn.softmax.",
          "the only thing that is equal between the two statements is the basic calculation.",
          "+ does the same thing tf.add does so tf.add ( tf.matmul ( x , weights ), biases ) is equal to tf.matmul ( x , weights ) + biases."
        ]
      },
      {
        "title": "51088806",
        "h": "tf.matmul",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.8840699999999999",
        "sentences": [
          "each time when you call apply_feature_extraction you put a new operation tf.add ( tf.matmul (...) to your graph.",
          "as a result your graph gets bloated."
        ]
      },
      {
        "title": "51245924",
        "h": "tf.add",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.96",
        "r_prob": "0.96",
        "prob": "0.857088",
        "sentences": [
          "the function add takes 2 arguments.",
          "in the line tf.add ( tf.matmul ( data , hidden_layer_1 [' weights ']) + hidden_layer_1 [' biases ']) you are trying to use the function add and using the + too.",
          "either do tf.add ( tf.matmul ( data , hidden_layer_1 [' weights ']), hidden_layer_1 [' biases ']) or tf.matmul ( data , hidden_layer_1 [' weights ']) + hidden_layer_1 [' biases ']. "
        ]
      },
      {
        "title": "48562852",
        "h": "tf.add",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.818235",
        "sentences": [
          "each layer should be : layer = tf.add ( tf.matmul ( x , weights [ h ]), biases [ h ). ",
          "you ' re also completely missing tf.add () on your output layer , and are trying to run a matrix multiplication with three inputs , one of which are your biases."
        ]
      },
      {
        "title": "54492280",
        "h": "tf.matmul",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.6365",
        "sentences": [
          "there is absolutely no difference between using tf.layers and defining your own layers by creating w and b matricies and then doing tf.matmul and tf.add."
        ]
      },
      {
        "title": "44142142",
        "h": "tf.matmul",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.89",
        "r_prob": "0.54",
        "prob": "0.418122",
        "sentences": [
          "the + operator in tf.matmul ( x , w ) + b is actually shorthand for tf.add ( tf.matmul ( x , w ), b ) ( operator overloading ). ",
          "the documentation for tf.add mentions that it supports broadcasting , which means that when you add a tensor with shape ( 10 ) to a tensor with shape ( 100 , 10 ), it ' s the equivalent of adding the ( 10 ) tensor to each row of the ( 100 , 10 ) tensor."
        ]
      }
    ],
    "tf.multiply": [
      {
        "title": "49977599",
        "h": "tf.add",
        "t": "tf.multiply",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "c.f.",
          "tf.add () doc , or tf.multiply () doc , etc."
        ]
      },
      {
        "title": "39112432",
        "h": "tf.add",
        "t": "tf.multiply",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9409",
        "sentences": [
          "this looks like an unfortunate implementation detail of tf.random_uniform (): it currently uses tf.add () and tf.multiply () to rescale the random value from [- 1 , + 1 ] to [ minval , maxval ], but if the shape of minval or maxval is unknown , tf.add () and tf.multiply () can ' t infer the proper shapes , because there might be broadcasting involved."
        ]
      },
      {
        "title": "56833985",
        "h": "tf.add",
        "t": "tf.multiply",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.97",
        "r_prob": "0.85",
        "prob": "0.6843349999999999",
        "sentences": [
          "update : so , you want to initialize prevop with tf.add ( inp1 , 10 ) and then update it with values of op , which is tf.add ( tf.multiply ( prevop , inp2 ), inp1 ). "
        ]
      },
      {
        "title": "57469599",
        "h": "tf.multiply",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.65",
        "r_prob": "0.79",
        "prob": "0.42107000000000006",
        "sentences": [
          "then c = tf.add ( b , 2.0 ): "
        ]
      }
    ],
    "tf.tensor": [
      {
        "title": "37901852",
        "h": "tf.tensor",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.855",
        "sentences": [
          "there ' s no difference in precision between a + b and tf.add ( a , b ). ",
          "you can see from tf.tensor.overloadable_operators that following python special methods are potentially overloaded by appropriate tensorflow versions "
        ]
      },
      {
        "title": "35095052",
        "h": "tf.add",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.85",
        "r_prob": "0.99",
        "prob": "0.7910099999999999",
        "sentences": [
          "if at least one of x or y is a tf.tensor object , the expressions tf.add ( x , y ) and x + y are equivalent.",
          "the main reason you might use tf.add () is to specify an explicit name keyword argument for the created op , which is not possible with the overloaded operator version.",
          "note that if neither x nor y is a tf.tensor — for example if they are numpy arrays — then x + y will not create a tensorflow op.",
          "tf.add () always creates a tensorflow op and converts its arguments to tf.tensor objects.",
          "therefore , if you are writing a library function that might accept both tensors and numpy arrays , you might prefer to use tf.add (). ",
          "__floordiv__ ( binary // in python 3 ). "
        ]
      },
      {
        "title": "39513635",
        "h": "tf.tensor",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.7",
        "r_prob": "0.96",
        "prob": "0.6048",
        "sentences": [
          "the four examples you gave will all give the same result , and generate the same graph ( if you ignore that some of the operation names in the graph are different ). ",
          "tensorflow will convert many different python objects into tf.tensor objects when they are passed as arguments to tensorflow operators , such as tf.add () here.",
          "the + operator is just a simple wrapper on tf.add (), and the overload is used when either the left - hand or right - hand argument is a tf.tensor ( or tf.variable ). ",
          "in that case , you may wish to convert the array to a tf.tensor once ",
          "creating a tf.constant () explicitly allows you to set its name property , which can be useful for tensorboard debugging and graph visualization."
        ]
      },
      {
        "title": "37572852",
        "h": "tf.add",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.62",
        "t_prob": "0.76",
        "r_prob": "1.0",
        "prob": "0.4712",
        "sentences": [
          "here you can see a bidirectional edge because the operation reads the old value from x and then writes back the new value.",
          "a tf.tensor : the value assigned to the variable ( or added to the variable ). ",
          "in the operation tf.add ( x , y ): the graph reads the value of x as an input."
        ]
      }
    ],
    "tf.variable": [
      {
        "title": "36113230",
        "h": "tf.add",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.8188000000000001",
        "sentences": [
          "yes , - and + resolve to tf.sub ad tf.add.",
          "if you look at the tensorflow code you will see that these operators on tf.variable are overloaded with the tf."
        ]
      },
      {
        "title": "56833985",
        "h": "tf.variable",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.83",
        "r_prob": "0.98",
        "prob": "0.7971319999999998",
        "sentences": [
          "as the value of op is always changing , you can use tf.variable () to store its value after every iteration.",
          "here , tf.variable () is initialized with a zero tensor at the start.",
          "update : so , you want to initialize prevop with tf.add ( inp1 , 10 ) and then update it with values of op , which is tf.add ( tf.multiply ( prevop , inp2 ), inp1 ). "
        ]
      },
      {
        "title": "53915891",
        "h": "tf.add",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.91",
        "r_prob": "0.81",
        "prob": "0.6265350000000001",
        "sentences": [
          "this is resetting x back to its original value ( 0 ). ",
          "your code to increase x replaces ' x ' with a tf.add operation and then your summary value is no longer tracing a tf.variable but an addition operation."
        ]
      },
      {
        "title": "39513635",
        "h": "tf.add",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.7",
        "t_prob": "0.63",
        "r_prob": "0.98",
        "prob": "0.43217999999999995",
        "sentences": [
          "the four examples you gave will all give the same result , and generate the same graph ( if you ignore that some of the operation names in the graph are different ). ",
          "the + operator is just a simple wrapper on tf.add (), and the overload is used when either the left - hand or right - hand argument is a tf.tensor ( or tf.variable ). "
        ]
      },
      {
        "title": "37572852",
        "h": "tf.variable",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.61",
        "t_prob": "0.62",
        "r_prob": "0.94",
        "prob": "0.35550799999999994",
        "sentences": [
          "each operation assign or assignadd has two inputs and no output : ",
          "a tf.variable : the variable to which we assign a value.",
          "here you can see a bidirectional edge because the operation reads the old value from x and then writes back the new value.",
          "a tf.tensor : the value assigned to the variable ( or added to the variable ). ",
          "the variable x appears once in the graph in the big block named x , but is used twice : ",
          "in the operation tf.add ( x , y ): the graph reads the value of x as an input."
        ]
      }
    ],
    "tf.constant": [
      {
        "title": "49720236",
        "h": "tf.add",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.61",
        "r_prob": "0.97",
        "prob": "0.42010699999999995",
        "sentences": [
          "computed tensors ( ops / operations ) - these are computed values such as tf.add ( a , b ), the value is discarded ( or returned ) at the end of a call to sess.run.",
          "they are computed only .. ",
          "constants - e.g.",
          "tf.constant ( 12 ). ",
          "variables need to be initialized precisely because they maintain state , so they need an initial state.",
          "if you were to request the value of ans at some point in the future with sess.run ( ans ), it would simply retrieve the value of the variable and return it ( no computation performed ). "
        ]
      },
      {
        "title": "38294089",
        "h": "tf.constant",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.48",
        "t_prob": "0.76",
        "r_prob": "0.96",
        "prob": "0.350208",
        "sentences": [
          "for example tf.constant constructs a new op , but actualy returns a reference to tensor being a result of this operation , namely instance of tensorflow.python.framework.ops.tensor , but it is not a constructor in the oop sense.",
          "in particular things that have actual logic , like tf.add will create both tensorflow.python.framework.ops.operation ( to perform addition ) and tensorflow.python.framework.ops.tensor ( to store the result of the operation ), and only tensor will be returned ( this is what cited part of documentation tries to explain ). ",
          "a simple python - based example might be more usefull , so tf does something like this : "
        ]
      },
      {
        "title": "49350002",
        "h": "tf.add",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.58",
        "t_prob": "0.46",
        "r_prob": "1.0",
        "prob": "0.2668",
        "sentences": [
          "e.g.",
          "tf.constant ( 42 ) and should be specified at compile time , not runtime ( eluding to your primary mistake here ). ",
          "3 ) computed tensors - x = tf.add ( a , b ) is a computed tensor , it ' s computed from a , b.",
          "if you were passing in a vector then it ' s a rank 1 tensor ( aka a vector ). ",
          "now , nextdd fails becuase you tried to create a constant from a variable term , which isn ' t a defined operation."
        ]
      }
    ]
  },
  "tf.io.decode_jpeg": {},
  "tf.int": {
    "tf.cast": [
      {
        "title": "51714563",
        "h": "tf.int",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.776",
        "sentences": [
          "note that the error states a valueerror regarding integers , int64 and int32 , so the conversion at question should be int not float.",
          "new_var = tf.cast ( old_var , tf.int32 ) "
        ]
      },
      {
        "title": "51813781",
        "h": "tf.cast",
        "t": "tf.int",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.79",
        "r_prob": "0.83",
        "prob": "0.583573",
        "sentences": [
          "correct = tf.nn.in_top_k ( logits , tf.cast ( y , tf.int32 ), 1 ) "
        ]
      },
      {
        "title": "45053430",
        "h": "tf.cast",
        "t": "tf.int",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.52",
        "r_prob": "0.86",
        "prob": "0.406952",
        "sentences": [
          "this issue can arise when your labels were tf.int16 before it was saved in bytes in the tfrecords.",
          "so when you read , as tf.int8 it has twice the numbers you expect.",
          "so you can make sure your labels are written properly by : label = tf.cast ( y [ i ], tf.int8 ) in your tfrecords conversion code."
        ]
      }
    ]
  },
  "tf.nn.softmax produces": {},
  "tf.contrib.layers": {
    "tf.layers": [
      {
        "title": "47862519",
        "h": "tf.contrib.layers",
        "t": "tf.layers",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.94",
        "prob": "0.8935639999999999",
        "sentences": [
          "just to mention that : ",
          "there is no need to extract weights and biases just to save them.",
          "for tf.layers or tf.contrib.layers , if trainable is set to true , the weights and biases are added to graphkeys.trainable_variables , which is a subset of graphkeys.global_variables."
        ]
      },
      {
        "title": "55518654",
        "h": "tf.contrib.layers",
        "t": "tf.layers",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.8633",
        "sentences": [
          "this not the case of tf.conrib.layers.",
          "even tf.layers ( which was distilled from tf.contrib.layers ) will be no longer supported."
        ]
      },
      {
        "title": "44805926",
        "h": "tf.contrib.layers",
        "t": "tf.layers",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.85",
        "r_prob": "1.0",
        "prob": "0.816",
        "sentences": [
          "apis in this namespace are allowed to change rapidly between versions , whereas the others usually can ' t without a new major version.",
          "in particular , the functions in tf.contrib.layers are not identical to those found in tf.layers , although some of them might be replicated with different names."
        ]
      }
    ]
  },
  "tf.map_fn": {
    "tf.cond": [
      {
        "title": "44952751",
        "h": "tf.cond",
        "t": "tf.map_fn",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9408",
        "sentences": [
          "the general recipe for a problem like this is to use tf.map_fn () to create a new tensor with the appropriate value by applying a function to each row.",
          "now the we have a condition , we can use tf.cond () to build a conditional expression that swaps the first two elements if the condition is true : "
        ]
      },
      {
        "title": "49649378",
        "h": "tf.map_fn",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.93",
        "r_prob": "0.89",
        "prob": "0.802869",
        "sentences": [
          "you can do it with tf.map_fn and tf.cond like this : "
        ]
      },
      {
        "title": "62407166",
        "h": "tf.map_fn",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.58",
        "r_prob": "0.99",
        "prob": "0.5512319999999999",
        "sentences": [
          "there are several issues in the code , you shouldn ' t use tf.variable objects for this , those tf.map_fn are avoidable and tf.cond must always have two branches."
        ]
      },
      {
        "title": "42792589",
        "h": "tf.cond",
        "t": "tf.map_fn",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.55",
        "prob": "0.52283",
        "sentences": [
          "a tf.tensor in tensorflow is a read - only value — in fact , a symbolic expression for computing a read - only value — so you cannot in general assign values to it.",
          "however , since the function is simple , cheap to compute , and representable using simple tensorflow ops , you can avoid using tf.map_fn () and instead use tf.where (): ",
          "( you could also use tf.where () instead of tf.cond () in the tf.map_fn () version .) "
        ]
      }
    ],
    "tf.stack": [
      {
        "title": "51315996",
        "h": "tf.map_fn",
        "t": "tf.stack",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.8929999999999999",
        "sentences": [
          "here is another example using tf.map_fn () to cleverly iterate over a tensor , and using tf.stack () to convert the list elements back into a tensor."
        ]
      },
      {
        "title": "48138052",
        "h": "tf.map_fn",
        "t": "tf.stack",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.846",
        "sentences": [
          "in your specific example of wanting to map individually to each of the x , y , z coordinates , you can accomplish this readily with tf.split () and tf.stack (). ",
          "if so , then use tf.split () to break up k into kx , ky , kz.",
          "then apply your map operation ( i use tf.map_fn () for this purpose typically ), and then finally stack things back together with tf.stack (). "
        ]
      },
      {
        "title": "57974089",
        "h": "tf.stack",
        "t": "tf.map_fn",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.91",
        "r_prob": "0.54",
        "prob": "0.47174400000000005",
        "sentences": [
          "you can make it into a tensor for example with tf.stack or tf.convert_to_tensor : ",
          "however , you can do the same operation without tf.map_fn more simply and efficiently like this : "
        ]
      }
    ],
    "tf.while_loop": [
      {
        "title": "43592281",
        "h": "tf.while_loop",
        "t": "tf.map_fn",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.98",
        "r_prob": "0.9",
        "prob": "0.8290799999999999",
        "sentences": [
          "while the tf.map_fn () does use tf.tensorarray objects internally , and a tf.tensorarray can hold objects of different size , this program won ' t work as - is because tf.map_fn () converts its tf.tensorarray result back to a tf.tensor by stacking the elements together , and it is this operation that fails.",
          "you can however implement the tf.tensorarray - based using the lower - lever tf.while_loop () op instead : "
        ]
      },
      {
        "title": "49717913",
        "h": "tf.map_fn",
        "t": "tf.while_loop",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.84",
        "r_prob": "0.95",
        "prob": "0.77406",
        "sentences": [
          "you can use tf.while_loop to execute an arbitrary tensorflow operation repeatedly , until some stopping condition occurs.",
          "the stopping condition is itself specified as an op.",
          "note that tf.while_loop should be used with care , as its iterations will run in parallel by default.",
          "for this reason , tf.while_loop and tf.map_fn are most commonly used when the stopping condition is not known at graph - building time.",
          "you might still want to use tf.while_loop instead of a python loop if there is a fixed but very large number of iterations , because there is a nontrivial memory cost per op."
        ]
      },
      {
        "title": "62407166",
        "h": "tf.while_loop",
        "t": "tf.map_fn",
        "r": "S1",
        "h_prob": "0.66",
        "t_prob": "0.96",
        "r_prob": "0.86",
        "prob": "0.544896",
        "sentences": [
          "there are several issues in the code , you shouldn ' t use tf.variable objects for this , those tf.map_fn are avoidable and tf.cond must always have two branches.",
          "one caveat with this function is that the tf.while_loop tries to find good h and w values for all images in the batch , but if it fails to sample a good pair of values in the 100 loop iterations even for just one of the images , then if will not apply the erasing to any image."
        ]
      },
      {
        "title": "48876666",
        "h": "tf.map_fn",
        "t": "tf.while_loop",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.85",
        "r_prob": "0.65",
        "prob": "0.51935",
        "sentences": [
          "both approaches use functional primitives : tf.map_fn and tf.foldl.",
          "these primitives are built upon tf.while_loop and tensorarray."
        ]
      }
    ]
  },
  "tf.contrib.distributions.percentile": {},
  "tf.contrib.rnn.dropoutwrapper": {},
  "tf.nn.droput": {},
  "tf.image.crop_to_bounding_box": {},
  "tf.image.crop_and_resize": {},
  "tf.graphdef": {},
  "tf.graph_util.extract_sub_graph": {},
  "tf.contrib.framework.load_checkpoint": {},
  "tf.contrib.framework.list_variables": {},
  "tf.parse_single_sequence_example": {},
  "tf.scalar_summary": {},
  "tf.nn.conv2d ). layers": {},
  "tf.layers.conv2d": {
    "tf.nn.conv2d": [
      {
        "title": "47170433",
        "h": "tf.layers.conv2d",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9503999999999999",
        "sentences": [
          "and the output is ... kernels are exactly the same.",
          "the docs , btw : tf.nn.conv2d and tf.layers.conv2d."
        ]
      },
      {
        "title": "50030458",
        "h": "tf.nn.conv2d",
        "t": "tf.layers.conv2d",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9311999999999999",
        "sentences": [
          "i ' d recommend setting it up a little bit differently.",
          "instead of using tf.layers.conv2d , i would explicitly make the weights using calls to tf.get_variable () and then use these weights with calls to tf.nn.conv2d (). ",
          "references here for variable sharing and here for tf.nn.conv2d.",
          "hopefully that helps ! "
        ]
      },
      {
        "title": "53678910",
        "h": "tf.nn.conv2d",
        "t": "tf.layers.conv2d",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "0.95",
        "prob": "0.9216899999999999",
        "sentences": [
          "the default value of padding in these versions of conv2d are as follows : ",
          "tf.nn.conv2d , tf.nn.max_pool no default value.",
          "tf.layers.conv2d , tf.layers.max_pool2d."
        ]
      },
      {
        "title": "43587561",
        "h": "tf.nn.conv2d",
        "t": "tf.layers.conv2d",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.94",
        "r_prob": "0.99",
        "prob": "0.921294",
        "sentences": [
          "tf.nn.conv2d (...) is the core , low - level convolution functionality provided by tensorflow.",
          "note , that in current tensorflow versions , parts of layers are now in core , too , e.g.",
          "tf.layers.conv2d.",
          "the difference is simply , that tf.nn.conv2d is an op , that does convolution , nothing else.",
          "tf.layers.conv2d does more , e.g.",
          "without knowing your use case : most likely you want to use tf.layers.conv2d."
        ]
      },
      {
        "title": "52533491",
        "h": "tf.nn.conv2d",
        "t": "tf.layers.conv2d",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.96",
        "prob": "0.9125759999999999",
        "sentences": [
          "for tf.layers.conv2d and tf.nn.conv2d you can pass an additional parameter called name.",
          "in all cases your weights and biases are named like for example my_conv1 / weights and my_conv1 / bias."
        ]
      },
      {
        "title": "45308609",
        "h": "tf.layers.conv2d",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.8256",
        "sentences": [
          "there is a slight difference in the parameters.",
          "for tf.nn.conv2d : ",
          "for tf.layers.conv2d : ",
          "i would use tf.nn.conv2d when loading a pretrained model ( example code : https :// github.com / ry / tensorflow - vgg16 ), and tf.layers.conv2d for a model trained from scratch."
        ]
      },
      {
        "title": "47321605",
        "h": "tf.nn.conv2d",
        "t": "tf.layers.conv2d",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.87",
        "r_prob": "0.97",
        "prob": "0.818583",
        "sentences": [
          "as others mentioned the parameters are different especially the \" filter ( s )\". ",
          "tf.nn.conv2d takes a tensor as a filter , which means you can specify the weight decay ( or maybe other properties ) like the following in cifar10 code.",
          "i ' m not quite sure how to set weight decay in tf.layers.conv2d since it only take an integer as filters.",
          "maybe using kernel_constraint ? ",
          "on the other hand , tf.layers.conv2d handles activation and bias automatically while you have to write additional codes for these if you use tf.nn.conv2d."
        ]
      },
      {
        "title": "42785422",
        "h": "tf.nn.conv2d",
        "t": "tf.layers.conv2d",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.95",
        "r_prob": "0.78",
        "prob": "0.7335900000000001",
        "sentences": [
          "take a look here : tensorflow > tf.layers.conv2d ",
          "and here : tensorflow > conv2d ",
          "as you can see the arguments to the layers version are : ",
          "tf.layers.conv2d ( inputs , filters , kernel_size , strides =( 1 , 1 ), padding =' valid ', data_format =' channels_last ', dilation_rate =( 1 , 1 ), activation = none , use_bias = true , kernel_initializer = none , bias_initializer = tf.zeros_initializer (), kernel_regularizer = none , bias_regularizer = none , activity_regularizer = none , trainable = true , name = none , reuse = none ) ",
          "and the nn version : ",
          "tf.nn.conv2d ( input , filter , strides , padding , use_cudnn_on_gpu = none , data_format = none , name = none ) "
        ]
      },
      {
        "title": "49526130",
        "h": "tf.layers.conv2d",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.6052000000000001",
        "sentences": [
          "below are two methods , one which uses a named tf.layers.conv2d and a variable scope to force kernel reuse for each channel , and another which uses tf.nn.conv2d and a declared kernel to achieve the same result ( by simply passing the same filter in each time ). "
        ]
      }
    ]
  },
  "tf.op_scope": {},
  "tf.name_scope": {
    "tf.get_variable": [
      {
        "title": "43580096",
        "h": "tf.name_scope",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.97",
        "r_prob": "0.98",
        "prob": "0.922082",
        "sentences": [
          "now you see that tf.variable_scope () adds a prefix to the names of all variables ( no matter how you create them ), ops , constants.",
          "on the other hand tf.name_scope () ignores variables created with tf.get_variable () because it assumes that you know which variable and in which scope you wanted to use."
        ]
      },
      {
        "title": "43581502",
        "h": "tf.name_scope",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.97",
        "r_prob": "0.98",
        "prob": "0.922082",
        "sentences": [
          "now you see that tf.variable_scope () adds a prefix to the names of all variables ( no matter how you create them ), ops , constants.",
          "on the other hand tf.name_scope () ignores variables created with tf.get_variable () because it assumes that you know which variable and in which scope you wanted to use."
        ]
      },
      {
        "title": "51732374",
        "h": "tf.name_scope",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.8924",
        "sentences": [
          "you can try using tf.variable_scope instead.",
          "tf.name_scope is ignored by variables created via tf.get_variable () which is usually used by tf.layers functions.",
          "see this question for an ( albeit somewhat outdated ) explanation of the differences."
        ]
      },
      {
        "title": "37670385",
        "h": "tf.name_scope",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.82",
        "r_prob": "0.99",
        "prob": "0.738738",
        "sentences": [
          "tf.name_scope () is used to visualize variables.",
          "tf.name_scope ( name ) ",
          "wrapper for graph.name_scope () using the default graph .. ",
          "variable scope mechanism in tensorflow consists of 2 main functions : ",
          "tf.get_variable (, , ): creates or returns a variable with a given name."
        ]
      },
      {
        "title": "39981684",
        "h": "tf.name_scope",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.96",
        "r_prob": "0.56",
        "prob": "0.41932800000000003",
        "sentences": [
          "just use tf.variable_scope instead of tf.name_scope.",
          "tf.name_scope doesn ' t add prefixes to the variables created with tf.get_variable (). "
        ]
      }
    ],
    "tf.variable": [
      {
        "title": "55376814",
        "h": "tf.name_scope",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.9",
        "r_prob": "0.99",
        "prob": "0.77517",
        "sentences": [
          "to control variable naming users can use tf.name_scope + tf.variable.",
          "to control variable naming users can use tf.name_scope + tf.variable.",
          "indeed , tf.name_scope still exists in tensorflow 2.0 , so you can just do : "
        ]
      },
      {
        "title": "37115316",
        "h": "tf.name_scope",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.88",
        "r_prob": "0.71",
        "prob": "0.443608",
        "sentences": [
          "you might want to use tf.get_variable () instead of ' tf.variable `. ",
          "when we do with tf.variable_scope (\" name \"), this implicitly opens a tf.name_scope (\" name \"). ",
          "name_scope is originally used for managing operation names ( such as add , matmul ), because tf.variable is actually an operation and its operation name will be \" inherited \" by variables created by it , so the name of name_scope rather than variable_scope is used as prefix.",
          "but if you want to use tf.variable , you can also directly use name_scope in with statement : ",
          "if name has been used before , it will be made unique by calling self.unique_name ( name ). "
        ]
      },
      {
        "title": "34232533",
        "h": "tf.name_scope",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.47",
        "t_prob": "0.83",
        "r_prob": "1.0",
        "prob": "0.39009999999999995",
        "sentences": [
          "when you create a variable with tf.get_variable instead of tf.variable , tensorflow will start checking the names of the vars created with the same method to see if they collide.",
          "if you created a var with tf.get_variable and you try to change the prefix of your variable names by using the tf.name_scope context manager , this won ' t prevent the tensorflow of raising an exception.",
          "in summary , tf.name_scope just add a prefix to all tensor created in that scope ( except the vars created with tf.get_variable ), and tf.variable_scope add a prefix to the variables created with tf.get_variable."
        ]
      },
      {
        "title": "42542428",
        "h": "tf.variable",
        "t": "tf.name_scope",
        "r": "S1",
        "h_prob": "0.61",
        "t_prob": "0.78",
        "r_prob": "0.72",
        "prob": "0.342576",
        "sentences": [
          "you don ' t need to define two function for generating models , you can use tf.name_scope , and pass a model name to the function to use it as a prefix for variable declaration.",
          "even if you feed both models with exactly same training batches , you can ' t expect to have exactly similar characteristics models since initial values for weights are different and this could cause falling into different local minimum of error surface."
        ]
      }
    ],
    "tf.variable_scope": [
      {
        "title": "37670385",
        "h": "tf.name_scope",
        "t": "tf.variable_scope",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.7189000000000001",
        "sentences": [
          "tf.name_scope () is used to visualize variables.",
          "tf.name_scope ( name ) ",
          "wrapper for graph.name_scope () using the default graph .. ",
          "what i think you are looking for is tf.variable_scope (): ",
          "tf.get_variable (, , ): creates or returns a variable with a given name.",
          "tf.variable_scope (): manages namespaces for names passed to tf.get_variable (). "
        ]
      },
      {
        "title": "47153523",
        "h": "tf.name_scope",
        "t": "tf.variable_scope",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.87",
        "r_prob": "0.89",
        "prob": "0.704613",
        "sentences": [
          "change tf.name_scope ( timeseriesname ) to tf.variable_scope ( timeseriesname ). ",
          "the difference between tf.name_scope and tf.variable_scope is discussed in this quesion."
        ]
      },
      {
        "title": "48194899",
        "h": "tf.name_scope",
        "t": "tf.variable_scope",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.77",
        "r_prob": "1.0",
        "prob": "0.5929",
        "sentences": [
          "ok i ' ve found the issue apparently tf.name_scope is for operation only and tf.variable_scope works for both operations and variables ( as per this tf issue ). "
        ]
      },
      {
        "title": "52277944",
        "h": "tf.variable_scope",
        "t": "tf.name_scope",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.68",
        "r_prob": "0.88",
        "prob": "0.5804480000000001",
        "sentences": [
          "tf.variable_scope is an evolution of tf.name_scope to handle variable reuse.",
          "as you noticed , it does more than tf.name_scope , so there is no real reason to use tf.name_scope : not surprisingly , a tf developper advises to just use tf.variable_scope.",
          "my understanding for having tf.name_scope still lying around is that there are subtle incompatibilities in the behavior of those two , which invalidates tf.variable_scope as a drop - in replacement for tf.name_scope."
        ]
      },
      {
        "title": "43343662",
        "h": "tf.name_scope",
        "t": "tf.variable_scope",
        "r": "S1",
        "h_prob": "0.61",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.49410000000000004",
        "sentences": [
          "tf.name_scope defines a prefix for the operations defined within the scope.",
          "tf.variable_scope defines a prefix for the operations and variables defined within the scope.",
          "you have to use tf.variable_scope if you want to create a variable with the same name of another variable but in a different scope.",
          "tf.name_scope is used to define custom operations in order to well define the context.",
          "personally , i use tf.variable_scope almost always.",
          "moreover , yes , tf.variable_scope creates good looking graph in tensorboard exactly as tf.named_scope "
        ]
      },
      {
        "title": "34606240",
        "h": "tf.name_scope",
        "t": "tf.variable_scope",
        "r": "S1",
        "h_prob": "0.79",
        "t_prob": "0.81",
        "r_prob": "0.62",
        "prob": "0.39673800000000004",
        "sentences": [
          "every time you create a new name scope ( with tf.name_scope ()) or variable scope ( with tf.variable_scope ()) a unique suffix is added to the current name scope , based on the given string , possibly with an additional suffix to make it unique.",
          "when you enter a new name scope ( by entering a with tf.name_scope (\"...\"): or with tf.variable_scope (\"...\"): block ), tensorflow creates a new , unique name for the scope."
        ]
      }
    ]
  },
  "select": {
    "tf.where": [
      {
        "title": "42684587",
        "h": "select",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.27",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.2565",
        "sentences": [
          "the former tf.select () op is now called tf.where () ( in tensorflow 1.0 ). ",
          "the tf.where () op has a slightly odd condition on the shape of the condition : it can have a different shape from the two branches , but only if it is a vector whose length is the same size as the 0th dimension of t and e.therefore , you can make your program work as follows : "
        ]
      },
      {
        "title": "38527873",
        "h": "select",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.07",
        "t_prob": "0.75",
        "r_prob": "0.99",
        "prob": "0.05197500000000001",
        "sentences": [
          "i think you need to use tf.select.",
          "update : tensorflow 1.0 has deprecated tf.select in favor of numpy compatible tf.where."
        ]
      },
      {
        "title": "44981465",
        "h": "select",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.05",
        "t_prob": "0.42",
        "r_prob": "1.0",
        "prob": "0.021",
        "sentences": [
          "tf.select is no more working as indicated by this thread as well https :// github.com / tensorflow / tensorflow / issues / 8647 ",
          "something that worked for me was tf.where "
        ]
      }
    ],
    "tf.cond": [
      {
        "title": "34964654",
        "h": "select",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.22",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.21559999999999999",
        "sentences": [
          "i ' d like to draw your attention to two other ways you can compute conditional expressions using tensorflow , which don ' t require you to evaluate the predicate and get a python value back.",
          "the first way uses the tf.select () op to conditionally pass through values from two tensors passed as arguments : ",
          "the drawback of tf.select () is that it evaluates both val_if_true and val_if_false before computing the result , which might be expensive if they are complicated expressions.",
          "the second way uses the tf.cond () op , which conditionally evaluates one of two expressions."
        ]
      },
      {
        "title": "40695473",
        "h": "select",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.15",
        "t_prob": "0.99",
        "r_prob": "0.75",
        "prob": "0.111375",
        "sentences": [
          "if you want to vary at runtime you can use tf.cond or tf.select."
        ]
      },
      {
        "title": "37868116",
        "h": "select",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.05",
        "t_prob": "0.61",
        "r_prob": "1.0",
        "prob": "0.0305",
        "sentences": [
          "you are right , tf.select wants the condition to be of the same shape as the two other inputs.",
          "you should instead use tf.cond."
        ]
      },
      {
        "title": "35606830",
        "h": "select",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.04",
        "t_prob": "0.76",
        "r_prob": "1.0",
        "prob": "0.0304",
        "sentences": [
          "tf.select and tf.cond come in handy for situations where you have to perform computations conditionally on elements of a tensor."
        ]
      }
    ]
  },
  "tf.where": {
    "tf.cond": [
      {
        "title": "49649378",
        "h": "tf.cond",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.96",
        "r_prob": "0.82",
        "prob": "0.732096",
        "sentences": [
          "you can do it with tf.map_fn and tf.cond like this : ",
          "but you can also simply use tf.where like this : "
        ]
      },
      {
        "title": "53212479",
        "h": "tf.where",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.74",
        "r_prob": "0.95",
        "prob": "0.61864",
        "sentences": [
          "in the case of tf.where , you have a function with three inputs , condition c , value on true t and value on false f , and one output out.",
          "currently , no gradient is computed for the condition ( that would hardly make sense ), so you just need to do the gradients for t and f.assuming the input and the outputs are vectors , imagine c [ 0 ] is true.",
          "for tf.cond , the code is a bit more complicated , because the same operation ( merge ) is used in different contexts , and also tf.cond also uses switch operations inside."
        ]
      },
      {
        "title": "42164494",
        "h": "tf.where",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.58",
        "r_prob": "1.0",
        "prob": "0.5451999999999999",
        "sentences": [
          "tf.where sounds like what you want : a vectorized selection between tensors.",
          "tf.cond is a control flow modifier : it determines which ops are executed , and so it ' s difficult to think of useful batch semantics."
        ]
      },
      {
        "title": "51453667",
        "h": "tf.cond",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.61",
        "r_prob": "0.93",
        "prob": "0.45951300000000006",
        "sentences": [
          "the error occurs because tf.cond takes a decision based on a single boolean — much like an if statement.",
          "you could use tf.where to fix that problem , but then you will run into another one , which is that trainable is not a property that you can fix at runtime , it is part of the definition of a variable."
        ]
      }
    ],
    "tf.equal": [
      {
        "title": "58407536",
        "h": "tf.equal",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.7238",
        "sentences": [
          "try to use tf.where and tf.equal instead of use if else statement."
        ]
      },
      {
        "title": "54345492",
        "h": "tf.where",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.9",
        "r_prob": "0.72",
        "prob": "0.6155999999999999",
        "sentences": [
          "here is one possible solution with tf.equal , tf.where , tf.scater_nd_update , tf.gather_nd and tf.reverse_v2 : "
        ]
      },
      {
        "title": "52815622",
        "h": "tf.equal",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.94",
        "r_prob": "0.65",
        "prob": "0.55601",
        "sentences": [
          "you can just compare with tf.equal and then convert the boolean result to a number with tf.cast : "
        ]
      }
    ],
    "select": [
      {
        "title": "42684587",
        "h": "select",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.27",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.2565",
        "sentences": [
          "the former tf.select () op is now called tf.where () ( in tensorflow 1.0 ). ",
          "the tf.where () op has a slightly odd condition on the shape of the condition : it can have a different shape from the two branches , but only if it is a vector whose length is the same size as the 0th dimension of t and e.therefore , you can make your program work as follows : "
        ]
      },
      {
        "title": "38527873",
        "h": "select",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.07",
        "t_prob": "0.75",
        "r_prob": "0.99",
        "prob": "0.05197500000000001",
        "sentences": [
          "i think you need to use tf.select.",
          "update : tensorflow 1.0 has deprecated tf.select in favor of numpy compatible tf.where."
        ]
      },
      {
        "title": "44981465",
        "h": "select",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.05",
        "t_prob": "0.42",
        "r_prob": "1.0",
        "prob": "0.021",
        "sentences": [
          "tf.select is no more working as indicated by this thread as well https :// github.com / tensorflow / tensorflow / issues / 8647 ",
          "something that worked for me was tf.where "
        ]
      }
    ]
  },
  "dataset": {},
  "tf.image.resize_image_with_crop_or_pad": {},
  "tf.nn.softmax": {
    "tf.sigmoid": [
      {
        "title": "50090357",
        "h": "tf.nn.softmax",
        "t": "tf.sigmoid",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "tf.nn.softmax () computes probability distribution over classes ( output neurons ), if you have just 1 output neuron then probability distribution over 1 neuron will always be 1.0.",
          "i would suggest to use tf.sigmoid () in combination with tf.greater (), e.g : "
        ]
      },
      {
        "title": "36481418",
        "h": "tf.nn.softmax",
        "t": "tf.sigmoid",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.9114",
        "sentences": [
          "alternatively you could change your code is to use tf.nn.softmax instead of tf.sigmoid."
        ]
      },
      {
        "title": "47761718",
        "h": "tf.sigmoid",
        "t": "tf.nn.softmax",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.874",
        "sentences": [
          "instead of tf.nn.softmax , you could also use tf.sigmoid on a single logit , then set the other output to one minus that."
        ]
      }
    ]
  },
  "tf.histogra": {},
  "tf.contrib.seq2seq.traininghelper": {},
  "tf.contrib.seq2seq.basicdecoder": {},
  "tf.enable_eager_execution": {},
  "neural network": {
    "learning rate": [
      {
        "title": "45696327",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.9",
        "t_prob": "0.85",
        "r_prob": "1.0",
        "prob": "0.765",
        "sentences": [
          "i have faced a similar issue with the weight histograms in my neural network."
        ]
      },
      {
        "title": "46388899",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.86",
        "r_prob": "1.0",
        "prob": "0.6708000000000001",
        "sentences": [
          "your learning rate is too big ( try starting with 1e - 3 ). "
        ]
      },
      {
        "title": "46628573",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.71",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.6673999999999999",
        "sentences": [
          "if you only have 65 / 70 % accuracy on your training data that is really poor and indicates your neural network is not converging properly."
        ]
      },
      {
        "title": "63242757",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.74",
        "r_prob": "0.98",
        "prob": "0.61642",
        "sentences": [
          "your optimizer ' s learning rate is a little too high , so the step is too high and jumping all over the cost function."
        ]
      },
      {
        "title": "64227877",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.72",
        "t_prob": "0.74",
        "r_prob": "1.0",
        "prob": "0.5327999999999999",
        "sentences": [
          "this tends to be application - specific and not every problem can benefit from retraining the whole neural network.",
          "my advice is to keep lowering learning rate until it starts improving instead of damaging the weights but this may lead to such a small learning rate that it will be of no practical use."
        ]
      },
      {
        "title": "60393907",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.65",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.5265000000000001",
        "sentences": [
          "your neural network is ridiculously small , and that is paired with a ridiculously small learning rate."
        ]
      },
      {
        "title": "38575943",
        "h": "learning rate",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.59",
        "t_prob": "0.74",
        "r_prob": "0.94",
        "prob": "0.410404",
        "sentences": [
          "use a smaller learning rate ( 1e - 3 to 1e - 5 ). ",
          "use more layers.",
          "follow the xor neural network architecture."
        ]
      }
    ],
    "neural networks": [
      {
        "title": "50505609",
        "h": "neural networks",
        "t": "neural network",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.92",
        "r_prob": "0.91",
        "prob": "0.7451080000000001",
        "sentences": [
          "in multiclass , your classifier chooses one class from n other classes.",
          "usually , the last layer in neural networks that do multiclass classification is a softmax layer.",
          "it makes sense to use binary cross - entropy for that , since the way most neural network framework work makes it behave like you calculate average binary cross - entropy over these binary tasks.",
          "in neural networks that are multilabel classifiers , sigmoid is used as the last layer ( kaggle kernel you linked uses sigmoid as activation in the last layer ). "
        ]
      },
      {
        "title": "39023007",
        "h": "neural network",
        "t": "neural networks",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.8",
        "r_prob": "0.99",
        "prob": "0.74448",
        "sentences": [
          "the sklearn gradientboostingclassifier is a different algorithm than a neural network.",
          "this is the trade - off when using neural networks ; if you want performance better than alternative algorithms like random forests and svm , you need to tune the hyper parameters."
        ]
      },
      {
        "title": "62668130",
        "h": "neural networks",
        "t": "neural network",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.78",
        "r_prob": "0.89",
        "prob": "0.5623020000000001",
        "sentences": [
          "neural networks are designed to copy - cat.",
          "this is the nature of a neural network.",
          "an isolated optimization problem asks a fundamentally different question.",
          "if you want to phrase an optimization problem in terms of a neural network solution it would look like this.",
          "given a collection of approximated functions ( millions of trained neural networks ) and known optimized solutions ( the expected solutions for each one ), train a new neural network that mimics this behavior.",
          "these are great examples of something a neural network could learn to do , on unseen functions from the dataset.",
          "the neural network , might even learn the underlying behavior so well that it works with coefficients outside the initial dataset too.",
          "of course , one could start building a massive collection of optimization neural networks that applies in millions of different cases for all kinds of different problems.",
          "such \" neural network zoo \" could solve all of optimization theory."
        ]
      },
      {
        "title": "59999062",
        "h": "neural network",
        "t": "neural networks",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.89",
        "r_prob": "0.63",
        "prob": "0.487809",
        "sentences": [
          "the second question is case - dependent ; when building neural networks from scratch , there is no guarantee that your problem will work better with approach a or approach b ; this is why we do hyperparameter search and optimization , in order to seek for the best overall parameters in order to minimize our loss on the validation set .. "
        ]
      },
      {
        "title": "47182326",
        "h": "neural networks",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.79",
        "t_prob": "0.87",
        "r_prob": "0.62",
        "prob": "0.426126",
        "sentences": [
          "convolutional networks are just another type of neural network.",
          "even in \" normal \" neural networks , one doesn ' t typically specify weights and biases manually."
        ]
      },
      {
        "title": "49952428",
        "h": "neural networks",
        "t": "neural network",
        "r": "S1",
        "h_prob": "0.62",
        "t_prob": "0.76",
        "r_prob": "0.79",
        "prob": "0.372248",
        "sentences": [
          "while neural networks are very complex and powerful , they aren ' t a magic box.",
          "this neural network has a clear sense of the frequency of the wave , but needs a bit more work on determining the general trend of the line."
        ]
      },
      {
        "title": "54173845",
        "h": "neural networks",
        "t": "neural network",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.74",
        "r_prob": "0.56",
        "prob": "0.348096",
        "sentences": [
          "still , i can already tell you that starting with neural networks is a poor course of action , as you immediately forsake understanding of the domain.",
          "troubleshooting of misbehaving neural network is far more cumbersome than majority of other learning systems."
        ]
      }
    ],
    "loss function": [
      {
        "title": "52902870",
        "h": "neural network",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.93",
        "t_prob": "0.75",
        "r_prob": "0.98",
        "prob": "0.68355",
        "sentences": [
          "however , probably the better way is to integrate this additional objective in the loss function during training.",
          "this way you can trade off between your additional requirement and fitting the weights of your neural network."
        ]
      },
      {
        "title": "63712487",
        "h": "neural network",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.91",
        "r_prob": "0.84",
        "prob": "0.64974",
        "sentences": [
          "in a nutshell , when you are using the option from_logits = true , you are telling the loss function that your neural network output is not normalized."
        ]
      },
      {
        "title": "60098915",
        "h": "neural network",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.74",
        "t_prob": "0.88",
        "r_prob": "0.96",
        "prob": "0.6251519999999999",
        "sentences": [
          "by the way , if you are taking gradients of the loss function of your neural network with respect to the parameters of you network , the time to compute the gradients will be o ( 1 ) the cost of computing the loss itself."
        ]
      },
      {
        "title": "42284733",
        "h": "neural network",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.87",
        "t_prob": "0.87",
        "r_prob": "0.76",
        "prob": "0.575244",
        "sentences": [
          "then , you have to define a loss function and a neural network.",
          "for the loss function , you can simply use the negative log - probability."
        ]
      },
      {
        "title": "55158267",
        "h": "neural network",
        "t": "loss function",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.78",
        "r_prob": "0.59",
        "prob": "0.38656799999999997",
        "sentences": [
          "a metric function is similar to a loss function , except that the results from evaluating a metric are not used when training the model."
        ]
      }
    ],
    "activation function": [
      {
        "title": "51831196",
        "h": "activation function",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.83",
        "r_prob": "0.82",
        "prob": "0.5512859999999999",
        "sentences": [
          "i agree with @ cyniikal , your network seems too complex for this dataset.",
          "if you would like to add layers to your neural network ( the network will converge with more difficulties ), i highly recommend reading this article on neural nets.",
          "specifically , since you added sigmoid as your last activation function , i believe you are suffering from a vanishing gradient problem."
        ]
      },
      {
        "title": "52336461",
        "h": "activation function",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.67",
        "t_prob": "0.81",
        "r_prob": "0.97",
        "prob": "0.5264190000000001",
        "sentences": [
          "the reason for your problem is because the output of your neural network is being squashed by an activation function.",
          "this way , initially , your neural network will output close to 0 values , and then the weights will be updated and the learning will be more stable."
        ]
      },
      {
        "title": "44498122",
        "h": "activation function",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.74",
        "t_prob": "0.91",
        "r_prob": "0.51",
        "prob": "0.343434",
        "sentences": [
          "these weights ( and sometimes biases ) are what we learn in a neural network."
        ]
      }
    ]
  },
  "tf.clip_by_norm": {},
  "tf.nn.xw_plus_b": {},
  "tf.initialize_variables": {},
  "tf.fifoqueue": {},
  "tf.get_default_graph": {
    "tf.graph": [
      {
        "title": "51183870",
        "h": "tf.get_default_graph",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "fortunately , tensorflow has a convenient tool to spot those errors : tf.graph.finalize.",
          "it is good practice to call this function before iterating.",
          "so in your case i would call tf.get_default_graph (). finalize () before your loop and look for any error it may throw."
        ]
      },
      {
        "title": "45996277",
        "h": "tf.get_default_graph",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9309999999999999",
        "sentences": [
          "my_graph can be tf.get_default_graph () if you are using the default graph or any other tf.graph ( or tf.graphdef ) object."
        ]
      },
      {
        "title": "54669386",
        "h": "tf.get_default_graph",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.92",
        "prob": "0.85652",
        "sentences": [
          "so it is exactly the same as calling tf.get_default_graph (). add_to_collections."
        ]
      },
      {
        "title": "44903831",
        "h": "tf.get_default_graph",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.83",
        "r_prob": "0.99",
        "prob": "0.632709",
        "sentences": [
          "if no graph is specified , the session constructor tries to build a graph using the default one ( that you can get using tf.get_default_graph ). "
        ]
      }
    ]
  },
  "tf.keras.layers.batchnormalization": {},
  "tf.keras.layers.dropout": {},
  "tf.layers.average_pooling2d": {},
  "tf.keras.layers.globalaveragepooling2d": {},
  "rnn": {},
  "tf.batch_self_": {},
  "tf.self_adjoint_eig": {},
  "tf.contrib.data.batch_and_drop_remainder": {},
  "tf.data.dataset": {
    "tf.estimator.estimator": [
      {
        "title": "48628009",
        "h": "tf.estimator.estimator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "in tensorflow version 1.5 and later , the tf.estimator.estimator will automatically create and initialize an initializable iterator when you return a tf.data.dataset from your input_fn."
        ]
      },
      {
        "title": "51063972",
        "h": "tf.data.dataset",
        "t": "tf.estimator.estimator",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.9216899999999999",
        "sentences": [
          "2 ) define an input_fn , which you will use tf.data.dataset.",
          "3 ) define your model , to be used in your tf.estimator.estimator."
        ]
      },
      {
        "title": "49963569",
        "h": "tf.data.dataset",
        "t": "tf.estimator.estimator",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.97",
        "r_prob": "0.95",
        "prob": "0.9122849999999999",
        "sentences": [
          "there are two parameters that are causing this : ",
          "tf.data.dataset.repeat has a count parameter : ",
          "a tf.int64 scalar tf.tensor , representing the number of times the dataset should be repeated.",
          "in your case , count is always none , so the dataset is repeated indefinitely.",
          "tf.estimator.estimator.evaluate has the steps parameter : "
        ]
      },
      {
        "title": "49837908",
        "h": "tf.data.dataset",
        "t": "tf.estimator.estimator",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "0.67",
        "prob": "0.656667",
        "sentences": [
          "if you cannot , consider using tf.data.dataset.from_generator.",
          "to augment your dataset , use tf.data.dataset.map either before or after the batch operation , depending on whether or not you want to apply a batch - wise operation ( something working on a 4d image tensor ) or element - wise operation ( 3d image tensor ). ",
          "if you ' re using a tf.estimator.estimator ( or wouldn ' t mind converting your code to using it ), then check out tf.estimator.train_and_evaluate for an in - built way of switching between datasets .. ",
          "see the article for notes on efficiencies."
        ]
      }
    ],
    "tf.data.iterator": [
      {
        "title": "47396308",
        "h": "tf.data.iterator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "if you see the error message \" function ... is not defined \" when creating a tf.data.dataset or tf.data.iterator , upgrade to a newer version of tensorflow."
        ]
      },
      {
        "title": "49902781",
        "h": "tf.data.dataset",
        "t": "tf.data.iterator",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.9108",
        "sentences": [
          "you can express the entire pipeline using tf.data.dataset objects , which might make things slightly easier : ",
          "to use the values from the dataset , you can make a tf.data.iterator to get the next element as a pair of tf.tensor objects , then use these as the input to your model."
        ]
      },
      {
        "title": "50191742",
        "h": "tf.data.dataset",
        "t": "tf.data.iterator",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.855",
        "sentences": [
          "i would create 2 tf.data.dataset , one for training and one for validation subsets.",
          "once you have both datasets pipelines defined ( where you are able to define 2 different batch sizes ), you can join them in the graph by creating a single tf.data.iterator with a handler ( in my case , the tf.placeholder handle ). ",
          "edit : your current error seems to be due to trying to do a sess.run () on a tf.data.iterator.",
          "try to replace sess.run ( train_iter ) for sess.run ( train_iter.initializer ) ( and same for validation iterator ). "
        ]
      },
      {
        "title": "48127601",
        "h": "tf.data.iterator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.84",
        "prob": "0.798504",
        "sentences": [
          "in short , there is not a good way to get the size / length ; tf.data.dataset is built for pipelines of data , so has an iterator structure ( in my understanding and according to my read of the dataset ops code.",
          "from the programmer ' s guide : ",
          "a tf.data.iterator provides the main way to extract elements from a dataset.",
          "more generally though , why does this problem arise ? ",
          "if you are calling batch , you are also getting a tf.data.dataset , so whatever you are running on a batch you should be able to run on the whole dataset ; it will iterate through all the elements and calculate validation accuracy."
        ]
      }
    ],
    "tf.contrib.data": [
      {
        "title": "48031461",
        "h": "tf.data.dataset",
        "t": "tf.contrib.data",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9603999999999999",
        "sentences": [
          "tf.contrib.data has been deprecated and been removed ( check here ). ",
          "you need to upgrade the tensorflow version using : sudo pip3 install -- upgrade tensorflow.",
          "check the installation guide here.",
          "open the python terminal and type ",
          "import tensorflow as tf ",
          "dataset = tf.data.dataset ",
          "hope it will help."
        ]
      },
      {
        "title": "46763985",
        "h": "tf.contrib.data",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.99",
        "r_prob": "0.99",
        "prob": "0.940896",
        "sentences": [
          "alternatively , i ' d encourage you to check out the tf.data.dataset interface ( possible tf.contrib.data.dataset in tensorflow 1.3 or prior ). "
        ]
      },
      {
        "title": "51179552",
        "h": "tf.data.dataset",
        "t": "tf.contrib.data",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.776",
        "sentences": [
          "as far as i know a recent update completely removed most \" standard \" ops from tf.contrib.data.",
          "it now only contains \" experimental \"/ volatile code.",
          "simply use tf.data.dataset instead."
        ]
      }
    ],
    "tf.data.": [
      {
        "title": "59767370",
        "h": "tf.data.dataset",
        "t": "tf.data.",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.94",
        "r_prob": "0.99",
        "prob": "0.921294",
        "sentences": [
          "tf.data.csv returns a tf.data.dataset.",
          "if you would like to evaluate a tf.data.dataset , the method evaluatedataset can be used instead.",
          "evaluatedataset returns a promise."
        ]
      },
      {
        "title": "47946271",
        "h": "tf.data.dataset",
        "t": "tf.data.",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.87",
        "r_prob": "0.99",
        "prob": "0.835461",
        "sentences": [
          "if you implement the same pipeline using the tf.data.dataset api and using queues , the performance of the dataset version should be better than the queue - based version.",
          "however , there are a few performance best practices to observe in order to get the best performance.",
          "we have collected these in a performance guide for tf.data.",
          "adding dataset.prefetch ( 1 ) to the end of your pipeline will give you most of the benefit of prefetching , but you might need to tune this further.",
          "in a dataset pipeline , this would be equivalent to dataset.repeat ( num_epochs ). shuffle ( n ). "
        ]
      },
      {
        "title": "48713164",
        "h": "tf.data.",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.99",
        "r_prob": "0.92",
        "prob": "0.7741800000000001",
        "sentences": [
          "i advise you to read the tutorial by @ mrry on tf.data.",
          "on slide 42 he explains how to use tf.data.dataset.interleave () to read multiple tfrecord files at the same time."
        ]
      }
    ],
    "tf.data": [
      {
        "title": "62187234",
        "h": "tf.data",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.96",
        "prob": "0.8937599999999999",
        "sentences": [
          "tf.data does not use tf.placeholders under the hood.",
          "tf.data.dataset uses the same mechanisms as other tensorflow ops."
        ]
      },
      {
        "title": "48738199",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.9",
        "r_prob": "0.98",
        "prob": "0.87318",
        "sentences": [
          "the more recent tf.data library provides a tf.data.dataset.cache method to cache an entire dataset into memory or into a file."
        ]
      },
      {
        "title": "52417770",
        "h": "tf.data",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.97",
        "r_prob": "0.94",
        "prob": "0.8206199999999999",
        "sentences": [
          "the tf.data api ( tensorflow 1.4 onwards ) is great for things like this.",
          "the pipeline will looks something like the following : ",
          "create an initial tf.data.dataset object that iterates over all examples.",
          "you can also generate the tfrecords using lower level operations.",
          "load images via tf.data.dataset.map and tf.py_func ( tion ). ",
          "alternatively you can load the image files from filenames inside tf.data.dataset.map as below.",
          "if you need more custom loading functions , also check out tf.py_func."
        ]
      },
      {
        "title": "47996856",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.81",
        "r_prob": "0.96",
        "prob": "0.7698240000000001",
        "sentences": [
          "the tf.data.dataset.from_generator () method provides a way to convert python generators into tf.tensor objects that evaluate to each successive element from the generator.",
          "you can use the tf.data api to convert the generator first to a tf.data.dataset , then to a tf.data.iterator , and finally to a tuple of tf.tensor objects.",
          "see the tf.data programmer ' s guide for more ideas."
        ]
      },
      {
        "title": "64211418",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.752",
        "sentences": [
          "the tf.data.dataset is the tf2 way of setting this up.",
          "at a high - level , the tf.data api sets up a stream of examples."
        ]
      },
      {
        "title": "64119127",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.75",
        "r_prob": "0.97",
        "prob": "0.7202249999999999",
        "sentences": [
          "you don ' t need your own tfds.load , it simply returns a tf.data.dataset object , which you can easily build yourself.",
          "note : do not confuse tfds ( this library ) with tf.data ( tensorflow api to build efficient data pipelines ). ",
          "tfds is a high level wrapper around tf.data.",
          "if you ' re not familiar with this api , we encourage you to read the official tf.data guide first.",
          "read more about tf.data.dataset."
        ]
      },
      {
        "title": "56340136",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.74",
        "r_prob": "0.95",
        "prob": "0.6748799999999999",
        "sentences": [
          "you can pass tf.data.dataset iterators instead of numpy data.",
          "more information on how to work with tf.data you can find here."
        ]
      },
      {
        "title": "58231402",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.64",
        "r_prob": "0.98",
        "prob": "0.620928",
        "sentences": [
          "alongside custom defined python generators , you can wrap the imagedatagenerator from keras inside tf.data.",
          "therefore , one can still use the typical keras imagedatagenerator , you just need to wrap it into a tf.data.dataset like above."
        ]
      },
      {
        "title": "48817892",
        "h": "tf.data.dataset",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.66",
        "r_prob": "1.0",
        "prob": "0.5874",
        "sentences": [
          "you should be able to use tf.data.dataset.shuffle to achieve what you want.",
          "add enough shuffle so that a batch of patches is diverse ( all the patches come from different images ). ",
          "don ' t load too many big images in cache.",
          "you can achieve all that using the tf.data api by doing the following steps : "
        ]
      },
      {
        "title": "47946271",
        "h": "tf.data",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.97",
        "r_prob": "0.58",
        "prob": "0.5175919999999999",
        "sentences": [
          "if you implement the same pipeline using the tf.data.dataset api and using queues , the performance of the dataset version should be better than the queue - based version.",
          "adding dataset.prefetch ( 1 ) to the end of your pipeline will give you most of the benefit of prefetching , but you might need to tune this further.",
          "in a dataset pipeline , this would be equivalent to dataset.repeat ( num_epochs ). shuffle ( n ). "
        ]
      },
      {
        "title": "37343690",
        "h": "tf.data",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.89",
        "r_prob": "0.59",
        "prob": "0.43058199999999996",
        "sentences": [
          "new answer ( with tf.data ) and with labels.",
          "with the introduction of tf.data in r1.4 , we can create a batch of images without placeholders and without queues.",
          "create a list containing the filenames of the images and a corresponding list of labels.",
          "create a tf.data.dataset reading these filenames and labels.",
          "preprocess the data.",
          "create an iterator from the tf.data.dataset which will yield the next batch.",
          "create a list of filenames ( ex : the paths to your images ). "
        ]
      }
    ],
    "tf.data.tfrecorddataset": [
      {
        "title": "55219739",
        "h": "tf.data.tfrecorddataset",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.8827",
        "sentences": [
          "in the real use case you would replace data with the list of file names and tf.data.dataset.from_tensor_slices ( string_data ( d )) with tf.data.tfrecorddataset ( d ), but otherwise it should work similarly."
        ]
      },
      {
        "title": "54399204",
        "h": "tf.data.tfrecorddataset",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "1.0",
        "r_prob": "0.99",
        "prob": "0.8712",
        "sentences": [
          "you can use tf.data.dataset.batch ( batch_size = train_batch_size ) for batching the input data but for that frist you have to create a dataset from your input data by using the relevant method for your data for example dataset = tf.data.tfrecorddataset ( filename ). "
        ]
      },
      {
        "title": "62083141",
        "h": "tf.data.tfrecorddataset",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.7789999999999999",
        "sentences": [
          "for example , you could pass the queue as a generator into the dataset api ( see tf.data.dataset.from_generator ). ",
          "for a tensorflow implementation , you could use tf.data.dataset.shard with tf.data.tfrecorddataset."
        ]
      }
    ],
    "tf.py_func": [
      {
        "title": "52417770",
        "h": "tf.py_func",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.56",
        "t_prob": "0.97",
        "r_prob": "0.84",
        "prob": "0.45628799999999997",
        "sentences": [
          "the tf.data api ( tensorflow 1.4 onwards ) is great for things like this.",
          "the pipeline will looks something like the following : ",
          "create an initial tf.data.dataset object that iterates over all examples.",
          "you can also generate the tfrecords using lower level operations.",
          "load images via tf.data.dataset.map and tf.py_func ( tion ). ",
          "alternatively you can load the image files from filenames inside tf.data.dataset.map as below.",
          "if you need more custom loading functions , also check out tf.py_func.",
          "more general information here , and notes on performance here "
        ]
      },
      {
        "title": "45206574",
        "h": "tf.py_func",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.45",
        "t_prob": "0.99",
        "r_prob": "0.99",
        "prob": "0.441045",
        "sentences": [
          "since tf 1.4 , the best way to do it is to use tf.data.dataset , and particularly tf.data.dataset.from_tensor_slices.",
          "to make sure that those operations happen concurrently , make sure to use tensorflow operations only and avoid wrapping python operations with tf.py_func."
        ]
      },
      {
        "title": "52063718",
        "h": "tf.py_func",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.42",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.3948",
        "sentences": [
          "just in case someone else has a similar problem , i will write a small version of a solution to this question.",
          "i defined a function called \" extract \" and mapped this extracted numpy array as a dataset using tf.py_func.",
          "the extracted dataframes and one - hot arrays are zipped with tf.data.dataset.zip into a final dataset."
        ]
      },
      {
        "title": "49837908",
        "h": "tf.data.dataset",
        "t": "tf.py_func",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.33",
        "r_prob": "1.0",
        "prob": "0.3267",
        "sentences": [
          "if you cannot , consider using tf.data.dataset.from_generator.",
          "my preferred method is to load some keys tensor entirely into memory - it might just be the indices of each example - then map that key value to data values using tf.py_func.",
          "to augment your dataset , use tf.data.dataset.map either before or after the batch operation , depending on whether or not you want to apply a batch - wise operation ( something working on a 4d image tensor ) or element - wise operation ( 3d image tensor ). ",
          "see the article for notes on efficiencies."
        ]
      },
      {
        "title": "47884927",
        "h": "tf.data.dataset",
        "t": "tf.py_func",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.29",
        "r_prob": "0.96",
        "prob": "0.26169599999999993",
        "sentences": [
          "i am working on a from_indexable for tf.data.dataset https :// github.com / tensorflow / tensorflow / issues / 14448 ",
          "the function from_indexable makes a tf.data.range , wraps the indexable in a generalized tf.py_func and calls map."
        ]
      }
    ],
    "generator": [
      {
        "title": "50181132",
        "h": "generator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.0",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.0",
        "sentences": [
          "better yet , if you ' re distributing your training you can reduce to the set of servers who need some shard of your final dataset.",
          "the tensorflow dataset will provide you with an iterator that ' s accessed directly by the graph so there ' s no need for tf.placeholders or marshaling data outside of the tf.data.dataset.from_generator () code you write."
        ]
      },
      {
        "title": "56602867",
        "h": "generator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.0",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.0",
        "sentences": [
          "instead of passing the pandas dataframes arguments for the generator function in the args parameter in the tf.data.dataset.from_generator method , i used lambda to pass them in the generator function itself : "
        ]
      },
      {
        "title": "50278981",
        "h": "generator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.0",
        "t_prob": "0.98",
        "r_prob": "0.98",
        "prob": "0.0",
        "sentences": [
          "it may be possible to use tf.data.dataset.from_generator () for this case.",
          "you could convert this to a tf.data.dataset with the following code : "
        ]
      },
      {
        "title": "52989446",
        "h": "generator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.0",
        "t_prob": "0.97",
        "r_prob": "0.96",
        "prob": "0.0",
        "sentences": [
          "i found a way to do it using tf.data.from_generator the trick i found was to make two separate dataset ( one for mat file and one for the jpg file ) and then to combine them using tf.data.dataset.zip "
        ]
      },
      {
        "title": "49837908",
        "h": "tf.data.dataset",
        "t": "generator",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.0",
        "r_prob": "0.74",
        "prob": "0.0",
        "sentences": [
          "if you cannot , consider using tf.data.dataset.from_generator.",
          "to augment your dataset , use tf.data.dataset.map either before or after the batch operation , depending on whether or not you want to apply a batch - wise operation ( something working on a 4d image tensor ) or element - wise operation ( 3d image tensor ). ",
          "see the article for notes on efficiencies."
        ]
      }
    ],
    "module": [
      {
        "title": "53320326",
        "h": "module",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.0",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.0",
        "sentences": [
          "when you use the tf.data.dataset module , it actually defines an input graph which is independant from the model graph.",
          "what happens here is that you first created a small graph by calling tf.data.dataset.from_tensor_slices (), then the estimator api created a second graph by calling dataset.make_one_shot_iterator () automatically."
        ]
      },
      {
        "title": "57057141",
        "h": "tf.data.dataset",
        "t": "module",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.0",
        "r_prob": "0.99",
        "prob": "0.0",
        "sentences": [
          "you can change your batch_size or using smarter ways to input your training data ( such as tf.data.dataset and using cache ). "
        ]
      },
      {
        "title": "54897167",
        "h": "tf.data.dataset",
        "t": "module",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.0",
        "r_prob": "0.94",
        "prob": "0.0",
        "sentences": [
          "the tf.data nodule has specific tools which help in building a input pipeline for your ml model.",
          "a input pipeline takes in the raw data , processes it and then feeds it to the model .. ",
          "when should i use tf.data module ? ",
          "there are two options to use tf.placeholder () or tf.data.dataset.",
          "the tf.data.dataset is a much easier implementation.",
          "this process would consume more time feeding in the data .. "
        ]
      }
    ]
  },
  "tf.sparse_placeholder": {},
  "tf.example pro": {},
  "tf.sparse_tensor_to_dense": {
    "tf.sparsetensor": [
      {
        "title": "34686952",
        "h": "tf.sparse_tensor_to_dense",
        "t": "tf.sparsetensor",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.86",
        "r_prob": "1.0",
        "prob": "0.8428",
        "sentences": [
          "update : tensorflow 1.0 includes a tf.scatter_nd () operator , which can be used to create delta below without creating a tf.sparsetensor.",
          "one way you could do this is to define a tf.sparsetensor , delta , representing the change : ",
          "then you can use the tf.sparse_tensor_to_dense () op to make a dense tensor from delta and add it to c : "
        ]
      },
      {
        "title": "50235076",
        "h": "tf.sparse_tensor_to_dense",
        "t": "tf.sparsetensor",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.6930000000000001",
        "sentences": [
          "use tf.sparsetensor and , if required , tf.sparse_tensor_to_dense."
        ]
      },
      {
        "title": "49243181",
        "h": "tf.sparse_tensor_to_dense",
        "t": "tf.sparsetensor",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.87",
        "r_prob": "1.0",
        "prob": "0.6699",
        "sentences": [
          "you can make use of a combination of tf.sparsetensor and tf.sparse_tensor_to_dense to achieve what you want : "
        ]
      }
    ]
  },
  "tf.sparse_to_dense": {},
  "tf.estimator.inputs.numpy_input_fn": {},
  "tf.estimator.linearregressor": {},
  "neural -": {},
  "tf.random.set_seed": {},
  "training process": {},
  "tf.train.shuffle_batch": {
    "tf.train.string_input_producer": [
      {
        "title": "45807749",
        "h": "tf.train.string_input_producer",
        "t": "tf.train.shuffle_batch",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.891",
        "sentences": [
          "the call to input_pipeline () creates two new input pipeline stages ( in the calls to tf.train.string_input_producer () and tf.train.shuffle_batch ()). "
        ]
      },
      {
        "title": "39358402",
        "h": "tf.train.string_input_producer",
        "t": "tf.train.shuffle_batch",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.8644999999999999",
        "sentences": [
          "the default behavior for tf.train.string_input_producer ( fnames ) is to produce an infinite number of copies of the elements in fnames.",
          "therefore , since your tf.train.shuffle_batch () capacity is larger than the total number of elements in your input files ( 5 elements per file * 10 files = 50 elements ), and the min_after_dequeue is also larger than the number of elements , the queue will contain at least two full copies of the input data before the first batch is produced.",
          "if you only want to process each example once , you can set an explicit num_epochs = 1 when creating the tf.train.string_input_producer (). "
        ]
      },
      {
        "title": "34258214",
        "h": "tf.train.string_input_producer",
        "t": "tf.train.shuffle_batch",
        "r": "S1",
        "h_prob": "0.79",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.7268000000000001",
        "sentences": [
          "first , randomly shuffle the order in which you input your datafiles , by reading from them using a tf.train.string_input_producer with shuffle = true that feeds into whatever input method you use ( if you can put your examples into tf.example proto format , that ' s easy to use with parse_example ). ",
          "second , you need to mix at a finer granularity.",
          "you can accomplish this by feeding the input examples into a tf.train.shuffle_batch node with a large capacity and large value of min_after_dequeue."
        ]
      },
      {
        "title": "44913933",
        "h": "tf.train.string_input_producer",
        "t": "tf.train.shuffle_batch",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.84",
        "r_prob": "1.0",
        "prob": "0.7055999999999999",
        "sentences": [
          "you can put even all examples in one file or have 10k per file.",
          "regarding shuffling : there are two types shuffling which serve different purposes and shuffle different things : ",
          "tf.train.string_input_producer shuffle : boolean.",
          "if case of false , the files follow one after each other .. ",
          "tf.train.shuffle_batch creates batches by randomly shuffling tensors."
        ]
      }
    ],
    "tf.randomshufflequeue": [
      {
        "title": "35729020",
        "h": "tf.train.shuffle_batch",
        "t": "tf.randomshufflequeue",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.47",
        "r_prob": "0.98",
        "prob": "0.45138799999999996",
        "sentences": [
          "if you move this line after images = get_batch (), your program should work.",
          "what is the problem here ? ",
          "the tf.train.shuffle_batch () function internally uses a tf.randomshufflequeue to produce a randomized batch."
        ]
      },
      {
        "title": "34594851",
        "h": "tf.train.shuffle_batch",
        "t": "tf.randomshufflequeue",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.37",
        "r_prob": "0.99",
        "prob": "0.35164799999999996",
        "sentences": [
          "the tf.train.shuffle_batch () function can be used to produce ( one or more ) tensors containing a batch of inputs.",
          "internally , tf.train.shuffle_batch () creates a tf.randomshufflequeue , on which it calls q.enqueue () with the image and label tensors to enqueue a single element ( image - label pair ). ",
          "note that , although it looks from the code like read_input and filename_queue have a functional relationship , there is an additional wrinkle.",
          "simply evaluating the result of tf.train.shuffle_batch () will block forever , because no elements have been added to the internal queue.",
          "to simplify this , when you call tf.train.shuffle_batch (), tensorflow will add a queuerunner to an internal collection in the graph."
        ]
      },
      {
        "title": "41971507",
        "h": "tf.train.shuffle_batch",
        "t": "tf.randomshufflequeue",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.35",
        "r_prob": "0.99",
        "prob": "0.33263999999999994",
        "sentences": [
          "the tf.train.shuffle_batch () function uses a tf.randomshufflequeue internally to accumulate batches of batch_size elements , which are sampled uniformly at random from the elements currently in the queue.",
          "if the dataset has only 200 images , it would be easily possible to load the entire dataset in memory.",
          "tf.train.shuffle_batch () would be quite inefficient , because it enqueue each image and label multiple times in the tf.randomshufflequeue."
        ]
      },
      {
        "title": "39493501",
        "h": "tf.train.shuffle_batch",
        "t": "tf.randomshufflequeue",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.24",
        "r_prob": "0.99",
        "prob": "0.22571999999999998",
        "sentences": [
          "the tensors returned from tf.train.shuffle_batch (), image_batch and label_batch correspond to 32 probably *- different images packed together , and the 32 related labels.",
          "tensorflow uses a tf.randomshufflequeue internally to shuffle the data , and creates additional threads to evaluate single_image and single_label so that they can be added to this queue.",
          "the tf.train.shuffle_batch () function has different behaviors depending on the arguments you pass."
        ]
      }
    ]
  },
  "tf.strings.split": {},
  "tf.summary.merge": {
    "tf.summary.merge_all": [
      {
        "title": "46304709",
        "h": "tf.summary.merge",
        "t": "tf.summary.merge_all",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.71",
        "r_prob": "0.87",
        "prob": "0.592992",
        "sentences": [
          "i replaced tf.summary.merge_all with tf.summary.merge ([ summary_var1 , summary_var2 ]) "
        ]
      },
      {
        "title": "48946182",
        "h": "tf.summary.merge_all",
        "t": "tf.summary.merge",
        "r": "S1",
        "h_prob": "0.63",
        "t_prob": "0.86",
        "r_prob": "1.0",
        "prob": "0.5418",
        "sentences": [
          "if you have multiple summaries merged into a single tf.summary object ( e.g.",
          "made with tf.summary.merge / tf.summary.merge_all ), then you would have to filter the value field : "
        ]
      },
      {
        "title": "47723110",
        "h": "tf.summary.merge_all",
        "t": "tf.summary.merge",
        "r": "S1",
        "h_prob": "0.62",
        "t_prob": "0.9",
        "r_prob": "0.97",
        "prob": "0.5412600000000001",
        "sentences": [
          "instead of merging all summaries by merged_summary = tf.summary.merge_all (), you can merge the ops that you wanted like merged_summary_group1 = tf.summary.merge ([ op1 , op2 , ...]). "
        ]
      }
    ]
  },
  "tf.random_normal_initializer": {},
  "tf.contrib.layers.xavier_initializer": {},
  "tf.train.gradientdescentoptimizer": {},
  "tf.nn.weighted_cross_entropy_with_logits": {},
  "tf.losses.sigmoid_cross_entropy": {},
  "tf.nn.embedding_lookup": {
    "tf.tensor": [
      {
        "title": "40903921",
        "h": "tf.nn.embedding_lookup",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.8832",
        "sentences": [
          "the tf.nn.embedding_lookup ( params , ids ) function only accepts dense , rectangular tensors as the ids argument."
        ]
      },
      {
        "title": "36241501",
        "h": "tf.nn.embedding_lookup",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.8464",
        "sentences": [
          "the cell callable is a function that takes an input tf.tensor and the current state as a tf.tensor , and returns an output tf.tensor and the new state as a tf.tensor.",
          "the inputs tensor is the result of a tf.nn.embedding_lookup () operation ; the outputs list is later concatenated and used as the input to a loss calculation.",
          "tensorflow backprops from the loss through the rnn and the embedding lookup back to the model variables."
        ]
      },
      {
        "title": "34032563",
        "h": "tf.nn.embedding_lookup",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.99",
        "r_prob": "0.66",
        "prob": "0.5292540000000001",
        "sentences": [
          "if you have a tf.sparsetensor and a tf.tensor , you can use tf.sparse_tensor_dense_matmul () to multiply them."
        ]
      }
    ]
  },
  "tf.contrib.layers.crossed_column": {},
  "tf.contrib.layers.bucketized_column": {},
  "tf.strings.regex_replace": {},
  "tf.contrib.learn.estimator": {
    "tf.estimator.estimator": [
      {
        "title": "47254759",
        "h": "tf.contrib.learn.estimator",
        "t": "tf.estimator.estimator",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "afaik , you cannot incorporate l1 or l2 regularizations on provided estimators , like the subclasses of tf.estimator.estimator and tf.contrib.learn.estimator.",
          "nonetheless , you could create customized estimators by using tf.layers api , explained here : https :// www.tensorflow.org / extend / estimators."
        ]
      },
      {
        "title": "46230705",
        "h": "tf.contrib.learn.estimator",
        "t": "tf.estimator.estimator",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.9507959999999999",
        "sentences": [
          "evaluating every n steps , early stopping using a metric , etc .) ",
          "one more thing — to use this hook you ' ll need to update from a tf.contrib.learn.estimator ( which only accepts monitors ) to the more full - fledged and official tf.estimator.estimator ( which only accepts hooks ). "
        ]
      },
      {
        "title": "47249767",
        "h": "tf.estimator.estimator",
        "t": "tf.contrib.learn.estimator",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9408",
        "sentences": [
          "first up , you should stop using tf.contrib.learn.estimator in favor of tf.estimator.estimator , because contrib is an experimental module , and classes that have graduated to the core api ( such es estimator ) automatically get deprecated."
        ]
      },
      {
        "title": "45186929",
        "h": "tf.contrib.learn.estimator",
        "t": "tf.estimator.estimator",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.8832",
        "sentences": [
          "it seems that tf.estimator.estimator together with a model function that returns tf.estimator.estimatorspec is the most current one that is used in the newer examples and the one to be used in new code.",
          "my guess now is that the tf.contrib.learn.estimator is an early prototype that got replaced by the tf.estimator.estimator."
        ]
      }
    ]
  },
  "tf.contrib.tpu.keras_to_tpu_model": {},
  "tf.contrib.tpu.tpudistributionstrategy": {},
  "tf.contrib.layers.relu": {},
  "tf.nn.relu": {},
  "tf.image.central_crop": {},
  "tf.trainable_variables": {
    "tf.variable": [
      {
        "title": "38580555",
        "h": "tf.trainable_variables",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.9016000000000001",
        "sentences": [
          "since tf.trainable_variables () returns a list of tf.variable objects , you should be able to pass its result straight to session.run (): "
        ]
      },
      {
        "title": "46369727",
        "h": "tf.variable",
        "t": "tf.trainable_variables",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.97",
        "r_prob": "0.95",
        "prob": "0.8662099999999999",
        "sentences": [
          "actually , it ' s pretty straightforward.",
          "a tf.variable constructor has an argument trainable.",
          "so at any moment you can invoke tf.trainable_variables () to see the variables that are going to be updated through backprop.",
          "a tf.placeholder only adds a node to the graph."
        ]
      },
      {
        "title": "43075138",
        "h": "tf.trainable_variables",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.86",
        "r_prob": "0.94",
        "prob": "0.6952239999999998",
        "sentences": [
          "the easiest way to turn a variable name into a tf.variable object is to filter tf.trainable_variables (), matching on the name : ",
          "once you have a tf.variable object , you can use its load () method to assign a new weight : "
        ]
      },
      {
        "title": "43730032",
        "h": "tf.variable",
        "t": "tf.trainable_variables",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.98",
        "r_prob": "0.74",
        "prob": "0.674436",
        "sentences": [
          "you can create non - trainable variables in two different ways : ",
          "tf.variable ( a , trainable = false ). ",
          "tf.get_variable (\" a \", a , trainable = false ). ",
          "also there is no easy way to check whether the variable is trainable ( you need to check whether the name of your variable is in the list of tf.trainable_variables () "
        ]
      },
      {
        "title": "37624060",
        "h": "tf.variable",
        "t": "tf.trainable_variables",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.72",
        "r_prob": "0.94",
        "prob": "0.629424",
        "sentences": [
          "most tensorflow tensors ( tf.tensor objects ) are immutable , so you cannot simply assign a value to them.",
          "however , if you created the tensor as a tf.variable , you can assign a value to it by calling variable.assign (). ",
          "the code you have unnecessarily converts a tf.variable object ( from the list of tf.trainable_variables ()) into a string name."
        ]
      },
      {
        "title": "48460190",
        "h": "tf.trainable_variables",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.8",
        "r_prob": "0.71",
        "prob": "0.51688",
        "sentences": [
          "the distinction between trainable variables and non - trainable variables is used to let optimizers know which variables they can act upon.",
          "when defining a tf.variable (), setting trainable = true ( the default ) automatically adds the variable to the graphkeys.trainable_variables collection.",
          "during training , an optimizer gets the content of that collection via tf.trainable_variables () and applies the training to all of them."
        ]
      },
      {
        "title": "36193677",
        "h": "tf.variable",
        "t": "tf.trainable_variables",
        "r": "S1",
        "h_prob": "0.66",
        "t_prob": "0.97",
        "r_prob": "0.64",
        "prob": "0.409728",
        "sentences": [
          "in tensorflow , trained weights are represented by tf.variable objects.",
          "if you created a tf.variable — e.g.",
          "if you do not currently have a pointer to the tf.variable , you can get a list of the trainable variables in the current graph by calling tf.trainable_variables (). ",
          "this function returns a list of all trainable tf.variable objects in the current graph , and you can select the one that you want by matching the v.name property."
        ]
      }
    ]
  },
  "tf.sparse_concat": {},
  "tf.ones_like": {},
  "tf.zeros_like": {},
  "tf.contrib.layers.conv2d": {},
  "tf.contrib.layers.linear": {},
  "tf.contrib.layers.fully_connected": {},
  "tf.cond": {
    "tf.map_fn": [
      {
        "title": "44952751",
        "h": "tf.cond",
        "t": "tf.map_fn",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9408",
        "sentences": [
          "the general recipe for a problem like this is to use tf.map_fn () to create a new tensor with the appropriate value by applying a function to each row.",
          "now the we have a condition , we can use tf.cond () to build a conditional expression that swaps the first two elements if the condition is true : "
        ]
      },
      {
        "title": "49649378",
        "h": "tf.map_fn",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.93",
        "r_prob": "0.89",
        "prob": "0.802869",
        "sentences": [
          "you can do it with tf.map_fn and tf.cond like this : "
        ]
      },
      {
        "title": "62407166",
        "h": "tf.map_fn",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.58",
        "r_prob": "0.99",
        "prob": "0.5512319999999999",
        "sentences": [
          "there are several issues in the code , you shouldn ' t use tf.variable objects for this , those tf.map_fn are avoidable and tf.cond must always have two branches."
        ]
      },
      {
        "title": "42792589",
        "h": "tf.cond",
        "t": "tf.map_fn",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.55",
        "prob": "0.52283",
        "sentences": [
          "a tf.tensor in tensorflow is a read - only value — in fact , a symbolic expression for computing a read - only value — so you cannot in general assign values to it.",
          "however , since the function is simple , cheap to compute , and representable using simple tensorflow ops , you can avoid using tf.map_fn () and instead use tf.where (): ",
          "( you could also use tf.where () instead of tf.cond () in the tf.map_fn () version .) "
        ]
      }
    ],
    "tf.while_loop": [
      {
        "title": "46701916",
        "h": "tf.while_loop",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.9216899999999999",
        "sentences": [
          "first of all , dead tensors are an implementation detail of tensorflow ' s control flow constructs : tf.cond () and tf.while_loop (). ",
          "let ' s consider the simpler tf.cond ( pred , true_fn , false_fn ) case.",
          "if pred is true , the dead tensor is sent along output_false ( and vice versa ) the tf.cond () implementation is set up so that the ops in true_fn depend on the output_true and the ops in the false_fn depend on output_false.",
          "this dead - tensor propagation ensures that only the ops in the appropriate branch will execute.",
          "how does tf.cond () stop a dead tensor from propagating all the way to the output ? ",
          "a second special op , called a merge op handles dead inputs differently.",
          "a merge op has two or more inputs , and it expects to get a dead input for all except one of the inputs ; it then forwards the not - dead input to its output.",
          "tf.cond () uses merge ops to combine the results from the true_fn and false_fn , and so the results of the taken branch are returned as the output of the overall tf.cond () subgraph."
        ]
      },
      {
        "title": "48528363",
        "h": "tf.while_loop",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.89",
        "r_prob": "0.99",
        "prob": "0.872289",
        "sentences": [
          "each stateful operation in a graph ( that is not in a tf.while_loop () or tf.cond ()) will execute exactly once per session.run () call."
        ]
      },
      {
        "title": "51509387",
        "h": "tf.while_loop",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.86",
        "r_prob": "0.99",
        "prob": "0.7577459999999999",
        "sentences": [
          "you cannot have a variable initializer inside a conditional.",
          "this is using tf.while_loop but it is the same as tf.cond for this purposes.",
          "so , one might expect that the if the condition is false , the branch returning a will not be executed ( since it is not needed ). "
        ]
      },
      {
        "title": "48333302",
        "h": "tf.while_loop",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.738",
        "sentences": [
          "this \" answer \" is an implementation of muskrat ' s tf.slice suggestion with the details of tf.while_loop worked out ( with help from how to use tf.while_loop () in tensorflow and https :// www.tensorflow.org / api_docs / python / tf / while_loop ). ",
          "disadvantages : ",
          "tf.while_loop is treacherous.",
          "the missing documentation of tf.while_loop is that tensors outside the body of the loop are only evaluated once , even if inner ops depend on them.",
          "presumably this could be accomplished with tf.cond statements and the appropriate flags passed in via feed_dict."
        ]
      },
      {
        "title": "61521741",
        "h": "tf.while_loop",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.75",
        "r_prob": "0.95",
        "prob": "0.66975",
        "sentences": [
          "this tf issue and so question was about how to dynamically catch an exception , so basically implementing try : ... except : ... in the tf graph.",
          "other tf functionality which introduce control structure are : ",
          "tf.while_loop.",
          "tf.cond.",
          "tf.cond is the answer to your question how you conditionally execute code.",
          "a bool scalar."
        ]
      },
      {
        "title": "46523259",
        "h": "tf.while_loop",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.73",
        "r_prob": "0.93",
        "prob": "0.6313770000000001",
        "sentences": [
          "if / else blocks , for , while and other python stuff should be replaced with appropriate tensorflow operations like tf.while_loop , tf.cond and so on.",
          "thus , when you are calling sess.run () you get python object ( more precisely , numpy one ). ",
          "you could either evaluate z with another sess.run () and then switch to regular python operations , or properly use tf.cond and create a subgraph based on the values of cost and z which are both tensors."
        ]
      }
    ],
    "tf.control_dependencies": [
      {
        "title": "45056367",
        "h": "tf.control_dependencies",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.7360000000000001",
        "sentences": [
          "then , if you want to return 0 when a condition is met you can ' t use python if statement ( that ' s not computed insede the tensorflow graph ) but you have to use tf.cond ( computation inside the graph ). ",
          "to force the evaluation of tf.cond after the p and r update , you can use tf.control_dependencies "
        ]
      },
      {
        "title": "57659781",
        "h": "tf.control_dependencies",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.96",
        "r_prob": "0.97",
        "prob": "0.6797759999999999",
        "sentences": [
          "interesting question.",
          "to execute a tf operation just in the first epoch , one could use tf.cond and tf.control_dependencies to check / update the value of a boolean tensor."
        ]
      },
      {
        "title": "50645142",
        "h": "tf.control_dependencies",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.55",
        "t_prob": "0.85",
        "r_prob": "0.99",
        "prob": "0.46282500000000004",
        "sentences": [
          "both optimizers compute the variable updates from the out computed with the values that the variables had before calling session.run.",
          "optimize the ( possibly weighted ) sum of both losses ( tf.train.adamoptimzer ( 0.001 ). minimize ( loss_one + loss_two )).. ",
          "use tf.control_dependencies to make sure that one optimization step always takes place after the other.",
          "however this means that using the second optimizer will always require using the first one ( could be work around , maybe with tf.cond , but it ' s more of a hassle ).. "
        ]
      }
    ],
    "tf.where": [
      {
        "title": "49649378",
        "h": "tf.cond",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.96",
        "r_prob": "0.82",
        "prob": "0.732096",
        "sentences": [
          "you can do it with tf.map_fn and tf.cond like this : ",
          "but you can also simply use tf.where like this : "
        ]
      },
      {
        "title": "53212479",
        "h": "tf.where",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.74",
        "r_prob": "0.95",
        "prob": "0.61864",
        "sentences": [
          "in the case of tf.where , you have a function with three inputs , condition c , value on true t and value on false f , and one output out.",
          "currently , no gradient is computed for the condition ( that would hardly make sense ), so you just need to do the gradients for t and f.assuming the input and the outputs are vectors , imagine c [ 0 ] is true.",
          "for tf.cond , the code is a bit more complicated , because the same operation ( merge ) is used in different contexts , and also tf.cond also uses switch operations inside."
        ]
      },
      {
        "title": "42164494",
        "h": "tf.where",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.58",
        "r_prob": "1.0",
        "prob": "0.5451999999999999",
        "sentences": [
          "tf.where sounds like what you want : a vectorized selection between tensors.",
          "tf.cond is a control flow modifier : it determines which ops are executed , and so it ' s difficult to think of useful batch semantics."
        ]
      },
      {
        "title": "51453667",
        "h": "tf.cond",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.61",
        "r_prob": "0.93",
        "prob": "0.45951300000000006",
        "sentences": [
          "the error occurs because tf.cond takes a decision based on a single boolean — much like an if statement.",
          "you could use tf.where to fix that problem , but then you will run into another one , which is that trainable is not a property that you can fix at runtime , it is part of the definition of a variable."
        ]
      }
    ],
    "select": [
      {
        "title": "34964654",
        "h": "select",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.22",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.21559999999999999",
        "sentences": [
          "i ' d like to draw your attention to two other ways you can compute conditional expressions using tensorflow , which don ' t require you to evaluate the predicate and get a python value back.",
          "the first way uses the tf.select () op to conditionally pass through values from two tensors passed as arguments : ",
          "the drawback of tf.select () is that it evaluates both val_if_true and val_if_false before computing the result , which might be expensive if they are complicated expressions.",
          "the second way uses the tf.cond () op , which conditionally evaluates one of two expressions."
        ]
      },
      {
        "title": "40695473",
        "h": "select",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.15",
        "t_prob": "0.99",
        "r_prob": "0.75",
        "prob": "0.111375",
        "sentences": [
          "if you want to vary at runtime you can use tf.cond or tf.select."
        ]
      },
      {
        "title": "37868116",
        "h": "select",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.05",
        "t_prob": "0.61",
        "r_prob": "1.0",
        "prob": "0.0305",
        "sentences": [
          "you are right , tf.select wants the condition to be of the same shape as the two other inputs.",
          "you should instead use tf.cond."
        ]
      },
      {
        "title": "35606830",
        "h": "select",
        "t": "tf.cond",
        "r": "S1",
        "h_prob": "0.04",
        "t_prob": "0.76",
        "r_prob": "1.0",
        "prob": "0.0304",
        "sentences": [
          "tf.select and tf.cond come in handy for situations where you have to perform computations conditionally on elements of a tensor."
        ]
      }
    ]
  },
  "tf.train.coordinator": {},
  "tf.contrib.data.make_csv_dataset": {},
  "tf.summary.scalar": {
    "tf.summary.histogram": [
      {
        "title": "54695716",
        "h": "tf.summary.histogram",
        "t": "tf.summary.scalar",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.9405",
        "sentences": [
          "generally , you just need to specify tf.summary.scalar (), tf.summary.histogram () or tf.summary.image () anywhere in the code."
        ]
      },
      {
        "title": "42967733",
        "h": "tf.summary.histogram",
        "t": "tf.summary.scalar",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.8918",
        "sentences": [
          "to perform your own logging in tensorboard , use something like tf.summary.scalar or tf.summary.histogram."
        ]
      },
      {
        "title": "49921758",
        "h": "tf.summary.histogram",
        "t": "tf.summary.scalar",
        "r": "S1",
        "h_prob": "0.72",
        "t_prob": "0.93",
        "r_prob": "0.74",
        "prob": "0.495504",
        "sentences": [
          "this gives you a scalar value.",
          "you can add it to tensorboard using tf.summary.scalar.",
          "the same as you do when using tf.summary.histogram to view values."
        ]
      }
    ]
  },
  "tf.initialize_all_vra": {},
  "tf.merge_all_summaries": {},
  "tf.variables_initializer": {
    "tf.global_variables_initializer": [
      {
        "title": "48505953",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "looking at the documentation the init = tf.global_variables_initializer () is the same as init = tf.variables_initializer ( tf.global_variables ()) ",
          "instead , the vanilla gradient descent optimizer tf.train.gradientdescentoptimizer does not depend on any variables.",
          "if you would place the init = tf.global_variables_initializer () before the tf.train.adamoptimizer like ",
          "so tf.global_variables_initializer () does not see the needed variables of adam , when placing init = tf.global_variables_initializer () before the adam definition tf.train.adamoptimizer."
        ]
      },
      {
        "title": "43583960",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "in your case the error even explains what variable was not initialized : attempting to use uninitialized value variable_1.",
          "one of the tf tutorials explains a lot about variables , their creation / initialization / saving / loading ",
          "basically to initialize the variable you have 3 options : ",
          "initialize all global variables with tf.global_variables_initializer (). ",
          "initialize variables you care about with tf.variables_initializer ( list_of_vars ). "
        ]
      },
      {
        "title": "43773554",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "failedpreconditionerror : attempting to use uninitialized value is one of the most frequent errors related to tensorflow.",
          "in your case the error even explains what variable was not initialized : attempting to use uninitialized value variable_1.",
          "one of the tf tutorials explains a lot about variables , their creation / initialization / saving / loading ",
          "basically to initialize the variable you have 3 options : ",
          "initialize all global variables with tf.global_variables_initializer (). ",
          "initialize variables you care about with tf.variables_initializer ( list_of_vars ). "
        ]
      },
      {
        "title": "54401883",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "the easiest way is initializing all variables at once using : tf.global_variables_initializer () ",
          "you use sess.run ( init ) to run the initializer , without fetching any value.",
          "to initialize only a subset of variables , you use tf.variables_initializer () listing the variables : "
        ]
      },
      {
        "title": "47271906",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "0.95",
        "prob": "0.9123799999999999",
        "sentences": [
          "the tf.global_variables_initializer just initializes all variables that tf.global_variables () would list.",
          "this actually makes much sense in a distributed environment where the graph might be located in different computing nodes in a cluster.",
          "in such a case , tf.global_variables_initializer () which is just an alias for tf.variables_initializer ( tf.global_variables ()) would initialize all the variables in all the computing nodes , where the graph is placed."
        ]
      },
      {
        "title": "52725027",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.99",
        "r_prob": "0.95",
        "prob": "0.9122849999999999",
        "sentences": [
          "tf.global_variables_initializer makes an initialization op for all the global variables created up to that point."
        ]
      },
      {
        "title": "45976536",
        "h": "tf.global_variables_initializer",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9009",
        "sentences": [
          "if var_list is empty , however , the function still returns an op that can be run.",
          "that op just has no effect.",
          "the load script has no global varibale , and since tf.global_variables_initializer () is equivalent to tf.variables_initializer ( tf.global_variables ()), the operation is a no - op."
        ]
      }
    ],
    "tf.global_variables": [
      {
        "title": "47916956",
        "h": "tf.global_variables",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "global_variables and local_variables contain all variables of the graph , which need to be initialized before training.",
          "tf.global_variables () returns the global variables in a list and can be used with tf.variables_initializer for initialization .. ",
          "summaries contains keys for all summaries added by tf.summary ( scalar , image , histogram , text , etc ). "
        ]
      },
      {
        "title": "47271906",
        "h": "tf.global_variables",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "the tf.global_variables_initializer just initializes all variables that tf.global_variables () would list.",
          "this actually makes much sense in a distributed environment where the graph might be located in different computing nodes in a cluster.",
          "in such a case , tf.global_variables_initializer () which is just an alias for tf.variables_initializer ( tf.global_variables ()) would initialize all the variables in all the computing nodes , where the graph is placed."
        ]
      },
      {
        "title": "48505953",
        "h": "tf.global_variables",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9108",
        "sentences": [
          "looking at the documentation the init = tf.global_variables_initializer () is the same as init = tf.variables_initializer ( tf.global_variables ()) ",
          "instead , the vanilla gradient descent optimizer tf.train.gradientdescentoptimizer does not depend on any variables."
        ]
      }
    ]
  },
  "tf.resour": {},
  "tf.res": {},
  "batch - size": {},
  "tf.gfile.fastgfile": {},
  "tf.gfile.gfile": {},
  "tf.get_variable": {
    "tf.variable": [
      {
        "title": "55415113",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.941094",
        "sentences": [
          "i believe that can ' t be done.",
          "as the main aim for tf.get_variable () is to search for a variable with the same name first and if it didn ' t find it it creates a new one.",
          "so if you just want to create a new variable use tf.variable () instead "
        ]
      },
      {
        "title": "63075230",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "0.96",
        "prob": "0.9125759999999999",
        "sentences": [
          "if you define a variable with a name that has been defined before , then tensorflow throws an exception.",
          "hence , it is convenient to use the tf.get_variable () function instead of tf.variable (). "
        ]
      },
      {
        "title": "41501486",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.93",
        "r_prob": "0.98",
        "prob": "0.902286",
        "sentences": [
          "when you simply write x + 300 , you are not creating a tf.variable.",
          "you need to explicitly use tf.get_variable () or tf.variable () to create a variable which can be saved."
        ]
      },
      {
        "title": "49660916",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.96",
        "r_prob": "0.94",
        "prob": "0.8482559999999998",
        "sentences": [
          "instantiating variables ",
          "tf.variable () constructer prefixes variable name with current name_scope and variable_scope ",
          "tf.get_variable () constructor ignores name_scope and only prefixes name with the current variable_scope "
        ]
      },
      {
        "title": "39104281",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.98",
        "r_prob": "0.96",
        "prob": "0.837312",
        "sentences": [
          "the problem is that you should be using tf.get_variable () to create your variables , instead of tf.variable (), if you are reusing a scope."
        ]
      },
      {
        "title": "37115316",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.88",
        "r_prob": "0.91",
        "prob": "0.776776",
        "sentences": [
          "you might want to use tf.get_variable () instead of ' tf.variable `. ",
          "name_scope is originally used for managing operation names ( such as add , matmul ), because tf.variable is actually an operation and its operation name will be \" inherited \" by variables created by it , so the name of name_scope rather than variable_scope is used as prefix.",
          "but if you want to use tf.variable , you can also directly use name_scope in with statement : ",
          "if name has been used before , it will be made unique by calling self.unique_name ( name ). "
        ]
      },
      {
        "title": "52535494",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.88",
        "r_prob": "0.96",
        "prob": "0.768768",
        "sentences": [
          "this is because tf.variable is a low level method which stores created variable in globals ( or locals ) collection while tf.get_variable keeps account of the variable it has created by storing them in a variable store.",
          "when you first call tf.variable , the variable created is not added to the variable store letting think that no variable with name \" test \" has been created.",
          "so , when you later call tf.get_variable (\" test \") it will look at the variable store , see that no variable with name \" test \" is in it.",
          "it will thus call tf.variable , which will create a variable with an incremented name \" test_1 \" stored in the variable store under the key \" test \". "
        ]
      },
      {
        "title": "37102908",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.8",
        "r_prob": "0.95",
        "prob": "0.7372",
        "sentences": [
          "i ' d recommend to always use tf.get_variable (...) -- it will make it way easier to refactor your code if you need to share variables at any time , e.g.",
          "in a multi - gpu setting ( see the multi - gpu cifar example ). ",
          "there is no downside to it.",
          "pure tf.variable is lower - level ; at some point tf.get_variable () did not exist so some code still uses the low - level way."
        ]
      },
      {
        "title": "41525110",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.89",
        "r_prob": "0.91",
        "prob": "0.704613",
        "sentences": [
          "call tf.initialize_all_variables ()) after you declared all variables.",
          "creating a variable via tf.get_variable or tf.variable places it in global_variables collection ( unless otherwise specified with collections kwarg ). "
        ]
      },
      {
        "title": "49672443",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.87",
        "r_prob": "0.94",
        "prob": "0.6951299999999999",
        "sentences": [
          "the use of variables in tensorflow can be sometimes confusing.",
          "when you do my_var = tf.variable (...) or my_var = tf.get_variable (...), my_var is a tf.variable object than can hold values for the duration of a session.",
          "to change the value , we could do my_var = tf.assign ( my_var / 2 ). "
        ]
      },
      {
        "title": "51238155",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.84",
        "r_prob": "0.97",
        "prob": "0.635544",
        "sentences": [
          "oh , you should never use tf.variable unless you have a very good reason.",
          "you should use tf.get_variable instead to avoid issues."
        ]
      },
      {
        "title": "46774528",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.91",
        "r_prob": "0.72",
        "prob": "0.635544",
        "sentences": [
          "tf.variable accepts an initial value upon creation ( a constant ), this explains deterministic results when you use it.",
          "tf.get_variable is slightly different : it has an initializer argument , by default none , which is interpreted like this : "
        ]
      },
      {
        "title": "50975398",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.81",
        "r_prob": "0.81",
        "prob": "0.5708070000000001",
        "sentences": [
          "however that won ' t solve your problem.",
          "in the case of variables , calling tf.variable will always create a new variable , whereas calling tf.get_variable will reuse it if it already exists."
        ]
      },
      {
        "title": "43730032",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.64",
        "t_prob": "0.93",
        "r_prob": "0.92",
        "prob": "0.5475840000000001",
        "sentences": [
          "you can create non - trainable variables in two different ways : ",
          "tf.variable ( a , trainable = false ). ",
          "tf.get_variable (\" a \", a , trainable = false ). "
        ]
      },
      {
        "title": "44711222",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.72",
        "t_prob": "0.59",
        "r_prob": "0.99",
        "prob": "0.4205519999999999",
        "sentences": [
          "the first parameter of tf.variable is the initial value of the variable , so in the upper statement x is a variable with value [ len ( _element_list ), 4 ], and it ' s rank of shape is 1.",
          "the second parameter of tf.get_variable is the shape of variable , so the rank of shape of variable x is 2."
        ]
      },
      {
        "title": "44076283",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.62",
        "r_prob": "0.87",
        "prob": "0.36139800000000005",
        "sentences": [
          "first is that tf.variable will always create a new variable , whereas tf.get_variable gets an existing variable with specified parameters from the graph , and if it doesn ' t exist , creates a new one.",
          "tf.variable requires that an initial value be specified."
        ]
      },
      {
        "title": "37534656",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.53",
        "t_prob": "0.75",
        "r_prob": "0.82",
        "prob": "0.32595",
        "sentences": [
          "it is a mechanism in tensorflow that allows for sharing variables accessed in different parts of the code without passing references to the variable around.",
          "the method tf.get_variable can be used with the name of the variable as the argument to either create a new variable with such name or retrieve the one that was created before.",
          "this is different from using the tf.variable constructor which will create a new variable every time it is called ( and potentially add a suffix to the variable name if a variable with such name already exists ). ",
          "variable scope , created using tf.variable_scope.",
          "both scopes have the same effect on all operations as well as variables created using tf.variable , i.e ., the scope will be added as a prefix to the operation or variable name.",
          "the only way to place a variable accessed using tf.get_variable in a scope is to use a variable scope , as in the following example : "
        ]
      },
      {
        "title": "47534542",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.84",
        "r_prob": "0.53",
        "prob": "0.324996",
        "sentences": [
          "as of now , it is advised to make use of tf.get_variable and avoid tf.variable as much as possible."
        ]
      },
      {
        "title": "45867685",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.5",
        "t_prob": "0.49",
        "r_prob": "0.79",
        "prob": "0.19355",
        "sentences": [
          "tf provides many nicely packaged ones .. ",
          "additionally in the past , tf.get_variable seems to be preferred over tf.variable when used from within control flow."
        ]
      }
    ],
    "tf.name_scope": [
      {
        "title": "43580096",
        "h": "tf.name_scope",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.97",
        "r_prob": "0.98",
        "prob": "0.922082",
        "sentences": [
          "now you see that tf.variable_scope () adds a prefix to the names of all variables ( no matter how you create them ), ops , constants.",
          "on the other hand tf.name_scope () ignores variables created with tf.get_variable () because it assumes that you know which variable and in which scope you wanted to use."
        ]
      },
      {
        "title": "43581502",
        "h": "tf.name_scope",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.97",
        "r_prob": "0.98",
        "prob": "0.922082",
        "sentences": [
          "now you see that tf.variable_scope () adds a prefix to the names of all variables ( no matter how you create them ), ops , constants.",
          "on the other hand tf.name_scope () ignores variables created with tf.get_variable () because it assumes that you know which variable and in which scope you wanted to use."
        ]
      },
      {
        "title": "51732374",
        "h": "tf.name_scope",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.8924",
        "sentences": [
          "you can try using tf.variable_scope instead.",
          "tf.name_scope is ignored by variables created via tf.get_variable () which is usually used by tf.layers functions.",
          "see this question for an ( albeit somewhat outdated ) explanation of the differences."
        ]
      },
      {
        "title": "37670385",
        "h": "tf.name_scope",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.82",
        "r_prob": "0.99",
        "prob": "0.738738",
        "sentences": [
          "tf.name_scope () is used to visualize variables.",
          "tf.name_scope ( name ) ",
          "wrapper for graph.name_scope () using the default graph .. ",
          "variable scope mechanism in tensorflow consists of 2 main functions : ",
          "tf.get_variable (, , ): creates or returns a variable with a given name."
        ]
      },
      {
        "title": "39981684",
        "h": "tf.name_scope",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.96",
        "r_prob": "0.56",
        "prob": "0.41932800000000003",
        "sentences": [
          "just use tf.variable_scope instead of tf.name_scope.",
          "tf.name_scope doesn ' t add prefixes to the variables created with tf.get_variable (). "
        ]
      }
    ],
    "tf.constant_initializer": [
      {
        "title": "37976597",
        "h": "tf.constant_initializer",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.8649000000000001",
        "sentences": [
          "i ' m not familiar with the tutorial but it looks like you provided tf.constant_initializer ( 0.0 ) as your data type which returns an initializer to generate constants.",
          "the third parameter of tf.get_variable () should be the data type of your variable which for a biases variable is usually something like tf.float32 or tf.float64."
        ]
      },
      {
        "title": "38820162",
        "h": "tf.constant_initializer",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "0.51",
        "prob": "0.48980399999999996",
        "sentences": [
          "the tf.constant_initializer () function might not accept a tf.tensor as an argument , but tf.get_variable () does accept a tf.tensor as its initializer argument.",
          "... which requires even fewer characters ! ",
          "the reason tf.constant_initializer () doesn ' t take an arbitrary tensor is that it is designed to initialize variables of many different shapes with the same constant value for each element.",
          "... wouldn ' t make much sense.",
          "arguably we could make tf.constant_initializer () accept a scalar tf.tensor , and then it would have semantics similar to tf.fill (), but we haven ' t had any demand for that yet."
        ]
      },
      {
        "title": "57440644",
        "h": "tf.get_variable",
        "t": "tf.constant_initializer",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.96",
        "r_prob": "0.6",
        "prob": "0.47807999999999995",
        "sentences": [
          "and below three things are from tensorflow site and also useful.",
          "tf.get_variable : gets an existing variable with these parameters or create a new one.",
          "tf.constant_initializer : initializer that generates tensors with constant values.",
          "to put it simply , the role of tf.constant_initializer is to generate constant values.",
          "in tensorflow , you should initialize variables before creating graphs and sessions.",
          "therefore you have to use tf.global_variables_initializer when using variables.",
          "actually , in your code , you don ' t have to use tf.constant_initializer because tf.get_variable uses default initializer as glorot_uniform_initializer.",
          "example code 2 : you don ' t have to use tf.constant_initializer.",
          "finally , even if you use tf.constant_initializer in tf.get_variable you should use tf.global_variables_initializer.",
          "the reason is that tf.get_variable is a variable.",
          "so you initialize variables before tf.session by using tf.global_variables_initializer.",
          "maybe you can think of initializer in tf.get_variable ( e.g tf.constant_initializer , glorot_uniform_initializer ) as initializing value by some kind of distributions when initializing variables by using tf.global_variables_initializer."
        ]
      }
    ]
  },
  "tf.image.decode_jpeg": {},
  "tf.nn.softmax_cross_entropy_with_logits_v2": {},
  "initializations": {},
  "dnn": {},
  "repeat": {},
  "tf.graph": {
    "tf.import_graph_def": [
      {
        "title": "34699391",
        "h": "tf.import_graph_def",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "the basic idea is to use tf.import_graph_def () to replace the variables in the original ( training ) graph with constants , and then write out the resulting graphdef using tf.graph.as_graph_def (). "
        ]
      },
      {
        "title": "47067335",
        "h": "tf.graph",
        "t": "tf.import_graph_def",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "graph is an abstract concept , which can be in different forms for different frontends.",
          "for python , tf.graph () would return an python object ( code ) that contains the graphdef and many utilities.",
          "for python , you can load a graphdef using tf.import_graph_def."
        ]
      },
      {
        "title": "33770771",
        "h": "tf.import_graph_def",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.93",
        "r_prob": "0.99",
        "prob": "0.893079",
        "sentences": [
          "one way to do this in tensorflow is to build the disjoint computations as separate tf.graph objects , then convert them to serialized protocol buffers using graph.as_graph_def (): ",
          "then you could compose gdef_1 and gdef_2 into a third graph , using tf.import_graph_def (): "
        ]
      }
    ],
    "tf.get_default_graph": [
      {
        "title": "51183870",
        "h": "tf.get_default_graph",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "fortunately , tensorflow has a convenient tool to spot those errors : tf.graph.finalize.",
          "it is good practice to call this function before iterating.",
          "so in your case i would call tf.get_default_graph (). finalize () before your loop and look for any error it may throw."
        ]
      },
      {
        "title": "45996277",
        "h": "tf.get_default_graph",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9309999999999999",
        "sentences": [
          "my_graph can be tf.get_default_graph () if you are using the default graph or any other tf.graph ( or tf.graphdef ) object."
        ]
      },
      {
        "title": "54669386",
        "h": "tf.get_default_graph",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.92",
        "prob": "0.85652",
        "sentences": [
          "so it is exactly the same as calling tf.get_default_graph (). add_to_collections."
        ]
      },
      {
        "title": "44903831",
        "h": "tf.get_default_graph",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.83",
        "r_prob": "0.99",
        "prob": "0.632709",
        "sentences": [
          "if no graph is specified , the session constructor tries to build a graph using the default one ( that you can get using tf.get_default_graph ). "
        ]
      }
    ],
    "tf.session": [
      {
        "title": "59532982",
        "h": "tf.session",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.95",
        "r_prob": "0.96",
        "prob": "0.8572799999999999",
        "sentences": [
          "mainly you can stop using tf.session and tf.graph.",
          "use @ tf.function instead , i believe this basic structure will work : "
        ]
      },
      {
        "title": "49318734",
        "h": "tf.graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.87",
        "prob": "0.80997",
        "sentences": [
          "passing loaded_graph to the tf.session () means you can only run ops created in that graph.",
          "if you wish to have the for loop then you may need to on each loop create a new graph using ",
          "g_1 = tf.graph () with g_1.as_default (): ... "
        ]
      },
      {
        "title": "36286136",
        "h": "tf.graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.87",
        "r_prob": "0.97",
        "prob": "0.742632",
        "sentences": [
          "in in [ 8 ] you create a tf.graph called graph and set it as default for the with graph.as_default (): block.",
          "however , you exit the with block before creating ( i ) the tf.session , and ( ii ) the tf.train.saver.",
          "alternatively , you can create the tf.session inside the with graph.as_default (): block , in which case it will use graph for all of its operations."
        ]
      },
      {
        "title": "54783729",
        "h": "tf.session",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.87",
        "r_prob": "0.88",
        "prob": "0.72732",
        "sentences": [
          "estimators create and manage tf.graph and tf.session objects for you."
        ]
      },
      {
        "title": "47795685",
        "h": "tf.session",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.92",
        "r_prob": "0.77",
        "prob": "0.6588120000000001",
        "sentences": [
          "the estimator api manages the tf.graph and tf.session by itself , it uses an input_fn to feed all related ops with values."
        ]
      },
      {
        "title": "49279970",
        "h": "tf.session",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.69",
        "t_prob": "0.98",
        "r_prob": "0.96",
        "prob": "0.6491519999999998",
        "sentences": [
          "when you use dataset.map ( map_func ), tensorflow defines a subgraph for all the ops created in the function map_func , and arranges to execute it efficiently in the same session as the rest of your graph.",
          "there is almost never any need to create a tf.graph or tf.session inside map_func : if your parsing function is made up of tensorflow ops , these ops can be embedded directly in the graph that defines the input pipeline.",
          "if your map_func contains non - tensorflow operations that you want to apply to each element , you should wrap them in a tf.py_func () ( or dataset.from_generator (), if the data generation process is defined in python logic ). "
        ]
      },
      {
        "title": "47793706",
        "h": "tf.graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.66",
        "t_prob": "0.82",
        "r_prob": "0.77",
        "prob": "0.41672400000000004",
        "sentences": [
          "the kmeansclustering estimator api builds its own tf.graph and manage tf.session by itself , so you don ' t need to run a tf.session to feed values ( that is done by input_fn ), that ' s why the valueerror arise.",
          "here x is not a tf.placeholder that needs to be feed at a tf.session run."
        ]
      }
    ],
    "tf.device": [
      {
        "title": "51183870",
        "h": "tf.device",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.834372",
        "sentences": [
          "the fact that you specify a device ( with tf.device ('/ gpu : 0 ) for your loop is a hint that it is the case : you typically specify a device for new nodes as this does not affect nodes that are already defined.",
          "fortunately , tensorflow has a convenient tool to spot those errors : tf.graph.finalize."
        ]
      },
      {
        "title": "59532982",
        "h": "tf.device",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.95",
        "r_prob": "0.8",
        "prob": "0.7524000000000001",
        "sentences": [
          "with tf.device (...): applies to the graph nodes created within the scope , not session.run calls.",
          "mainly you can stop using tf.session and tf.graph.",
          "use @ tf.function instead , i believe this basic structure will work : "
        ]
      },
      {
        "title": "40873770",
        "h": "tf.device",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.96",
        "r_prob": "0.68",
        "prob": "0.55488",
        "sentences": [
          "yes.",
          "this process only needs to create part of the graph for the current node using with tf.device.",
          "if you use within - graph replication with single client , your client needs to create graph for all nodes using multiple with tf.graph sections .. "
        ]
      }
    ],
    "tf.operation": [
      {
        "title": "50802716",
        "h": "tf.operation",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.97",
        "r_prob": "0.89",
        "prob": "0.7079059999999999",
        "sentences": [
          "this import statement imports the tensorflow.python.framework.ops module , which ( at present ) includes the implementations of classes like tf.graph , tf.tensor , and tf.operation."
        ]
      },
      {
        "title": "54370971",
        "h": "tf.graph",
        "t": "tf.operation",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.68",
        "r_prob": "0.97",
        "prob": "0.646408",
        "sentences": [
          "for getting tf.operation by names , you can use tf.graph.get_operation_by_name "
        ]
      },
      {
        "title": "42600405",
        "h": "tf.operation",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.93",
        "r_prob": "0.87",
        "prob": "0.550188",
        "sentences": [
          "the tf.graph.get_operation_by_name () method always returns a tf.operation object.",
          "when you pass a tf.operation object to tf.session.run (), tensorflow will execute that operation ( and everything on which it depends ) and discard its outputs ( if any ). ",
          "get a tf.tensor from the graph by calling tf.graph.get_tensor_by_name (), and appending \":< output index >\" to the operation ' s name : "
        ]
      }
    ]
  },
  "tf.compat.v1.keras.layers.cudnnlstm": {},
  "tf.keras.layers.cudnnlstm": {},
  "tf.control_": {},
  "tf.layers.dense": {},
  "tf.int32": {
    "tf.placeholder": [
      {
        "title": "35749825",
        "h": "tf.int32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.98",
        "prob": "0.931588",
        "sentences": [
          "you have to change your placeholder is_train = tf.placeholder ( tf.int32 ) for a tf.variable : is_train = tf.variable ( true , name =' training '). "
        ]
      },
      {
        "title": "38453485",
        "h": "tf.int32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.99",
        "r_prob": "0.98",
        "prob": "0.911988",
        "sentences": [
          "a = tf.placeholder ( tf.int32 , shape =[ none , 100 ]) a is a placeholder , not a variable."
        ]
      },
      {
        "title": "40274013",
        "h": "tf.int32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9117999999999999",
        "sentences": [
          "tf.placeholder ( tf.int32 , shape =[ none , none , seq_len ]) ( replacing seq_len with none if appropriate ). "
        ]
      },
      {
        "title": "39886448",
        "h": "tf.int32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.98",
        "r_prob": "0.98",
        "prob": "0.854756",
        "sentences": [
          "use ",
          "tf.placeholder ( tf.int32 ) "
        ]
      },
      {
        "title": "34428867",
        "h": "tf.int32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.96",
        "r_prob": "0.98",
        "prob": "0.84672",
        "sentences": [
          "the main issue is that you ' re using a single tensor for inputs and outputs , as in : inputs = tf.placeholder ( tf.int32 , [ batch_size , num_steps ]). ",
          "in tensorflow the rnn functions take a list of tensors ( because num_steps can vary in some models ). ",
          "so you should construct inputs like this : inputs = [ tf.placeholder ( tf.int32 , [ batch_size , 1 ]) for _ in xrange ( num_steps )] "
        ]
      },
      {
        "title": "44891572",
        "h": "tf.int32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.97",
        "r_prob": "0.97",
        "prob": "0.837401",
        "sentences": [
          "labels expects the input to be [ batch_size , num_classes ] but you are feeding it [ batch_size ]. ",
          "change to labels = tf.placeholder ( tf.int32 , [ none ]) and use tf.one_hot ( labels , num_classes ) when you pass it to the tf.nn.softmax_cross_entropy_with_logits () function."
        ]
      }
    ],
    "tf.float32": [
      {
        "title": "49626923",
        "h": "tf.int32",
        "t": "tf.float32",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.88",
        "r_prob": "1.0",
        "prob": "0.8623999999999999",
        "sentences": [
          "it wrong to use tf.cast (..., tf.int32 ) and actually , there is no need to use tf.cast (..., tf.float32 ) because it ' s already been tf.float32.",
          "p.s."
        ]
      },
      {
        "title": "46654346",
        "h": "tf.float32",
        "t": "tf.int32",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.825858",
        "sentences": [
          "first of all , i would recommend quantifying the performance overhead of feeding x and y each time you initialize the iterator.",
          "for primitive types like tf.int32 and tf.float32 it is often possible to feed a value without copying any data , and in that case the overhead will be negligible."
        ]
      },
      {
        "title": "47020866",
        "h": "tf.int32",
        "t": "tf.float32",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.66",
        "r_prob": "1.0",
        "prob": "0.5148",
        "sentences": [
          "if you don ' t specify it and don ' t have any initializer , the dtype will be tf.float32 by default and the loading of tf.int32 will fail."
        ]
      }
    ],
    "tf.float": [
      {
        "title": "47374305",
        "h": "tf.int32",
        "t": "tf.float",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.68",
        "r_prob": "0.98",
        "prob": "0.639744",
        "sentences": [
          "you ' re trying to multiple tensors with a tf.int32 ( dt_int32 ) datatype on the gpu."
        ]
      },
      {
        "title": "43853600",
        "h": "tf.float",
        "t": "tf.int32",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.6324000000000001",
        "sentences": [
          "another issue is the different data type between your matrix ( tf.int32 ) and the tensorarray ( tf.float32 ), based on your code you ' re multiplying the matrix ints by 2 and writing the result into the array so it should be int32 as well."
        ]
      },
      {
        "title": "52577242",
        "h": "tf.int32",
        "t": "tf.float",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.73",
        "r_prob": "0.97",
        "prob": "0.5806419999999999",
        "sentences": [
          "p , a = tf.get_session_tensor ( h.handle , tf.float32 ) ",
          "p , a = tf.get_session_tensor ( h.handle , tf.int32 ) "
        ]
      }
    ]
  },
  "tf.relu_layer": {},
  "binary_crossentropy": {},
  "multi -": {},
  "tf.no_op": {},
  "tf.merge_all_summar": {},
  "tf.merge_": {},
  "tf.nn.static_rnn": {
    "tf.nn.dynamic_rnn": [
      {
        "title": "53523873",
        "h": "tf.nn.static_rnn",
        "t": "tf.nn.dynamic_rnn",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9009",
        "sentences": [
          "a couple of important points from the doc : ",
          "use tf.contrib.cudnn_rnn () on nvidia gpus ;. ",
          "use tf.nn.dynamic_rnn () instead of tf.nn.static_rnn (). "
        ]
      },
      {
        "title": "48670578",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.static_rnn",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.85",
        "r_prob": "1.0",
        "prob": "0.833",
        "sentences": [
          "note that this way you won ' t need a loop of window_size , because tf.nn.static_rnn or tf.nn.dynamic_rnn will instantiate the cells wrapped with attention."
        ]
      },
      {
        "title": "47968175",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.static_rnn",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.8188000000000001",
        "sentences": [
          "lstmblockfusedcell is inherited from fusedrnncell instead of rnncell , so you cannot use standard tf.nn.static_rnn or tf.nn.dynamic_rnn in which they require rnncell instance ( as shown in your error message ). "
        ]
      },
      {
        "title": "44479970",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.static_rnn",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.77",
        "r_prob": "1.0",
        "prob": "0.7314999999999999",
        "sentences": [
          "tf.nn.static_rnn vs.tf.nn.dynamic_rnn.",
          "internally , tf.nn.static_rnn creates an unrolled graph for a fixed rnn length.",
          "that means that , if you call tf.nn.static_rnn with inputs having 200 time - steps you are creating a static graph with 200 rnn steps.",
          "second , you ’ re unable to pass in longer sequences (> 200 ) than you ' ve originally specified.",
          "tf.nn.dynamic_rnn solves this.",
          "it uses a tf.while_loop to dynamically construct the graph when it is executed.",
          "what about performance ?. ",
          "you may think the tf.nn.static_rnn is faster than its dynamic counterpart because it pre - builds the graph.",
          "please note , it is strongly encouraged to use tf.nn.dynamic_rnn."
        ]
      },
      {
        "title": "47284043",
        "h": "tf.nn.static_rnn",
        "t": "tf.nn.dynamic_rnn",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.7227",
        "sentences": [
          "tf.nn.dynamic_rnn ( also tf.nn.static_rnn ) has two return values ; \" outputs \", \" state \" ( https :// www.tensorflow.org / api_docs / python / tf / nn / dynamic_rnn ) "
        ]
      },
      {
        "title": "51426029",
        "h": "tf.nn.static_rnn",
        "t": "tf.nn.dynamic_rnn",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.94",
        "r_prob": "0.96",
        "prob": "0.6407039999999998",
        "sentences": [
          "second , you ’ re unable to pass in longer sequences (> 200 ) than you ’ ve originally specified.",
          "dynamic ",
          "tf.nn.dynamic_rnn solves this.",
          "in general he concludes that there is no real benefit in using tf.nn.static_rnn and that for most cases you ' ll want to resort to tf.nn.dynamic_rnn "
        ]
      },
      {
        "title": "45279243",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.static_rnn",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.85",
        "r_prob": "0.7",
        "prob": "0.5533499999999999",
        "sentences": [
          "in your case , it will break x which is of size ( batch_size , time_step , data_size ) into a list of length time_step containing tensors of size ( batch_size , data_size ). ",
          "this is needed for tf.nn.static_rnn since it unfolds the rnn during graph creation so it needs a pre - specified number of step which is the length of the list coming from tf.unstack.",
          "tf.nn.dynamic_rnn is unfolded in each run so that it can do a variable number of steps , therefore it takes one tensor where dimension 0 is the batch_size , dimension 1 is the time_step and dimension 2 is the data_size ( or the first two dimensions are reversed if time_major is true ). ",
          "tl ; dr use tf.unstack with tf.nn.static_rnn but never use it with tf.nn.dynamic_rnn."
        ]
      }
    ]
  },
  "tf.nn.dynamic_rnn": {
    "tf.nn.rnn": [
      {
        "title": "38465663",
        "h": "tf.nn.rnn",
        "t": "tf.nn.dynamic_rnn",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "the tf.nn.dynamic_rnn () or tf.nn.rnn () operations allow to specify the initial state of the rnn using the initial_state parameter.",
          "in tensorflow , you can wrap tensors in tf.variable () to keep their values in the graph between multiple session runs."
        ]
      },
      {
        "title": "36267491",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.rnn",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "for rnns specifically , there are two options : tf.nn.rnn , and tf.nn.dynamic_rnn.",
          "the first function creates t subgraphs , where t is the length of the python list of inputs you provide ( that is , inputs is a len t python list of shape [ batch , depth ] tensors ). ",
          "in this case , there is exactly one subgraph for the \" time step \", and it gets run over and over until your input has been processed."
        ]
      },
      {
        "title": "36950950",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.rnn",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9503999999999999",
        "sentences": [
          "use tf.nn.rnn or tf.nn.dynamic_rnn which do this , and a lot of other nice things , for you."
        ]
      },
      {
        "title": "40986014",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.rnn",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9215",
        "sentences": [
          "that means , if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 rnn steps.",
          "second , you ’ re unable to pass in longer sequences (> 200 ) than you ’ ve originally specified.",
          "tf.nn.dynamic_rnn solves this.",
          "it uses a tf.while loop to dynamically construct the graph when it is executed."
        ]
      },
      {
        "title": "42497900",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.rnn",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.8929999999999999",
        "sentences": [
          "you should use tf.nn.dynamic_rnn.",
          "fyi : what is the upside of using tf.nn.rnn instead of tf.nn.dynamic_rnn in tensorflow ? "
        ]
      },
      {
        "title": "51426029",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.rnn",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.8929999999999999",
        "sentences": [
          "this is still a useful resource ( despite being written a couple years ago ): http :// www.wildml.com / 2016 / 08 / rnns - in - tensorflow - a - practical - guide - and - undocumented - features / ",
          "in it , denny britz has the following comment on the static / dynamic issue : ",
          "static ",
          "internally , tf.nn.rnn creates an unrolled graph for a fixed rnn length.",
          "that means , if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 rnn steps.",
          "second , you ’ re unable to pass in longer sequences (> 200 ) than you ’ ve originally specified.",
          "dynamic ",
          "tf.nn.dynamic_rnn solves this.",
          "it uses a tf.while loop to dynamically construct the graph when it is executed."
        ]
      },
      {
        "title": "42627927",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.rnn",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.67",
        "r_prob": "1.0",
        "prob": "0.6499",
        "sentences": [
          "as of version 1.0 of the api tf.nn.rnn was removed.",
          "try using tf.nn.dynamic_rnn."
        ]
      }
    ],
    "tf.nn.static_rnn": [
      {
        "title": "53523873",
        "h": "tf.nn.static_rnn",
        "t": "tf.nn.dynamic_rnn",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9009",
        "sentences": [
          "a couple of important points from the doc : ",
          "use tf.contrib.cudnn_rnn () on nvidia gpus ;. ",
          "use tf.nn.dynamic_rnn () instead of tf.nn.static_rnn (). "
        ]
      },
      {
        "title": "48670578",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.static_rnn",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.85",
        "r_prob": "1.0",
        "prob": "0.833",
        "sentences": [
          "note that this way you won ' t need a loop of window_size , because tf.nn.static_rnn or tf.nn.dynamic_rnn will instantiate the cells wrapped with attention."
        ]
      },
      {
        "title": "47968175",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.static_rnn",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.8188000000000001",
        "sentences": [
          "lstmblockfusedcell is inherited from fusedrnncell instead of rnncell , so you cannot use standard tf.nn.static_rnn or tf.nn.dynamic_rnn in which they require rnncell instance ( as shown in your error message ). "
        ]
      },
      {
        "title": "44479970",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.static_rnn",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.77",
        "r_prob": "1.0",
        "prob": "0.7314999999999999",
        "sentences": [
          "tf.nn.static_rnn vs.tf.nn.dynamic_rnn.",
          "internally , tf.nn.static_rnn creates an unrolled graph for a fixed rnn length.",
          "that means that , if you call tf.nn.static_rnn with inputs having 200 time - steps you are creating a static graph with 200 rnn steps.",
          "second , you ’ re unable to pass in longer sequences (> 200 ) than you ' ve originally specified.",
          "tf.nn.dynamic_rnn solves this.",
          "it uses a tf.while_loop to dynamically construct the graph when it is executed.",
          "what about performance ?. ",
          "you may think the tf.nn.static_rnn is faster than its dynamic counterpart because it pre - builds the graph.",
          "please note , it is strongly encouraged to use tf.nn.dynamic_rnn."
        ]
      },
      {
        "title": "47284043",
        "h": "tf.nn.static_rnn",
        "t": "tf.nn.dynamic_rnn",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.7227",
        "sentences": [
          "tf.nn.dynamic_rnn ( also tf.nn.static_rnn ) has two return values ; \" outputs \", \" state \" ( https :// www.tensorflow.org / api_docs / python / tf / nn / dynamic_rnn ) "
        ]
      },
      {
        "title": "51426029",
        "h": "tf.nn.static_rnn",
        "t": "tf.nn.dynamic_rnn",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.94",
        "r_prob": "0.96",
        "prob": "0.6407039999999998",
        "sentences": [
          "second , you ’ re unable to pass in longer sequences (> 200 ) than you ’ ve originally specified.",
          "dynamic ",
          "tf.nn.dynamic_rnn solves this.",
          "in general he concludes that there is no real benefit in using tf.nn.static_rnn and that for most cases you ' ll want to resort to tf.nn.dynamic_rnn "
        ]
      },
      {
        "title": "45279243",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.static_rnn",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.85",
        "r_prob": "0.7",
        "prob": "0.5533499999999999",
        "sentences": [
          "in your case , it will break x which is of size ( batch_size , time_step , data_size ) into a list of length time_step containing tensors of size ( batch_size , data_size ). ",
          "this is needed for tf.nn.static_rnn since it unfolds the rnn during graph creation so it needs a pre - specified number of step which is the length of the list coming from tf.unstack.",
          "tf.nn.dynamic_rnn is unfolded in each run so that it can do a variable number of steps , therefore it takes one tensor where dimension 0 is the batch_size , dimension 1 is the time_step and dimension 2 is the data_size ( or the first two dimensions are reversed if time_major is true ). ",
          "tl ; dr use tf.unstack with tf.nn.static_rnn but never use it with tf.nn.dynamic_rnn."
        ]
      }
    ]
  },
  "tf.contrib.layers.l1_regularizer": {},
  "tf.nn.embedding_lookup_sparse": {},
  "tf.tensor": {
    "tf.placeholder": [
      {
        "title": "36240769",
        "h": "tf.placeholder",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.931392",
        "sentences": [
          "tl ; dr : you can ' t feed a tf.tensor object ( viz.",
          "unfortunately you can ' t currently use the feed / tf.placeholder () mechanism to pass the result of one tensorflow graph to another."
        ]
      },
      {
        "title": "51228411",
        "h": "tf.tensor",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.765",
        "sentences": [
          "the reason this was occuring is that every time you called model you were actually creating a bunch of tf.tensor objects , which you were attempting to add to the graph.",
          "you must create a tf.placeholder in your model graph , and load the value you want your model to process onto that placeholder."
        ]
      },
      {
        "title": "35375029",
        "h": "tf.tensor",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.87",
        "r_prob": "0.82",
        "prob": "0.6777299999999999",
        "sentences": [
          "the reason for the error is that tf.reshape () expects a value that is convertible to a tf.tensor as its second argument.",
          "tensorflow will automatically convert a list of python numbers to a tf.tensor but will not automatically convert a mixed list of numbers and tensors ( such as a tf.placeholder ())— instead raising the somewhat unintuitive error message you saw."
        ]
      },
      {
        "title": "42585542",
        "h": "tf.placeholder",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.59",
        "r_prob": "0.8",
        "prob": "0.45311999999999997",
        "sentences": [
          "as you suspect , there are some downsides to using variable - shaped tf.placeholder () ops to represent input in a tensorflow model : ",
          "for example , a call to tf.shape ( x ) returns a tf.tensor containing the true dynamic shape of a tensor x.",
          "as an extreme case , the xla compiler requires that all input tensor shapes be fully defined before code is generated , so that it can generate much more efficient kernel code where array bounds ( etc .) ",
          "xla recompiles the kernel code for each combination of input shapes , so using fixed - size tensors will avoid the recompilation overhead.",
          "however , the underlying memory allocators ( the bfc allocator for gpu memory , and tcmalloc or jemalloc for cpu memory ) tend to perform better if they have a static distribution of allocation requests ( since the requests can be satisfied from buffers that were recently freed ). "
        ]
      }
    ],
    "tf.shape": [
      {
        "title": "55145960",
        "h": "tf.shape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.99",
        "r_prob": "0.99",
        "prob": "0.901692",
        "sentences": [
          "however , in your case x has an unknown first dimension.",
          "in order to use the actual tensor shape as a regular tf.tensor ( with value only known at runtime ), you can use tf.shape : "
        ]
      },
      {
        "title": "40936677",
        "h": "tf.tensor",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "0.94",
        "prob": "0.8843519999999999",
        "sentences": [
          "if you want to use the dynamic shape of imgs in subsequent code , you should use the tf.shape () operator to get the shape as a tf.tensor.",
          "for example , instead of imgs.get_shape ()[ 2 ], you can use tf.shape ( imgs )[ 2 ]. "
        ]
      },
      {
        "title": "56761873",
        "h": "tf.tensor",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.92",
        "r_prob": "0.95",
        "prob": "0.8302999999999999",
        "sentences": [
          "when running in graph mode use tf.shape.",
          "tf.tensor.shape fails automatic shape inference when runnning in graph mode."
        ]
      },
      {
        "title": "38237169",
        "h": "tf.shape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.87",
        "r_prob": "0.95",
        "prob": "0.80997",
        "sentences": [
          "if x has a variable batch size , the only way to get the actual shape is to use the tf.shape () operator.",
          "this operator returns a symbolic value in a tf.tensor , so it can be used as the input to other tensorflow operations , but to get a concrete python value for the shape , you need to pass it to session.run (). "
        ]
      },
      {
        "title": "40920490",
        "h": "tf.tensor",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.97",
        "r_prob": "0.89",
        "prob": "0.759704",
        "sentences": [
          "a value of none for the number of rows in your placeholder means that it can vary at runtime , so you must use tf.shape ( x ) to get the shape as a tf.tensor."
        ]
      },
      {
        "title": "42585542",
        "h": "tf.tensor",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.59",
        "t_prob": "0.99",
        "r_prob": "0.98",
        "prob": "0.572418",
        "sentences": [
          "tensorflow is often able to simplify the dataflow graph when shapes are fully known.",
          "for example , a call to tf.shape ( x ) returns a tf.tensor containing the true dynamic shape of a tensor x.",
          "as an extreme case , the xla compiler requires that all input tensor shapes be fully defined before code is generated , so that it can generate much more efficient kernel code where array bounds ( etc .) ",
          "xla recompiles the kernel code for each combination of input shapes , so using fixed - size tensors will avoid the recompilation overhead.",
          "however , the underlying memory allocators ( the bfc allocator for gpu memory , and tcmalloc or jemalloc for cpu memory ) tend to perform better if they have a static distribution of allocation requests ( since the requests can be satisfied from buffers that were recently freed ). "
        ]
      },
      {
        "title": "48268674",
        "h": "tf.tensor",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.75",
        "r_prob": "0.8",
        "prob": "0.522",
        "sentences": [
          "matrix1 is an instance of tf.tensor , which supports two ways to access the shape : matrix1.shape attribute and matrix1.get_shape () method.",
          "the result of tf.tensor evaluation , a , is a numpy ndarray , which has just a.shape attribute.",
          "historically , tf.tensor had only get_shape () method , shape was added later to make it similar to numpy.",
          "and one more note : in tensorflow , tensor shape can be dynamic ( like in your example ), in which case neither get_shape () nor shape will return a number.",
          "in this case , one can use tf.shape function to access it in runtime ( here ' s an example when it might be useful ). "
        ]
      },
      {
        "title": "43960046",
        "h": "tf.shape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.88",
        "r_prob": "0.59",
        "prob": "0.472472",
        "sentences": [
          "in tensorflow , a tensor has both a static ( inferred ) shape and a dynamic ( true ) shape.",
          "the static shape can be read using the tf.tensor.get_shape method : this shape is inferred from the operations that were used to create the tensor , and may be partially complete.",
          "if the static shape is not fully defined , the dynamic shape of a tensor t can be determined by evaluating tf.shape ( t ). ",
          "so tf.shape () returns you a tensor , will always have a size of shape =( n ,), and can be calculated in a session : "
        ]
      },
      {
        "title": "43840779",
        "h": "tf.shape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.88",
        "r_prob": "0.59",
        "prob": "0.472472",
        "sentences": [
          "in tensorflow , a tensor has both a static ( inferred ) shape and a dynamic ( true ) shape.",
          "the static shape can be read using the tf.tensor.get_shape method : this shape is inferred from the operations that were used to create the tensor , and may be partially complete.",
          "if the static shape is not fully defined , the dynamic shape of a tensor t can be determined by evaluating tf.shape ( t ). ",
          "so tf.shape () returns you a tensor , will always have a size of shape =( n ,), and can be calculated in a session : "
        ]
      },
      {
        "title": "44214625",
        "h": "tf.shape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.74",
        "t_prob": "0.87",
        "r_prob": "0.68",
        "prob": "0.43778400000000006",
        "sentences": [
          "in tensorflow , a tensor has both a static ( inferred ) shape and a dynamic ( true ) shape.",
          "the static shape can be read using the tf.tensor.get_shape method : this shape is inferred from the operations that were used to create the tensor , and may be partially complete.",
          "if the static shape is not fully defined , the dynamic shape of a tensor t can be determined by evaluating tf.shape ( t ). ",
          "luckily the next few lines from the same faq tell you what to do : ",
          "the tf.tensor.set_shape method updates the static shape of a tensor object , and it is typically used to provide additional shape information when this cannot be inferred directly."
        ]
      }
    ],
    "tf.nn.embedding_lookup": [
      {
        "title": "40903921",
        "h": "tf.nn.embedding_lookup",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.8832",
        "sentences": [
          "the tf.nn.embedding_lookup ( params , ids ) function only accepts dense , rectangular tensors as the ids argument."
        ]
      },
      {
        "title": "36241501",
        "h": "tf.nn.embedding_lookup",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.8464",
        "sentences": [
          "the cell callable is a function that takes an input tf.tensor and the current state as a tf.tensor , and returns an output tf.tensor and the new state as a tf.tensor.",
          "the inputs tensor is the result of a tf.nn.embedding_lookup () operation ; the outputs list is later concatenated and used as the input to a loss calculation.",
          "tensorflow backprops from the loss through the rnn and the embedding lookup back to the model variables."
        ]
      },
      {
        "title": "34032563",
        "h": "tf.nn.embedding_lookup",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.99",
        "r_prob": "0.66",
        "prob": "0.5292540000000001",
        "sentences": [
          "if you have a tf.sparsetensor and a tf.tensor , you can use tf.sparse_tensor_dense_matmul () to multiply them."
        ]
      }
    ],
    "tf.add": [
      {
        "title": "37901852",
        "h": "tf.tensor",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.855",
        "sentences": [
          "there ' s no difference in precision between a + b and tf.add ( a , b ). ",
          "you can see from tf.tensor.overloadable_operators that following python special methods are potentially overloaded by appropriate tensorflow versions "
        ]
      },
      {
        "title": "35095052",
        "h": "tf.add",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.85",
        "r_prob": "0.99",
        "prob": "0.7910099999999999",
        "sentences": [
          "if at least one of x or y is a tf.tensor object , the expressions tf.add ( x , y ) and x + y are equivalent.",
          "the main reason you might use tf.add () is to specify an explicit name keyword argument for the created op , which is not possible with the overloaded operator version.",
          "note that if neither x nor y is a tf.tensor — for example if they are numpy arrays — then x + y will not create a tensorflow op.",
          "tf.add () always creates a tensorflow op and converts its arguments to tf.tensor objects.",
          "therefore , if you are writing a library function that might accept both tensors and numpy arrays , you might prefer to use tf.add (). ",
          "__floordiv__ ( binary // in python 3 ). "
        ]
      },
      {
        "title": "39513635",
        "h": "tf.tensor",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.7",
        "r_prob": "0.96",
        "prob": "0.6048",
        "sentences": [
          "the four examples you gave will all give the same result , and generate the same graph ( if you ignore that some of the operation names in the graph are different ). ",
          "tensorflow will convert many different python objects into tf.tensor objects when they are passed as arguments to tensorflow operators , such as tf.add () here.",
          "the + operator is just a simple wrapper on tf.add (), and the overload is used when either the left - hand or right - hand argument is a tf.tensor ( or tf.variable ). ",
          "in that case , you may wish to convert the array to a tf.tensor once ",
          "creating a tf.constant () explicitly allows you to set its name property , which can be useful for tensorboard debugging and graph visualization."
        ]
      },
      {
        "title": "37572852",
        "h": "tf.add",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.62",
        "t_prob": "0.76",
        "r_prob": "1.0",
        "prob": "0.4712",
        "sentences": [
          "here you can see a bidirectional edge because the operation reads the old value from x and then writes back the new value.",
          "a tf.tensor : the value assigned to the variable ( or added to the variable ). ",
          "in the operation tf.add ( x , y ): the graph reads the value of x as an input."
        ]
      }
    ],
    "tf.reshape": [
      {
        "title": "35375029",
        "h": "tf.tensor",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.855",
        "sentences": [
          "the reason for the error is that tf.reshape () expects a value that is convertible to a tf.tensor as its second argument.",
          "tensorflow will automatically convert a list of python numbers to a tf.tensor but will not automatically convert a mixed list of numbers and tensors ( such as a tf.placeholder ())— instead raising the somewhat unintuitive error message you saw."
        ]
      },
      {
        "title": "35464950",
        "h": "tf.reshape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.72",
        "r_prob": "0.97",
        "prob": "0.62856",
        "sentences": [
          "to elaborate on the faq entry you quoted , tensor.set_shape () is a pure - python function that improves the shape information for a given tf.tensor object.",
          "if you want to make a new tensor with that shape from the contents of t , you can use reshaped_t = tf.reshape ( t , ( 478 , 717 , 3 )). ",
          "this creates a new tf.tensor object in python ; the actual implementation of tf.reshape () does this using a shallow copy of the tensor buffer , so it is inexpensive in practice."
        ]
      },
      {
        "title": "34420827",
        "h": "tf.reshape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.55",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.5225",
        "sentences": [
          "this — not very helpful — error is raised when one of the values in the feed_dict argument to tf.session.run () is a tf.tensor object ( in this case , the result of tf.reshape ()). "
        ]
      }
    ],
    "tf.identity": [
      {
        "title": "38740948",
        "h": "tf.tensor",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.853776",
        "sentences": [
          "the typical way to rename an op is therefore to use tf.identity (). "
        ]
      },
      {
        "title": "34399966",
        "h": "tf.tensor",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.845064",
        "sentences": [
          "the typical way to rename an op is therefore to use tf.identity (), which has almost no runtime cost : "
        ]
      },
      {
        "title": "37949814",
        "h": "tf.identity",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.9",
        "r_prob": "0.82",
        "prob": "0.72324",
        "sentences": [
          "however , you can use additional \" no - op \" operations ( like you said ): ",
          "for a tf.tensor : tf.identity ( input_tensor , name =' your_new_name ') "
        ]
      }
    ],
    "tf.constant": [
      {
        "title": "35662013",
        "h": "tf.constant",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.8454999999999999",
        "sentences": [
          "the tf.constant () op takes a numpy array ( or something implicitly convertible to a numpy array ), and returns a tf.tensor whose value is the same as that array.",
          "it does not accept a tf.tensor as its argument.",
          "on the other hand , the tf.random_normal () op returns a tf.tensor whose value is generated randomly according to the given distribution each time it runs.",
          "since it returns a tf.tensor , it cannot be used as the argument to tf.constant (). "
        ]
      },
      {
        "title": "43007712",
        "h": "tf.constant",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.79",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.6241000000000001",
        "sentences": [
          "the shape argument of the tf.constant () op expects a static shape , so you can ' t use a tf.tensor as part of the argument.",
          "fortunately there is another op that will suffice : tf.fill (), which allows the shape ( its dims argument ) to be a tf.tensor."
        ]
      },
      {
        "title": "63584248",
        "h": "tf.tensor",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.94",
        "r_prob": "0.67",
        "prob": "0.5038400000000001",
        "sentences": [
          "because the embedding layer is expecting a tf.tensor and calling the model with model ( x ) does not perform that conversion.",
          "you can do it manually with tf.constant ([ array ]) and then it will work."
        ]
      },
      {
        "title": "53659222",
        "h": "tf.tensor",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.53",
        "t_prob": "0.94",
        "r_prob": "0.96",
        "prob": "0.478272",
        "sentences": [
          "so each time the tensors are evaluated , they take the value of the next element in the underlying dataset.",
          "note that the above code snippet will embed the features and labels arrays in your tensorflow graph as tf.constant () operations."
        ]
      }
    ],
    "tf.convert_to_tensor": [
      {
        "title": "47659409",
        "h": "tf.convert_to_tensor",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.8366",
        "sentences": [
          "you can use the tf.convert_to_tensor () function to do this : ",
          "also note that tensorflow will implicitly call tf.convert_to_tensor () on the inputs to any function that expects a tf.tensor as input , so you may be able to pass [[ x , x ], [ 0 , 0 ]] directly to many functions."
        ]
      },
      {
        "title": "34911086",
        "h": "tf.convert_to_tensor",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.91",
        "r_prob": "1.0",
        "prob": "0.8008000000000001",
        "sentences": [
          "each value in a feed_dict should be a numpy array ( or some other object that is trivially convertible to a numpy array ). ",
          "in particular , a tf.tensor is not a valid value for a feed_dict."
        ]
      },
      {
        "title": "35833270",
        "h": "tf.tensor",
        "t": "tf.convert_to_tensor",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.77",
        "r_prob": "1.0",
        "prob": "0.7161000000000001",
        "sentences": [
          "this seems like an inconsistency in the tensorflow api , since almost all other op functions accept numpy arrays wherever a tf.tensor is expected.",
          "i ' ve filed an issue to track the fix.",
          "fortunately , there is a simple workaround , using tf.convert_to_tensor (). "
        ]
      }
    ],
    "tf.variable": [
      {
        "title": "48796671",
        "h": "tf.variable",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.99",
        "r_prob": "0.87",
        "prob": "0.8354609999999999",
        "sentences": [
          "the problem is that the y_out argument to sess.run () is none , whereas it must be a tf.tensor ( or tensor - like object , such as a tf.variable ) or a tf.operation."
        ]
      },
      {
        "title": "37854244",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.8190000000000001",
        "sentences": [
          "variables are tensors which you can update ( with var.assign ()). ",
          "technically speaking , tf.variable is not a subclass of tf.tensor though."
        ]
      },
      {
        "title": "35931354",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.88",
        "r_prob": "0.99",
        "prob": "0.810216",
        "sentences": [
          "in tensorflow a tf.variable can be used anywhere a tf.tensor ( of the same element type and shape ) is expected."
        ]
      },
      {
        "title": "36240769",
        "h": "tf.variable",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.98",
        "r_prob": "0.97",
        "prob": "0.8080099999999999",
        "sentences": [
          "tl ; dr : you can ' t feed a tf.tensor object ( viz."
        ]
      },
      {
        "title": "37624060",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.93",
        "r_prob": "0.86",
        "prob": "0.687828",
        "sentences": [
          "most tensorflow tensors ( tf.tensor objects ) are immutable , so you cannot simply assign a value to them.",
          "however , if you created the tensor as a tf.variable , you can assign a value to it by calling variable.assign (). ",
          "the code you have unnecessarily converts a tf.variable object ( from the list of tf.trainable_variables ()) into a string name."
        ]
      },
      {
        "title": "37706972",
        "h": "tf.variable",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.79",
        "r_prob": "0.97",
        "prob": "0.674344",
        "sentences": [
          "* with the exception of tf.variable objects , using the variable.assign () etc.",
          "methods.",
          "however , rnn.rnn () likely returns a tf.tensor object that does not support this method."
        ]
      },
      {
        "title": "61735399",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.64",
        "r_prob": "0.99",
        "prob": "0.614592",
        "sentences": [
          "regarding how to implement your tftimeseries class , tf.variable and tf.costant might be interesting classes.",
          "here is their documentation : tf.variable , tf.costant "
        ]
      },
      {
        "title": "57957877",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.77",
        "r_prob": "0.81",
        "prob": "0.592515",
        "sentences": [
          "i believe the problem here is that you have previously manipulated your data sets in numpy formats , but now are using tensorflow foormats such as tf.variable or tf.tensor.",
          "this line ",
          "but if data is a tf.variable : "
        ]
      },
      {
        "title": "44486260",
        "h": "tf.variable",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.63",
        "t_prob": "0.91",
        "r_prob": "1.0",
        "prob": "0.5733",
        "sentences": [
          "here is a couple of workarounds : ",
          "tf.variable.",
          "tf.tensor cannot be changed , but tf.variable can."
        ]
      },
      {
        "title": "46375813",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.82",
        "r_prob": "0.71",
        "prob": "0.5647339999999998",
        "sentences": [
          "you can read more about variable initialization here , which says unlike tf.tensor objects , a tf.variable exists outside the context of a single session.run call.",
          "so before you can use a variable , it must be initialized."
        ]
      },
      {
        "title": "39513635",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.63",
        "r_prob": "0.98",
        "prob": "0.55566",
        "sentences": [
          "the four examples you gave will all give the same result , and generate the same graph ( if you ignore that some of the operation names in the graph are different ). ",
          "tensorflow will convert many different python objects into tf.tensor objects when they are passed as arguments to tensorflow operators , such as tf.add () here.",
          "the + operator is just a simple wrapper on tf.add (), and the overload is used when either the left - hand or right - hand argument is a tf.tensor ( or tf.variable ). ",
          "in that case , you may wish to convert the array to a tf.tensor once ",
          "creating a tf.constant () explicitly allows you to set its name property , which can be useful for tensorboard debugging and graph visualization."
        ]
      },
      {
        "title": "51381606",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.83",
        "r_prob": "0.6",
        "prob": "0.47807999999999995",
        "sentences": [
          "the error is because t1 is a tf.variable object , while t1 [ 1 ] is a tf.tensor.",
          "as it happens , tf.tensor can ' t be mutated ( it ' s read only ) whereas tf.variable can be ( read as well as write ) see here."
        ]
      },
      {
        "title": "44284879",
        "h": "tf.variable",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.84",
        "r_prob": "0.61",
        "prob": "0.445788",
        "sentences": [
          "you should see tensorflow as a graph , with : ",
          "w = tf.variable ([. 3 ], tf.float32 ) b = tf.variable ([-. 3 ], tf.float32 ) b = tf.variable ([-. 3 ], tf.float32 ) linear_model = w * x + b "
        ]
      },
      {
        "title": "63840420",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.61",
        "t_prob": "0.79",
        "r_prob": "0.85",
        "prob": "0.409615",
        "sentences": [
          "in the code you point to , initial_weights is only a collection of values ( tf.tensor objects ), and model_weights is a reference to the model ' s variables ( tf.variable objects ). "
        ]
      },
      {
        "title": "42792589",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.51",
        "r_prob": "0.88",
        "prob": "0.39045599999999997",
        "sentences": [
          "a tf.tensor in tensorflow is a read - only value — in fact , a symbolic expression for computing a read - only value — so you cannot in general assign values to it.",
          "( the main exceptions are tf.variable objects .) "
        ]
      },
      {
        "title": "37572852",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.76",
        "t_prob": "0.61",
        "r_prob": "0.77",
        "prob": "0.356972",
        "sentences": [
          "each operation assign or assignadd has two inputs and no output : ",
          "a tf.variable : the variable to which we assign a value.",
          "here you can see a bidirectional edge because the operation reads the old value from x and then writes back the new value.",
          "a tf.tensor : the value assigned to the variable ( or added to the variable ). ",
          "in the operation tf.add ( x , y ): the graph reads the value of x as an input."
        ]
      }
    ],
    "tf.operation": [
      {
        "title": "50802716",
        "h": "tf.tensor",
        "t": "tf.operation",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.82",
        "r_prob": "0.96",
        "prob": "0.739968",
        "sentences": [
          "this import statement imports the tensorflow.python.framework.ops module , which ( at present ) includes the implementations of classes like tf.graph , tf.tensor , and tf.operation."
        ]
      },
      {
        "title": "37955299",
        "h": "tf.tensor",
        "t": "tf.operation",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.76",
        "r_prob": "1.0",
        "prob": "0.6612",
        "sentences": [
          "tensorflow primarily expects tf.tensor objects as the keys in the feed dictionary.",
          "\" data_val \" does not work because it is the name of a tf.operation ( viz.",
          "x.op ) and not the name of a tf.tensor , which is the output of a tf.operation."
        ]
      },
      {
        "title": "42493778",
        "h": "tf.operation",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.75",
        "t_prob": "0.78",
        "r_prob": "0.99",
        "prob": "0.5791499999999999",
        "sentences": [
          "in your sample code , c is a tf.tensor while c_operation is a tf.operation.",
          "a tf.operation represents a computation that produces 0 or more tf.tensors.",
          "calling run on a tf.operation executes all operations in the graph required to produce inputs for this operation , but doesn ' t return anything ( documentation ). ",
          "calling eval on a tf.tensor executes the operation that produces it and returns its value ( documentation ). ",
          "in general , if you ' re interested in the value , you ' d want to call eval on the tf.tensor."
        ]
      },
      {
        "title": "34399966",
        "h": "tf.tensor",
        "t": "tf.operation",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.95",
        "r_prob": "0.66",
        "prob": "0.55176",
        "sentences": [
          "if you want to \" rename \" an op , there is no way to do that directly , because a tf.operation ( or tf.tensor ) is immutable once it has been created."
        ]
      },
      {
        "title": "44903756",
        "h": "tf.operation",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.76",
        "t_prob": "0.85",
        "r_prob": "0.83",
        "prob": "0.53618",
        "sentences": [
          "the tensorflow graph is an object which contains your various tf.tensor and tf.operation.",
          "work with the default graph ,. ",
          "create a fresh new graph ,. "
        ]
      }
    ]
  },
  "tf.contrib.learn.experiment": {},
  "tf.slice": {
    "tf.while_loop": [
      {
        "title": "55680074",
        "h": "tf.while_loop",
        "t": "tf.slice",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "0.89",
        "prob": "0.872289",
        "sentences": [
          "take a look at this example that does what you want using tf.while_loop () with tf.tensorarray and tf.slice () function : "
        ]
      },
      {
        "title": "51686804",
        "h": "tf.slice",
        "t": "tf.while_loop",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.84",
        "r_prob": "0.98",
        "prob": "0.765576",
        "sentences": [
          "you are not passing any structure / tensor to receive the values of your tf.slice (...). ",
          "your lambda b should have a signature such as lambda i , res : i + 1 , ... ",
          "tensors edited through a tf.while_loop should have a fixed shape.",
          "note : regarding your particular application ( collecting all pairs of neighbor columns ), this could be done without a tf.while_loop : "
        ]
      },
      {
        "title": "48333302",
        "h": "tf.while_loop",
        "t": "tf.slice",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.86",
        "r_prob": "0.99",
        "prob": "0.6981479999999999",
        "sentences": [
          "this \" answer \" is an implementation of muskrat ' s tf.slice suggestion with the details of tf.while_loop worked out ( with help from how to use tf.while_loop () in tensorflow and https :// www.tensorflow.org / api_docs / python / tf / while_loop ). ",
          "disadvantages : ",
          "tf.while_loop is treacherous.",
          "the missing documentation of tf.while_loop is that tensors outside the body of the loop are only evaluated once , even if inner ops depend on them."
        ]
      }
    ],
    "tf.gather": [
      {
        "title": "35172568",
        "h": "tf.slice",
        "t": "tf.gather",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.91",
        "r_prob": "0.97",
        "prob": "0.8297379999999999",
        "sentences": [
          "so even if you would use tf.gather / tf.slice , you still would have to get values of these operations via eval / run."
        ]
      },
      {
        "title": "36133338",
        "h": "tf.slice",
        "t": "tf.gather",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.99",
        "r_prob": "0.98",
        "prob": "0.82467",
        "sentences": [
          "you can take advantage of the fact that - 1 is a special argument to the tf.slice () size argument , meaning \" all remaining elements in that dimension \". ",
          "alternatively , you can use tf.gather () to select one or more slices from a tensor on the zeroth dimension."
        ]
      },
      {
        "title": "35158370",
        "h": "tf.slice",
        "t": "tf.gather",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.91",
        "r_prob": "0.99",
        "prob": "0.657657",
        "sentences": [
          "use the indexing operator ( based on tf.slice ()) to extract a contiguous slice from the tensor.",
          "use the tf.gather () op to select a non - contiguous slice from the tensor.",
          "note that tf.gather () only allows you to select whole slices in the 0th dimension ( whole rows in the example of a matrix ), so you may need to tf.reshape () or tf.transpose () your input to obtain the appropriate elements."
        ]
      }
    ]
  },
  "tf.py_func": {
    "tf.data.dataset": [
      {
        "title": "52417770",
        "h": "tf.py_func",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.56",
        "t_prob": "0.97",
        "r_prob": "0.84",
        "prob": "0.45628799999999997",
        "sentences": [
          "the tf.data api ( tensorflow 1.4 onwards ) is great for things like this.",
          "the pipeline will looks something like the following : ",
          "create an initial tf.data.dataset object that iterates over all examples.",
          "you can also generate the tfrecords using lower level operations.",
          "load images via tf.data.dataset.map and tf.py_func ( tion ). ",
          "alternatively you can load the image files from filenames inside tf.data.dataset.map as below.",
          "if you need more custom loading functions , also check out tf.py_func.",
          "more general information here , and notes on performance here "
        ]
      },
      {
        "title": "45206574",
        "h": "tf.py_func",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.45",
        "t_prob": "0.99",
        "r_prob": "0.99",
        "prob": "0.441045",
        "sentences": [
          "since tf 1.4 , the best way to do it is to use tf.data.dataset , and particularly tf.data.dataset.from_tensor_slices.",
          "to make sure that those operations happen concurrently , make sure to use tensorflow operations only and avoid wrapping python operations with tf.py_func."
        ]
      },
      {
        "title": "52063718",
        "h": "tf.py_func",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.42",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.3948",
        "sentences": [
          "just in case someone else has a similar problem , i will write a small version of a solution to this question.",
          "i defined a function called \" extract \" and mapped this extracted numpy array as a dataset using tf.py_func.",
          "the extracted dataframes and one - hot arrays are zipped with tf.data.dataset.zip into a final dataset."
        ]
      },
      {
        "title": "49837908",
        "h": "tf.data.dataset",
        "t": "tf.py_func",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.33",
        "r_prob": "1.0",
        "prob": "0.3267",
        "sentences": [
          "if you cannot , consider using tf.data.dataset.from_generator.",
          "my preferred method is to load some keys tensor entirely into memory - it might just be the indices of each example - then map that key value to data values using tf.py_func.",
          "to augment your dataset , use tf.data.dataset.map either before or after the batch operation , depending on whether or not you want to apply a batch - wise operation ( something working on a 4d image tensor ) or element - wise operation ( 3d image tensor ). ",
          "see the article for notes on efficiencies."
        ]
      },
      {
        "title": "47884927",
        "h": "tf.data.dataset",
        "t": "tf.py_func",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.29",
        "r_prob": "0.96",
        "prob": "0.26169599999999993",
        "sentences": [
          "i am working on a from_indexable for tf.data.dataset https :// github.com / tensorflow / tensorflow / issues / 14448 ",
          "the function from_indexable makes a tf.data.range , wraps the indexable in a generalized tf.py_func and calls map."
        ]
      }
    ]
  },
  "tf.contrib.integrate.odeint_fixed": {},
  "item": {},
  "tf.identity": {
    "tf.variable": [
      {
        "title": "41965889",
        "h": "tf.variable",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9215",
        "sentences": [
          "the tf.identity () operation is stateless.",
          "when you have a tf.variable called a , the value of tf.identity ( a ) will always be the same as the value of a.",
          "if you want b to remember a previous value of a , you should create b as a tf.variable as well : "
        ]
      },
      {
        "title": "34877802",
        "h": "tf.identity",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.74",
        "r_prob": "0.98",
        "prob": "0.6671840000000001",
        "sentences": [
          "tf.identity is useful when you want to explicitly transport tensor between devices ( like , from gpu to a cpu ). ",
          "a default behavior is that the send / recv nodes are added implicitly when the operation happens on a different device but you can imagine some situations ( especially in a multi - threaded / distributed settings ) when it might be useful to fetch the value of the variable multiple times within a single execution of the session.run.",
          "tf.identity allows for more control with regard to when the value should be read from the source device.",
          "also , please note that in the implementation of tf.variable link , the identity op is added in the constructor , which makes sure that all the accesses to the variable copy the data from the source only once.",
          "edit : updated answer after the question was edited.",
          "in addition , tf.identity can be used used as a dummy node to update a reference to the tensor."
        ]
      },
      {
        "title": "33717784",
        "h": "tf.variable",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.75",
        "t_prob": "0.96",
        "r_prob": "0.88",
        "prob": "0.6335999999999999",
        "sentences": [
          "this will create you a copy : v2 = tf.variable ( v1 ). ",
          "you can also use identity op : v2 = tf.identity ( v1 ) ( which i think is a proper way of doing it .. "
        ]
      }
    ],
    "tf.control_dependencies": [
      {
        "title": "37980704",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.8811",
        "sentences": [
          "there is no such function in the tensorflow api.",
          "instead you can use with tf.control_dependencies (): and tf.identity () to achieve the intended effect : "
        ]
      },
      {
        "title": "37863686",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.8118",
        "sentences": [
          "an alternative approach is to use with tf.control_dependencies ( ops ): blocks , where ops is a list of operations ( such as assignments ) that must run before the operations in the with block.",
          "the typical idiom to force a read is to use tf.identity ( var.ref ()), so the example would look something like : "
        ]
      },
      {
        "title": "49881987",
        "h": "tf.identity",
        "t": "tf.control_dependencies",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.75",
        "r_prob": "0.98",
        "prob": "0.7202999999999999",
        "sentences": [
          "when you do y = tf.identity ( x ) you are creating a new tensor , tensor_2 , which value will be the same as tensor_1.",
          "but it is a different node in your graph , so the value from tensor_1 to tensor_2 has to move.",
          "that is why the with tf.control_dependencies ([ x_plus_1 ]) does something in your second code but nothing in the first one.",
          "to sum up y = x makes the variable y to point to the same object in x , but y = tf.identity ( x ) creates a new object with the content of x."
        ]
      },
      {
        "title": "55480139",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.69",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.6692999999999999",
        "sentences": [
          "tf.control_dependencies only affects new operations created within the context.",
          "the simplest solution is to use a tf.identity operation that will produce the same result but will have the control dependencies : "
        ]
      },
      {
        "title": "44896587",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.65",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.624",
        "sentences": [
          "res3 = sess.run ( op3 ) ",
          "with dependencies ",
          "with tf.control_dependencies ([ op1 ]): ",
          "op2_after = tf.identity ( op1 ) ",
          "op3_after = tf.identity ( op1 ) "
        ]
      },
      {
        "title": "35704144",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.74",
        "t_prob": "0.83",
        "r_prob": "1.0",
        "prob": "0.6142",
        "sentences": [
          "the with tf.control_dependencies ([ op ]) block enforces control dependency on op to other ops created within with block.",
          "in your case , x * x is created outside , and the tf.identity just gets the old value."
        ]
      },
      {
        "title": "51938568",
        "h": "tf.identity",
        "t": "tf.control_dependencies",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.57",
        "r_prob": "1.0",
        "prob": "0.4673999999999999",
        "sentences": [
          "another option would be to use tf.control_dependencies which is a way to \" force \" tf to compute specific ops when it is computing other ones.",
          "we use tf.identity as a noop just to have something to wrap with control_dependencies."
        ]
      },
      {
        "title": "47252914",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.57",
        "t_prob": "0.88",
        "r_prob": "0.89",
        "prob": "0.44642399999999993",
        "sentences": [
          "now tensorflow needs to resolve the y variable.",
          "y variable is tf.identity ( t ). ",
          "tf.identity ( t ) must be executed after reset = tf.assign ( w , 0 ).. ",
          "tf.identity ( t ) references t.after executing reset , we have to resolve t , evaluate it and then exexute y .. ",
          "thus : t = tf.identity ( w ) -> only after the execution of update "
        ]
      },
      {
        "title": "44246730",
        "h": "tf.control_dependencies",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.62",
        "t_prob": "0.52",
        "r_prob": "1.0",
        "prob": "0.3224",
        "sentences": [
          "the tf.print ops can be run in any order , the strict ordering is only on the tf.identity ops.",
          "instead one has to use with tf.control_dependencies instead ( as already suggested by the op ): "
        ]
      }
    ],
    "tf.tensor": [
      {
        "title": "38740948",
        "h": "tf.tensor",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.853776",
        "sentences": [
          "the typical way to rename an op is therefore to use tf.identity (). "
        ]
      },
      {
        "title": "34399966",
        "h": "tf.tensor",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.845064",
        "sentences": [
          "the typical way to rename an op is therefore to use tf.identity (), which has almost no runtime cost : "
        ]
      },
      {
        "title": "37949814",
        "h": "tf.identity",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.9",
        "r_prob": "0.82",
        "prob": "0.72324",
        "sentences": [
          "however , you can use additional \" no - op \" operations ( like you said ): ",
          "for a tf.tensor : tf.identity ( input_tensor , name =' your_new_name ') "
        ]
      }
    ]
  },
  "tf.keras.losses.categoricalcrossentropy": {},
  "tf.keras.losses.sparsecategoricalcrossentropy": {},
  "tf.keras.layers.activation": {},
  "embedding layer": {},
  "tf.nn.l2_loss": {},
  "tf.reduce_mean": {
    "tf.nn.softmax_cross_entropy_with_logits": [
      {
        "title": "43930885",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.96",
        "r_prob": "0.99",
        "prob": "0.912384",
        "sentences": [
          "if you use tf.reduce_sum () in the upper example , as you did in the lower one , you should be able to achieve similar results with both methods : cost = tf.reduce_mean ( tf.reduce_sum ( tf.nn.softmax_cross_entropy_with_logits ( logits = logits , labels = y ))). "
        ]
      },
      {
        "title": "45287445",
        "h": "tf.reduce_mean",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.99",
        "r_prob": "0.85",
        "prob": "0.79101",
        "sentences": [
          "weight initialization is too big ( try to initialize the weights with a smaller scale , like stddev = 1e - 2 ). ",
          "add regularization to your loss function for all the weights ej : ",
          "cross_entropy = tf.reduce_mean ( tf.nn.softmax_cross_entropy_with_logits ( labels = y_true , logits = y_conv )+ beta * tf.nn.l2_loss ( w_conv1 ) + beta * tf.nn.l2_loss ( w_conv2 ) + beta * tf.nn.l2_loss ( w_fc1 )+ beta * tf.nn.l2_loss ( w_fc2 )) "
        ]
      },
      {
        "title": "56795292",
        "h": "tf.reduce_mean",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.98",
        "r_prob": "0.82",
        "prob": "0.6669879999999999",
        "sentences": [
          "you can see internally that tf.nn.softmax_cross_entropy_with_logits is called , which computes the softmax probabilities and the cross - entropy function at the same time.",
          "there is a function to get this encoding in tensoflow called tf.one_hot.",
          "you would have to call tf.reduce_mean on the result."
        ]
      }
    ],
    "tf.square": [
      {
        "title": "48836872",
        "h": "tf.square",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.93",
        "prob": "0.884058",
        "sentences": [
          "the - operator is called on var inside variable_summaries : ",
          "stddev = tf.sqrt ( tf.reduce_mean ( tf.square ( var - mean ))) "
        ]
      },
      {
        "title": "57379271",
        "h": "tf.reduce_mean",
        "t": "tf.square",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.874",
        "sentences": [
          "hence the steps error = output - y , error_sq = tf.square ( error ) and loss = tf.reduce_mean ( error_sq , axis =- 1 ) might be resulting in nan."
        ]
      },
      {
        "title": "41001210",
        "h": "tf.reduce_mean",
        "t": "tf.square",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.96",
        "r_prob": "0.9",
        "prob": "0.79488",
        "sentences": [
          "following modified code works.",
          "the main problem was loss function , which should be loss = 0.5 * tf.reduce_mean ( tf.square ( tf.transpose ( logits ) - tftrainy )) "
        ]
      },
      {
        "title": "63342681",
        "h": "tf.square",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.81",
        "r_prob": "0.96",
        "prob": "0.668736",
        "sentences": [
          "further , i found tf.reduce_mean , k.mean , tf.square , tf.exp etc.",
          "implemented in a loss funtion cause the same error."
        ]
      },
      {
        "title": "42474169",
        "h": "tf.square",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.94",
        "r_prob": "0.68",
        "prob": "0.626416",
        "sentences": [
          "so you are subtracting a row from a column at this line : loss = tf.reduce_mean ( tf.square ( tf.matmul ( _a , _x ) - _b )). ",
          "by default , reduction operations in numpy and tensorflow reduce along all dimensions , so you keep getting a single number regardless dimensions of the input array."
        ]
      }
    ],
    "tf.reduce_sum": [
      {
        "title": "43825740",
        "h": "tf.reduce_sum",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.93",
        "r_prob": "0.96",
        "prob": "0.80352",
        "sentences": [
          "also tf.reduce_sum ( cost ) will do what you want , i think it is better to use tf.reduce_mean (). ",
          "here are a few reasons why : ",
          "on average you will get reduce_sum 4 times bigger for a two times bigger matrix."
        ]
      },
      {
        "title": "52774155",
        "h": "tf.reduce_sum",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.91",
        "r_prob": "0.89",
        "prob": "0.793702",
        "sentences": [
          "so in your example , you would do [ 0 , 1 , 1 , 0 , 0 , 1 ] for one class and [ 1 , 0 , 0 , 1 , 1 , 0 ] for the other.",
          "use tf.reduce_mean () on the correct axis to get the average of each class."
        ]
      },
      {
        "title": "42611524",
        "h": "tf.reduce_sum",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.91",
        "r_prob": "0.89",
        "prob": "0.7613059999999999",
        "sentences": [
          "you can use tf.sqrt followed by tf.reduce_sum and tf.reduce_mean.",
          "both tf.reduce_sum and tf.reduce_mean have an axis argument that indicates which dimensions to reduce."
        ]
      },
      {
        "title": "46831346",
        "h": "tf.reduce_sum",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "0.7",
        "prob": "0.66542",
        "sentences": [
          "it is because you sum before taking the mean , so you get the squared error and not its mean.",
          "change tf.reduce_mean ( tf.reduce_sum ( tf.square ( tf.subtract ( y , y_ )))) to tf.reduce_mean (( tf.square ( tf.subtract ( y , y_ ))) "
        ]
      },
      {
        "title": "45160944",
        "h": "tf.reduce_mean",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.93",
        "r_prob": "0.97",
        "prob": "0.658533",
        "sentences": [
          "for example if you choose to use tf.reduce_mean , then your loss is averaged on all the elements of your batch.",
          "and so is the gradient.",
          "if you use tf.reduce_sum , then your gradient will be the sum of all your gradients element - wise."
        ]
      },
      {
        "title": "43930885",
        "h": "tf.reduce_sum",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.96",
        "r_prob": "0.64",
        "prob": "0.602112",
        "sentences": [
          "if you use tf.reduce_sum () in the upper example , as you did in the lower one , you should be able to achieve similar results with both methods : cost = tf.reduce_mean ( tf.reduce_sum ( tf.nn.softmax_cross_entropy_with_logits ( logits = logits , labels = y ))). "
        ]
      },
      {
        "title": "43932289",
        "h": "tf.reduce_sum",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.96",
        "r_prob": "0.6",
        "prob": "0.55296",
        "sentences": [
          "from tensorflow api here the second way , cost = tf.reduce_mean ( tf.reduce_sum (- y * tf.log ( hypothesis ))) is numerically unstable , and because of that you can ' t get same results , "
        ]
      }
    ],
    "loss function": [
      {
        "title": "45287445",
        "h": "tf.reduce_mean",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.94",
        "t_prob": "0.83",
        "r_prob": "0.87",
        "prob": "0.6787739999999999",
        "sentences": [
          "add regularization to your loss function for all the weights ej : ",
          "cross_entropy = tf.reduce_mean ( tf.nn.softmax_cross_entropy_with_logits ( labels = y_true , logits = y_conv )+ beta * tf.nn.l2_loss ( w_conv1 ) + beta * tf.nn.l2_loss ( w_conv2 ) + beta * tf.nn.l2_loss ( w_fc1 )+ beta * tf.nn.l2_loss ( w_fc2 )) "
        ]
      },
      {
        "title": "45160944",
        "h": "tf.reduce_mean",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.73",
        "t_prob": "0.9",
        "r_prob": "0.96",
        "prob": "0.6307200000000001",
        "sentences": [
          "the answer to this question depends on your loss function.",
          "if loss_element is your loss function for one element of the batch , then , the loss of your batch will be some function of all your individual losses."
        ]
      },
      {
        "title": "41001210",
        "h": "tf.reduce_mean",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.92",
        "t_prob": "0.81",
        "r_prob": "0.72",
        "prob": "0.536544",
        "sentences": [
          "following modified code works.",
          "the main problem was loss function , which should be loss = 0.5 * tf.reduce_mean ( tf.square ( tf.transpose ( logits ) - tftrainy )) "
        ]
      }
    ]
  },
  "gpus": {
    "batch size": [
      {
        "title": "57694601",
        "h": "batch size",
        "t": "gpus",
        "r": "S2",
        "h_prob": "0.92",
        "t_prob": "0.73",
        "r_prob": "1.0",
        "prob": "0.6716",
        "sentences": [
          "as far as i can ( heuristically ) tell , when doing distributed training / evaluation , the number of elements in the dataset must be evenly divisible by the batch size and number of gpus."
        ]
      },
      {
        "title": "48615939",
        "h": "batch size",
        "t": "gpus",
        "r": "S2",
        "h_prob": "0.88",
        "t_prob": "0.69",
        "r_prob": "1.0",
        "prob": "0.6072",
        "sentences": [
          "the problem is when you model received 0 batch size."
        ]
      },
      {
        "title": "41763164",
        "h": "batch size",
        "t": "gpus",
        "r": "S2",
        "h_prob": "0.9",
        "t_prob": "0.51",
        "r_prob": "1.0",
        "prob": "0.459",
        "sentences": [
          "generally deep learning algorithms are ran on gpus which has limited memory and thus a limited number of input data samples ( in the algorithm commonly defined as batch size ) could be loaded at a time.",
          "another probable benefit of large batch size is : in multi - class classification problems , if the number of classes are large , a larger batch size makes algorithm generalize better ( technically avoids over - fitting ) over the different classes ( while doing this a standard technique is to keep uniform distribution of classes in a batch ). "
        ]
      },
      {
        "title": "51343044",
        "h": "batch size",
        "t": "gpus",
        "r": "S2",
        "h_prob": "0.82",
        "t_prob": "0.59",
        "r_prob": "0.73",
        "prob": "0.35317399999999993",
        "sentences": [
          "now , as your dataset got smaller , your relative batch size can be larger.",
          "running models explicitly on multiple gpus will require you to set your algorithm in that fashion."
        ]
      },
      {
        "title": "34278070",
        "h": "gpus",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.62",
        "t_prob": "0.78",
        "r_prob": "0.66",
        "prob": "0.319176",
        "sentences": [
          "increase the batch size by a factor of n , where n is the number of gpus."
        ]
      },
      {
        "title": "63637806",
        "h": "batch size",
        "t": "gpus",
        "r": "S2",
        "h_prob": "0.73",
        "t_prob": "0.29",
        "r_prob": "1.0",
        "prob": "0.21169999999999997",
        "sentences": [
          "if you still want to train f - rcnn with batch size > 1 , you can do so with multiple gpus."
        ]
      }
    ]
  },
  "tf.layers.batch_normalization": {
    "tf.nn.batch_normalization": [
      {
        "title": "48006315",
        "h": "tf.nn.batch_normalization",
        "t": "tf.layers.batch_normalization",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.91",
        "r_prob": "0.99",
        "prob": "0.8828820000000001",
        "sentences": [
          "just to add to the list , there ' re several more ways to do batch - norm in tensorflow : ",
          "tf.nn.batch_normalization is a low - level op.",
          "the difference is that it ' s optimized for 4d input tensors , which is the usual case in convolutional neural networks.",
          "tf.nn.batch_normalization accepts tensors of any rank greater than 1 .. ",
          "tf.layers.batch_normalization is a high - level wrapper over the previous ops.",
          "tf.contrib.layers.batch_norm is the early implementation of batch norm , before it ' s graduated to the core api ( i.e ., tf.layers ). ",
          "finally , there ' s also keras layer keras.layers.batchnormalization , which in case of tensorflow backend invokes tf.nn.batch_normalization .. "
        ]
      },
      {
        "title": "49531019",
        "h": "tf.nn.batch_normalization",
        "t": "tf.layers.batch_normalization",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.88",
        "r_prob": "1.0",
        "prob": "0.8712",
        "sentences": [
          "there is a big difference between tf.nn.batch_normalization and tf.layers.batch_normalization.",
          "you might not even use batches as input , so there would be no minibatch statistics ). ",
          "by default , tensorflow only executes what it needs to.",
          "the tf.control_dependencies context manager forces tensorflow to do the updates every time it computes whatever is in the code block ( in this case the cost ). "
        ]
      },
      {
        "title": "47984274",
        "h": "tf.nn.batch_normalization",
        "t": "tf.layers.batch_normalization",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "0.82",
        "prob": "0.7955639999999999",
        "sentences": [
          "simply use tf.layers.batch_normalization.",
          "it also creates variables via tf.get_variable (), hence they can be shared as well.",
          "in addition , it works seamlessly with tf.layers.conv * functions.",
          "update : tf.nn.batch_normalization is fine too.",
          "it ' s a more low - level function that requires you manage mean and variance tensors yourself.",
          "in fact , tf.layers.batch_normalization is a wrapper over tf.nn."
        ]
      }
    ]
  },
  "tf.nn.leaky_relu": {},
  "tf.image.per_image_whitening": {},
  "tf.image.per_image_standardization": {},
  "convolutional neural network": {},
  "tf.random_crop": {},
  "tf.placeholder_with_default": {},
  "tf.contrib.data.get_single_element": {},
  "tf.keras.applications": {},
  "tf.keras.applications.resnet50": {},
  "tf.keras suppor": {},
  "tf.add_check_numerics_ops": {},
  "tf.compat.v1.estimator.inputs.pandas_input_fn": {},
  "tf.estimator.inputs.pandas_input_fn": {},
  "tf.contrib.layers.l2_regularizer": {},
  "tf.contrib.layers.apply_regularization": {},
  "tf.serialize_sparse": {},
  "tf.matrix_transpose": {},
  "tf.io.decode_raw": {},
  "tf.python_io.tf_record_iterator": {},
  "tf.config.experimental_run_functions_eagerly": {},
  "tf.compat.v1.disable_eager_execution": {},
  "tf.image.rgb_to_yuv": {},
  "tf.report_uninitialized_variables": {},
  "tf.math.top_k": {},
  "tf.math.pow": {},
  "tf.math.cumsum": {},
  "tf.sparsetensor": {
    "tf.sparse_tensor_to_dense": [
      {
        "title": "34686952",
        "h": "tf.sparse_tensor_to_dense",
        "t": "tf.sparsetensor",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.86",
        "r_prob": "1.0",
        "prob": "0.8428",
        "sentences": [
          "update : tensorflow 1.0 includes a tf.scatter_nd () operator , which can be used to create delta below without creating a tf.sparsetensor.",
          "one way you could do this is to define a tf.sparsetensor , delta , representing the change : ",
          "then you can use the tf.sparse_tensor_to_dense () op to make a dense tensor from delta and add it to c : "
        ]
      },
      {
        "title": "50235076",
        "h": "tf.sparse_tensor_to_dense",
        "t": "tf.sparsetensor",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.6930000000000001",
        "sentences": [
          "use tf.sparsetensor and , if required , tf.sparse_tensor_to_dense."
        ]
      },
      {
        "title": "49243181",
        "h": "tf.sparse_tensor_to_dense",
        "t": "tf.sparsetensor",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.87",
        "r_prob": "1.0",
        "prob": "0.6699",
        "sentences": [
          "you can make use of a combination of tf.sparsetensor and tf.sparse_tensor_to_dense to achieve what you want : "
        ]
      }
    ]
  },
  "tf.sparse.to_dense": {},
  "tf.contrib.metrics.streaming_accuracy": {},
  "tf.metrics.mean": {},
  "tf.contrib.image.transform": {},
  "tf.contrib.image.matrices_to_flat_transforms": {},
  "tf.contrib.lookup.string_to_index_table_from_tensor": {},
  "tf.keras.layers.dot": {},
  "tf.keras.layers.multiply": {},
  "tf.contrib.data.parallel_interleave": {},
  "tf.random.set_random_seed": {},
  "tf.random.uniform": {},
  "tf.keras.backend.arange": {},
  "tf.nn.moments": {},
  "tf.nn.fused_batch_norm": {},
  "batch recor": {},
  "tf.gather_nd": {},
  "tf.tensordot": {},
  "tf.contrib.layers.embed_sequence": {},
  "tf.global_variables": {
    "tf.variables_initializer": [
      {
        "title": "47916956",
        "h": "tf.global_variables",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "global_variables and local_variables contain all variables of the graph , which need to be initialized before training.",
          "tf.global_variables () returns the global variables in a list and can be used with tf.variables_initializer for initialization .. ",
          "summaries contains keys for all summaries added by tf.summary ( scalar , image , histogram , text , etc ). "
        ]
      },
      {
        "title": "47271906",
        "h": "tf.global_variables",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "the tf.global_variables_initializer just initializes all variables that tf.global_variables () would list.",
          "this actually makes much sense in a distributed environment where the graph might be located in different computing nodes in a cluster.",
          "in such a case , tf.global_variables_initializer () which is just an alias for tf.variables_initializer ( tf.global_variables ()) would initialize all the variables in all the computing nodes , where the graph is placed."
        ]
      },
      {
        "title": "48505953",
        "h": "tf.global_variables",
        "t": "tf.variables_initializer",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9108",
        "sentences": [
          "looking at the documentation the init = tf.global_variables_initializer () is the same as init = tf.variables_initializer ( tf.global_variables ()) ",
          "instead , the vanilla gradient descent optimizer tf.train.gradientdescentoptimizer does not depend on any variables."
        ]
      }
    ],
    "tf.global_variables_initializer": [
      {
        "title": "47271906",
        "h": "tf.global_variables_initializer",
        "t": "tf.global_variables",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "the tf.global_variables_initializer just initializes all variables that tf.global_variables () would list.",
          "this actually makes much sense in a distributed environment where the graph might be located in different computing nodes in a cluster.",
          "in such a case , tf.global_variables_initializer () which is just an alias for tf.variables_initializer ( tf.global_variables ()) would initialize all the variables in all the computing nodes , where the graph is placed."
        ]
      },
      {
        "title": "49785891",
        "h": "tf.global_variables_initializer",
        "t": "tf.global_variables",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9603999999999999",
        "sentences": [
          "as documented : ",
          "tf.global_variables_initializer () is just a shortcut for variables_initializer ( global_variables ()) ",
          "therefore , call tf.global_variables () will give you a list of initialized variables."
        ]
      },
      {
        "title": "48505953",
        "h": "tf.global_variables_initializer",
        "t": "tf.global_variables",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.92",
        "r_prob": "0.98",
        "prob": "0.892584",
        "sentences": [
          "looking at the documentation the init = tf.global_variables_initializer () is the same as init = tf.variables_initializer ( tf.global_variables ()) ",
          "instead , the vanilla gradient descent optimizer tf.train.gradientdescentoptimizer does not depend on any variables.",
          "if you would place the init = tf.global_variables_initializer () before the tf.train.adamoptimizer like ",
          "so tf.global_variables_initializer () does not see the needed variables of adam , when placing init = tf.global_variables_initializer () before the adam definition tf.train.adamoptimizer."
        ]
      },
      {
        "title": "45976536",
        "h": "tf.global_variables",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.91",
        "r_prob": "0.93",
        "prob": "0.6770400000000001",
        "sentences": [
          "if var_list is empty , however , the function still returns an op that can be run.",
          "that op just has no effect.",
          "the load script has no global varibale , and since tf.global_variables_initializer () is equivalent to tf.variables_initializer ( tf.global_variables ()), the operation is a no - op."
        ]
      }
    ]
  },
  "able_loop_fn": {},
  "tf.nn.raw_rnn": {},
  "tf.keras.layers.conv2d": {},
  "tf.layers.max_pooling2d": {},
  "batch sizes": {
    "batch size": [
      {
        "title": "50241169",
        "h": "batch sizes",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.87",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.783",
        "sentences": [
          "increase batch size ",
          "you should be using larger batch sizes , i would start with 32 and move around from there."
        ]
      },
      {
        "title": "55045782",
        "h": "batch sizes",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.9",
        "t_prob": "0.82",
        "r_prob": "0.96",
        "prob": "0.70848",
        "sentences": [
          "batch size is set to none.",
          "batch size is set to 64.",
          "both alternatives have different batch sizes of 32 and 64."
        ]
      },
      {
        "title": "62962432",
        "h": "batch size",
        "t": "batch sizes",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.77",
        "r_prob": "1.0",
        "prob": "0.7007",
        "sentences": [
          "consider using different batch size for generator and discriminator.",
          "and try to use batch sizes that are the power of 2."
        ]
      },
      {
        "title": "62694015",
        "h": "batch size",
        "t": "batch sizes",
        "r": "S2",
        "h_prob": "0.9",
        "t_prob": "0.77",
        "r_prob": "1.0",
        "prob": "0.6930000000000001",
        "sentences": [
          "however , in that case , the batch sizes of the corresponding data generators have to be determined proportion to the extent of data in each relevant directory."
        ]
      },
      {
        "title": "49705593",
        "h": "batch size",
        "t": "batch sizes",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.6715",
        "sentences": [
          "that is due to the batch size.",
          "when you train a model , you can pass through different batch sizes ( e.g.",
          "32 , 64 , ...). ",
          "when you build the model the input batch size is still not defined."
        ]
      },
      {
        "title": "62671456",
        "h": "batch size",
        "t": "batch sizes",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.76",
        "r_prob": "0.97",
        "prob": "0.670852",
        "sentences": [
          "the extra dimension at the beginning of images are generally for batch sizes.",
          "batch size represent how many images you bundle to feed the cnn at once."
        ]
      },
      {
        "title": "48034744",
        "h": "batch size",
        "t": "batch sizes",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.6642",
        "sentences": [
          "you can also try different batch sizes and see if validation performance increases.",
          "large batch size can break validation performance when batch normalization is used."
        ]
      },
      {
        "title": "47141873",
        "h": "batch size",
        "t": "batch sizes",
        "r": "S2",
        "h_prob": "0.8",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.6400000000000001",
        "sentences": [
          "you are likely using the keep_aspect_ratio_resizer which allows each image to be a different size , which means that you can only train with batch size 1.",
          "to train with larger batch sizes , the only way to currently handle this in the api is to use the fixed_shape_resizer."
        ]
      },
      {
        "title": "43770732",
        "h": "batch size",
        "t": "batch sizes",
        "r": "S2",
        "h_prob": "0.9",
        "t_prob": "0.8",
        "r_prob": "0.84",
        "prob": "0.6048",
        "sentences": [
          "i ' ve experimented with batch sizes in a project using a convolutional neural network and found something interesting : batch size is a regularizer.",
          "it also favored a lower batch size.",
          "and this time it favored a large batch size.",
          "you ' ll get far more mileage by training faster and testing more hyperparameter combinations than focusing on batch size and sacrificing your ability to run a lot of experimentation."
        ]
      },
      {
        "title": "44366880",
        "h": "batch size",
        "t": "batch sizes",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.67",
        "r_prob": "0.99",
        "prob": "0.603603",
        "sentences": [
          "there are several issues that could cause disappointing performance scaling with the batch size.",
          "note that even if increasing the batch size makes the gpu faster , it might make the whole thing slower , because the gpu now has to wait for the i / o of the whole batch to finish to even start working.",
          "ram , gpu ram , or pcie bandwidth - limited models : if your model ' s bottleneck is in any of these , it probably won ' t benefit from bigger batch sizes.",
          "the way to check this is to run your model directly ( perhaps with mock input ), compare the timing to the aforementioned time difference and plot it as a function of the batch size."
        ]
      },
      {
        "title": "63637806",
        "h": "batch size",
        "t": "batch sizes",
        "r": "S2",
        "h_prob": "0.73",
        "t_prob": "0.74",
        "r_prob": "1.0",
        "prob": "0.5402",
        "sentences": [
          "ssd is single stage and hence can be parallelized to give larger batch sizes.",
          "if you still want to train f - rcnn with batch size > 1 , you can do so with multiple gpus."
        ]
      },
      {
        "title": "61676506",
        "h": "batch size",
        "t": "batch sizes",
        "r": "S2",
        "h_prob": "0.74",
        "t_prob": "0.73",
        "r_prob": "1.0",
        "prob": "0.5402",
        "sentences": [
          "a batch size of 1 means your model weights are being adjusted based on 1 observation rather than optimizing for a handful of observations.",
          "common batch sizes are between 16 and 32 but can be adjusted depending on the model."
        ]
      },
      {
        "title": "58971124",
        "h": "batch size",
        "t": "batch sizes",
        "r": "S2",
        "h_prob": "0.67",
        "t_prob": "0.67",
        "r_prob": "1.0",
        "prob": "0.4489000000000001",
        "sentences": [
          "in the code below , the try / except is used to try different gpu batch sizes without halting training."
        ]
      }
    ]
  },
  "tf.scatter_n": {},
  "tf.random_uniform": {},
  "value": {},
  "optimizers": {},
  "tf.contrib.keras": {},
  "tf.contrib.util.make_tensor_proto": {},
  "tf.constant": {
    "tf.placeholder": [
      {
        "title": "43536220",
        "h": "tf.constant",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "tf.constant () and tf.placeholder () are nodes in the graph ( ops or operations ). "
        ]
      },
      {
        "title": "42906762",
        "h": "tf.constant",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.99",
        "r_prob": "0.99",
        "prob": "0.940896",
        "sentences": [
          "in particular , each embedding array is converted to a tf.constant () tensor , which will be quite large ( approximately 328mb by my estimate ). ",
          "the best way to avoid this is to load the variables from the previous model directly into your new model using a tf.train.saver.",
          "an alternative way to solve your problem would be to pre - create a tf.placeholder () op for assigning a value to each variable."
        ]
      },
      {
        "title": "35688187",
        "h": "tf.constant",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.94",
        "r_prob": "0.99",
        "prob": "0.874764",
        "sentences": [
          "simply create w as a tf.constant () that takes embedding as its value : ",
          "create w as a tf.variable and initialize it from the numpy array via a tf.placeholder (): "
        ]
      },
      {
        "title": "58430178",
        "h": "tf.placeholder",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.78",
        "r_prob": "0.95",
        "prob": "0.71136",
        "sentences": [
          "what you are missing is sentences_list should be passed through tf.constant or tf.placeholder depends on how you want to use it.",
          "for tf.constant use : x = tf.constant ( sentences_list ) "
        ]
      },
      {
        "title": "42633142",
        "h": "tf.constant",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.7",
        "r_prob": "1.0",
        "prob": "0.693",
        "sentences": [
          "tensorflow will apply constant propagation so that ( computed ) values that are the same in every execution of a subgraph will only be computed once.",
          "in your example , the entire expression is a constant , so tensorflow will replace it with a single tf.constant () value corresponding to the result ( 1.6 ). ",
          "if in your example x were a tf.placeholder (), the remainder of the computation could be compiled into a single kernel with one input and one output."
        ]
      },
      {
        "title": "54372304",
        "h": "tf.placeholder",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.49",
        "r_prob": "0.97",
        "prob": "0.423017",
        "sentences": [
          "like you said , this won ' t work for a varlenfeature.",
          "instead of using tf.constant try using tf.placeholder for a a fixedlenfeature and tf.sparse_placeholder for a varlenfeature."
        ]
      },
      {
        "title": "57959532",
        "h": "tf.constant",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.56",
        "t_prob": "0.97",
        "r_prob": "0.58",
        "prob": "0.315056",
        "sentences": [
          "you will not be able to do this for a ' tf.constant ()', as it is a constant variable and does not support having its values changed.",
          "if you want to change values within tensorflow data structures it is best to either pass values to a tf.placeholder or use a tf.variable."
        ]
      },
      {
        "title": "47257999",
        "h": "tf.placeholder",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.62",
        "r_prob": "0.53",
        "prob": "0.30559800000000004",
        "sentences": [
          "you defined keep_prob as a tf.constant , but then trying to feed the value into it.",
          "replace keep_prob = tf.constant ( 1.0 ) with keep_prob = tf.placeholder ( tf.float32 ,[]) or keep_prob = tf.placeholder_with_default ( 1.0 ,[]) "
        ]
      }
    ],
    "tf.variable": [
      {
        "title": "43536220",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.97",
        "prob": "0.9030699999999999",
        "sentences": [
          "tf.constant () and tf.placeholder () are nodes in the graph ( ops or operations ). ",
          "on the other hand tf.variable () is a class."
        ]
      },
      {
        "title": "37854244",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.9",
        "r_prob": "0.99",
        "prob": "0.7306199999999999",
        "sentences": [
          "variables are tensors which you can update ( with var.assign ()). ",
          "technically speaking , tf.variable is not a subclass of tf.tensor though.",
          "tf.constant is just the most basic tensor , which contains a fixed value given when you create it."
        ]
      },
      {
        "title": "56480077",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.92",
        "r_prob": "0.84",
        "prob": "0.726432",
        "sentences": [
          "if you declare something with tf.constant () you won ' t be able to change the value in future.",
          "but , tf.variable () let ' s you change the variable in future."
        ]
      },
      {
        "title": "63435824",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.94",
        "r_prob": "0.99",
        "prob": "0.725868",
        "sentences": [
          "what was wrong : you cannot convert a tf.variable to a numpy array using graph execution and need to use a tf.constant instead."
        ]
      },
      {
        "title": "54977745",
        "h": "tf.variable",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.6480000000000001",
        "sentences": [
          "tf.operation represents a graph node and performs computation on tensors.",
          "tf.constant returns a special kind of tf.operation which takes 0 tensors as input and produces 0 tensors as output since it performs no computation.",
          "while tf.variable is in fact a nested operation ( or subgraph ) consists of 3 nodes.",
          "to my understanding , when users call this method , say , a = tf.constant ([ 1 , 2 , 3 ], name =' input_a '), two things will happen in different aspects : ",
          "maybe this node is just like a pointer pointing to the corresponding device memory with its value , waiting for other callable nodes to find it .. "
        ]
      },
      {
        "title": "35688187",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.66",
        "r_prob": "1.0",
        "prob": "0.6204",
        "sentences": [
          "simply create w as a tf.constant () that takes embedding as its value : ",
          "since embedding can be very large , you should only use this approach for toy examples.",
          "create w as a tf.variable and initialize it from the numpy array via a tf.placeholder (): ",
          "this avoid storing a copy of embedding in the graph , but it does require enough memory to keep two copies of the matrix in memory at once ( one for the numpy array , and one for the tf.variable ). "
        ]
      },
      {
        "title": "45267542",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.63",
        "r_prob": "0.98",
        "prob": "0.543312",
        "sentences": [
          "if you don ' t want train this weights , you can define them as tf.constant not tf.variable "
        ]
      },
      {
        "title": "44220922",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.55",
        "t_prob": "0.95",
        "r_prob": "0.87",
        "prob": "0.45457499999999995",
        "sentences": [
          "however , you declared a as a tf.constant in the first line -- and you can ' t assign a new value to a constant.",
          "if you replace your first line with a = tf.variable ( 1 ) then you have a program that will execute successfully."
        ]
      },
      {
        "title": "56480281",
        "h": "tf.variable",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.43",
        "r_prob": "0.71",
        "prob": "0.28698199999999996",
        "sentences": [
          "if you declare a tf.variable , you can change it ' s value later on if you want to.",
          "if the first layers are defined as tf.constant , you can ' t do that."
        ]
      }
    ],
    "tf.matmul": [
      {
        "title": "52050884",
        "h": "tf.constant",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.9",
        "r_prob": "0.99",
        "prob": "0.86427",
        "sentences": [
          "according to documentation : ",
          "calling tf.constant () creates a single operation that produces a value , adds it to the default graph .. ",
          "calling tf.matmul ( x , y ) creates a single operation that multiplies the values of tf.tensor objects x and y , adds it to the default graph , and returns a tf.tensor that represents the result of the multiplication."
        ]
      },
      {
        "title": "43082103",
        "h": "tf.constant",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.57",
        "t_prob": "0.72",
        "r_prob": "1.0",
        "prob": "0.41039999999999993",
        "sentences": [
          "for example , input1 is converted to a constant tensor within tf.matmul for your first case , and directly within tf.constant for your second case."
        ]
      },
      {
        "title": "38448221",
        "h": "tf.constant",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.6",
        "t_prob": "0.8",
        "r_prob": "0.73",
        "prob": "0.3504",
        "sentences": [
          "your target is y_ = tf.constant ([ 1.0 , 2.0 , 3.0 ], dtype = tf.float64 ) has the shape ( 1 , 3 ). ",
          "the output of tf.matmul ( x , w ) has the shape ( 3 , 1 ). "
        ]
      }
    ],
    "tf.tensor": [
      {
        "title": "35662013",
        "h": "tf.constant",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.8454999999999999",
        "sentences": [
          "the tf.constant () op takes a numpy array ( or something implicitly convertible to a numpy array ), and returns a tf.tensor whose value is the same as that array.",
          "it does not accept a tf.tensor as its argument.",
          "on the other hand , the tf.random_normal () op returns a tf.tensor whose value is generated randomly according to the given distribution each time it runs.",
          "since it returns a tf.tensor , it cannot be used as the argument to tf.constant (). "
        ]
      },
      {
        "title": "43007712",
        "h": "tf.constant",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.79",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.6241000000000001",
        "sentences": [
          "the shape argument of the tf.constant () op expects a static shape , so you can ' t use a tf.tensor as part of the argument.",
          "fortunately there is another op that will suffice : tf.fill (), which allows the shape ( its dims argument ) to be a tf.tensor."
        ]
      },
      {
        "title": "63584248",
        "h": "tf.tensor",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.94",
        "r_prob": "0.67",
        "prob": "0.5038400000000001",
        "sentences": [
          "because the embedding layer is expecting a tf.tensor and calling the model with model ( x ) does not perform that conversion.",
          "you can do it manually with tf.constant ([ array ]) and then it will work."
        ]
      },
      {
        "title": "53659222",
        "h": "tf.tensor",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.53",
        "t_prob": "0.94",
        "r_prob": "0.96",
        "prob": "0.478272",
        "sentences": [
          "so each time the tensors are evaluated , they take the value of the next element in the underlying dataset.",
          "note that the above code snippet will embed the features and labels arrays in your tensorflow graph as tf.constant () operations."
        ]
      }
    ],
    "tf.fill": [
      {
        "title": "35855219",
        "h": "tf.constant",
        "t": "tf.fill",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.99",
        "r_prob": "0.83",
        "prob": "0.7970489999999999",
        "sentences": [
          "a tf.constant () has fixed size and value at graph construction time , so it probably isn ' t the right op for your application.",
          "if you are trying to create a tensor with a dynamic size and the same ( constant ) value for every element , you can use tf.fill () and tf.shape () to create an appropriately - shaped tensor."
        ]
      },
      {
        "title": "37380546",
        "h": "tf.constant",
        "t": "tf.fill",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.53",
        "r_prob": "1.0",
        "prob": "0.4982",
        "sentences": [
          "note that tf.constant () does not accept a dynamic shape as an argument — for then it would not be constant !— but the similar tf.fill () op does."
        ]
      },
      {
        "title": "43007712",
        "h": "tf.constant",
        "t": "tf.fill",
        "r": "S1",
        "h_prob": "0.79",
        "t_prob": "0.6",
        "r_prob": "0.75",
        "prob": "0.3555",
        "sentences": [
          "the shape argument of the tf.constant () op expects a static shape , so you can ' t use a tf.tensor as part of the argument.",
          "fortunately there is another op that will suffice : tf.fill (), which allows the shape ( its dims argument ) to be a tf.tensor."
        ]
      }
    ],
    "tf.convert_to_tensor": [
      {
        "title": "50110336",
        "h": "tf.convert_to_tensor",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.79",
        "t_prob": "0.85",
        "r_prob": "1.0",
        "prob": "0.6715",
        "sentences": [
          "for tf.constant , the input value must be a static non - tensor type.",
          "for example a numpy array.",
          "for tf.convert_to_tensor , the value \" an object whose type has a registered tensor conversion function .\" "
        ]
      },
      {
        "title": "50981199",
        "h": "tf.constant",
        "t": "tf.convert_to_tensor",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.67",
        "r_prob": "1.0",
        "prob": "0.536",
        "sentences": [
          "each time a tensorflow operation expects a tensor input but receives something else , it tries to convert it with tf.convert_to_tensor , and if successful , proceeds normally with that output.",
          "tf.constant is nonetheless useful because it allows you , among other things , to name your constant in the graph and to control its data type ( with name and dtype arguments respectively ). "
        ]
      },
      {
        "title": "43082103",
        "h": "tf.convert_to_tensor",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.63",
        "t_prob": "0.57",
        "r_prob": "0.93",
        "prob": "0.333963",
        "sentences": [
          "for example , input1 is converted to a constant tensor within tf.matmul for your first case , and directly within tf.constant for your second case.",
          "for example , anything object that can be passed to tf.convert_to_tensor can also be passed to any tensorflow function / method that accepts tensors as its arguments ( which can be quite handy at times ). "
        ]
      }
    ],
    "tf.float32": [
      {
        "title": "60066750",
        "h": "tf.float32",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.82",
        "r_prob": "0.99",
        "prob": "0.5926139999999999",
        "sentences": [
          "the problem is that i set the shape in tf.constant ( value , shape = outputs [ key ]. shape ). ",
          "i should have only used tf.constant ( value , dtype = tf.float32 ). "
        ]
      },
      {
        "title": "51996168",
        "h": "tf.float32",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.69",
        "r_prob": "1.0",
        "prob": "0.5864999999999999",
        "sentences": [
          "b1 = tf.variable ( tf.constant ( 0.1 , tf.float32 , [ k ])) "
        ]
      },
      {
        "title": "47469754",
        "h": "tf.float32",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.49",
        "r_prob": "1.0",
        "prob": "0.43119999999999997",
        "sentences": [
          "def score_converter_fn ( logits ): cr = logit_scale cr = tf.constant ([[ cr ]], tf.float32 ) print ( logit_scale ) print ( logits ) scaled_logits = tf.divide ( logits , cr , name =' scale_logits ')   change logit_scale return tf_score_converter_fn ( scaled_logits , name =' convert_scores ') score_converter_fn.__name__ = '% s_with_logit_scale ' % ( tf_score_converter_fn.__name__ ) return score_converter_fn "
        ]
      }
    ],
    "tf.add": [
      {
        "title": "49720236",
        "h": "tf.add",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.61",
        "r_prob": "0.97",
        "prob": "0.42010699999999995",
        "sentences": [
          "computed tensors ( ops / operations ) - these are computed values such as tf.add ( a , b ), the value is discarded ( or returned ) at the end of a call to sess.run.",
          "they are computed only .. ",
          "constants - e.g.",
          "tf.constant ( 12 ). ",
          "variables need to be initialized precisely because they maintain state , so they need an initial state.",
          "if you were to request the value of ans at some point in the future with sess.run ( ans ), it would simply retrieve the value of the variable and return it ( no computation performed ). "
        ]
      },
      {
        "title": "38294089",
        "h": "tf.constant",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.48",
        "t_prob": "0.76",
        "r_prob": "0.96",
        "prob": "0.350208",
        "sentences": [
          "for example tf.constant constructs a new op , but actualy returns a reference to tensor being a result of this operation , namely instance of tensorflow.python.framework.ops.tensor , but it is not a constructor in the oop sense.",
          "in particular things that have actual logic , like tf.add will create both tensorflow.python.framework.ops.operation ( to perform addition ) and tensorflow.python.framework.ops.tensor ( to store the result of the operation ), and only tensor will be returned ( this is what cited part of documentation tries to explain ). ",
          "a simple python - based example might be more usefull , so tf does something like this : "
        ]
      },
      {
        "title": "49350002",
        "h": "tf.add",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.58",
        "t_prob": "0.46",
        "r_prob": "1.0",
        "prob": "0.2668",
        "sentences": [
          "e.g.",
          "tf.constant ( 42 ) and should be specified at compile time , not runtime ( eluding to your primary mistake here ). ",
          "3 ) computed tensors - x = tf.add ( a , b ) is a computed tensor , it ' s computed from a , b.",
          "if you were passing in a vector then it ' s a rank 1 tensor ( aka a vector ). ",
          "now , nextdd fails becuase you tried to create a constant from a variable term , which isn ' t a defined operation."
        ]
      }
    ]
  },
  "data augmentation": {
    "l2 regularization": [
      {
        "title": "59171581",
        "h": "data augmentation",
        "t": "l2 regularization",
        "r": "S2",
        "h_prob": "0.71",
        "t_prob": "0.46",
        "r_prob": "0.99",
        "prob": "0.323334",
        "sentences": [
          "code for regularization is shown below ( you can try l1 regularization or l1_l2 regularization as well ): ",
          "perform image data augmentation using imagedatagenerator."
        ]
      },
      {
        "title": "59172613",
        "h": "data augmentation",
        "t": "l2 regularization",
        "r": "S2",
        "h_prob": "0.7",
        "t_prob": "0.46",
        "r_prob": "0.99",
        "prob": "0.31878",
        "sentences": [
          "code for regularization is shown below ( you can try l1 regularization or l1_l2 regularization as well ): ",
          "perform image data augmentation using imagedatagenerator."
        ]
      },
      {
        "title": "62391176",
        "h": "l2 regularization",
        "t": "data augmentation",
        "r": "S2",
        "h_prob": "0.67",
        "t_prob": "0.47",
        "r_prob": "1.0",
        "prob": "0.3149",
        "sentences": [
          "add more data use data augmentation use architectures that generalize well add regularization ( mostly dropout , l1 / l2 regularization are also possible ) reduce architecture complexity.",
          ". ",
          "use data augmentation.",
          "reduce architecture complexity .. "
        ]
      }
    ]
  },
  "tf.stack": {
    "tf.map_fn": [
      {
        "title": "51315996",
        "h": "tf.map_fn",
        "t": "tf.stack",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.8929999999999999",
        "sentences": [
          "here is another example using tf.map_fn () to cleverly iterate over a tensor , and using tf.stack () to convert the list elements back into a tensor."
        ]
      },
      {
        "title": "48138052",
        "h": "tf.map_fn",
        "t": "tf.stack",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.846",
        "sentences": [
          "in your specific example of wanting to map individually to each of the x , y , z coordinates , you can accomplish this readily with tf.split () and tf.stack (). ",
          "if so , then use tf.split () to break up k into kx , ky , kz.",
          "then apply your map operation ( i use tf.map_fn () for this purpose typically ), and then finally stack things back together with tf.stack (). "
        ]
      },
      {
        "title": "57974089",
        "h": "tf.stack",
        "t": "tf.map_fn",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.91",
        "r_prob": "0.54",
        "prob": "0.47174400000000005",
        "sentences": [
          "you can make it into a tensor for example with tf.stack or tf.convert_to_tensor : ",
          "however , you can do the same operation without tf.map_fn more simply and efficiently like this : "
        ]
      }
    ],
    "tf.concat": [
      {
        "title": "41079282",
        "h": "tf.stack",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.7789999999999999",
        "sentences": [
          "if instead you have a fixed smaller number of arrays ( loops in the context of the question ), you can do this via tf.stack (). ",
          "let ' s say your first loop produces array v1 , second v2 and third v3.",
          "you can use tf.stack () as follows , "
        ]
      },
      {
        "title": "59922177",
        "h": "tf.concat",
        "t": "tf.stack",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.65",
        "r_prob": "0.98",
        "prob": "0.59241",
        "sentences": [
          "the first tf.stack works because all the input tensors x , y , z have the same shape ( 2 ,). ",
          "in order to join them , you can use tf.concat but with adjusted tensor ' s shape : "
        ]
      },
      {
        "title": "53975093",
        "h": "tf.stack",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.82",
        "r_prob": "0.98",
        "prob": "0.5866279999999999",
        "sentences": [
          "tf.stack increases the rank of the tensor ( creating a new axis ) and combines them in the new axis.",
          "if you want to combine tensors along an existing axis , you should use tf.concat."
        ]
      },
      {
        "title": "44672272",
        "h": "tf.concat",
        "t": "tf.stack",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.51",
        "r_prob": "1.0",
        "prob": "0.34680000000000005",
        "sentences": [
          "edit : there exists a simpler solution using tf.stack instead of tf.concat."
        ]
      }
    ]
  },
  "tf.keras.metrics.precision": {},
  "tf.keras": {
    "tf.data": [
      {
        "title": "63074542",
        "h": "tf.keras",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9503999999999999",
        "sentences": [
          "if x is a tf.data dataset , and ' steps_per_epoch ' is none , the epoch will run until the input dataset is exhausted."
        ]
      },
      {
        "title": "54117754",
        "h": "tf.keras",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.8246699999999999",
        "sentences": [
          "tf.keras ( https :// www.tensorflow.org / guide / keras ) implements the keras api specification within tensorflow.",
          "in addition , the tf.keras api is optimized to work well with other tensorflow modules : you can pass a tf.data dataset to the.fit () method of a tf.keras model , for instance , or convert a tf.keras model to a tensorflow estimator with tf.keras.estimator.model_to_estimator."
        ]
      },
      {
        "title": "49970811",
        "h": "tf.keras",
        "t": "tf.data",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.75",
        "r_prob": "0.94",
        "prob": "0.6345",
        "sentences": [
          "tf.keras ( formerly tf.contrib.keras ) is an implementation of keras 2 implemented exclusively with / for tensorflow.",
          "as a rule of thumb , if your code use any tensorflow - specific code , say anything in tf.data.",
          "* for providing inputs or tf.summary.",
          "* for visualization in tensorboard , it is simpler to just use tf.keras.",
          "if you don ' t care much about being framework - agnostic but don ' t use tensorflow - specific code , i would probably advise to go with tf.keras and start using tensorflow - specific code , esp.",
          "tf.data which is a game - changer in my opinion.",
          "i attended a talk by chollet on tf2 ( couldn ' t find a recording online ) in which he basically said that support for frameworks other than tf would eventually drop and future developments of keras would happen exclusively in tf.keras.",
          "so today , my answer would be to use tf.keras by default , and keep keras for legacy projects that would be hard to migrate -- that is the future - proof choice for keras."
        ]
      }
    ]
  },
  "tf.concat": {
    "tf.split": [
      {
        "title": "39301914",
        "h": "tf.concat",
        "t": "tf.split",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "1.0",
        "r_prob": "0.99",
        "prob": "0.9702",
        "sentences": [
          "in the mean time , you can work around it by manually adding tf.split () or tf.slice (), and tf.concat () operations to partition the tensor for transfer."
        ]
      },
      {
        "title": "57549853",
        "h": "tf.concat",
        "t": "tf.split",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.86",
        "r_prob": "1.0",
        "prob": "0.774",
        "sentences": [
          ". ",
          "if your network has a lot of memory heavy operations : tf.slice , tf.split , tf.concat , or a lot of elementwise operations ( e.g , tf.exp ( tf.exp ( a + b - c ) ), there is little that can be done by tensorrt since the fused kernel is not implemented ( cannot fuse two consecutive exp ops ) or there is little to optimize for memory operations.",
          "launching a cuda kernel would incur overhead ( say 0.1 ms ). "
        ]
      },
      {
        "title": "57452253",
        "h": "tf.concat",
        "t": "tf.split",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.76",
        "r_prob": "0.99",
        "prob": "0.6320159999999999",
        "sentences": [
          "you can use tensor manipulation such as tf.split and tf.concat."
        ]
      },
      {
        "title": "45585174",
        "h": "tf.split",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.44",
        "t_prob": "0.73",
        "r_prob": "1.0",
        "prob": "0.3212",
        "sentences": [
          "the error is due to tensorflow version , syntax of tf.split is changed in the newer version.",
          "there is another same problem with tf.concat "
        ]
      }
    ],
    "tf.reshape": [
      {
        "title": "46439976",
        "h": "tf.concat",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.99",
        "r_prob": "0.97",
        "prob": "0.9218879999999999",
        "sentences": [
          "assuming the two tensors have the same shape in the outer ( none ) dimension and you want to alternate between rows of the two tensors , you can do this by adding a dimension with tf.expand_dims (), concatenating with tf.concat (), then reshaping with tf.reshape (): "
        ]
      },
      {
        "title": "45114096",
        "h": "tf.concat",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.902682",
        "sentences": [
          "in your case you want to merge axis 0 and axis 1 so you can do tf.concat ( tensor , axis = 0 ). ",
          "you can also use tf.reshape ( tensor , ( 28 * 256 , 397 )). "
        ]
      },
      {
        "title": "56833142",
        "h": "tf.concat",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.96",
        "r_prob": "0.97",
        "prob": "0.8753279999999999",
        "sentences": [
          "anyway , you can get what you need with tf.concat , tf.reshape , and tf.transpose.",
          "so you may need to use tf.transpose , interleave using concat and reshape , then transpose again to reorder the dimensions."
        ]
      },
      {
        "title": "36360943",
        "h": "tf.concat",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.79",
        "t_prob": "0.79",
        "r_prob": "0.99",
        "prob": "0.617859",
        "sentences": [
          "i am implementing seq2seq model too.",
          "the code line : ",
          "output = tf.reshape ( tf.concat ( 1 , outputs ), [- 1 , size ]) ",
          "loss = seq2seq.sequence_loss_by_example ([ logits ], [ tf.reshape ( self._targets , [- 1 ])], [ tf.ones ([ batch_size * num_steps ])]) "
        ]
      },
      {
        "title": "41079282",
        "h": "tf.reshape",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.75",
        "t_prob": "0.82",
        "r_prob": "0.91",
        "prob": "0.55965",
        "sentences": [
          "begin by reshaping it to dimensions ( 1 , vector_size ) using tf.reshape (). ",
          "if instead you have a fixed smaller number of arrays ( loops in the context of the question ), you can do this via tf.stack (). "
        ]
      }
    ],
    "tf.expand_dims": [
      {
        "title": "55566010",
        "h": "tf.expand_dims",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.8648",
        "sentences": [
          "you can use tf.tile and tf.expand_dims with tf.concat.",
          "an example : "
        ]
      },
      {
        "title": "55210090",
        "h": "tf.expand_dims",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.8280000000000001",
        "sentences": [
          "implement the call method using tf.sort and taking the desired amount of max and min elements from the sorted input , and concatenating them along a new dimension ( consider using tf.expand_dims and tf.concat ). ",
          "p.s."
        ]
      },
      {
        "title": "47895403",
        "h": "tf.expand_dims",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.66",
        "r_prob": "1.0",
        "prob": "0.6534",
        "sentences": [
          "you can use tf.expand_dims () to convert the smaller tensor to the correct rank , followed by tf.concat (): "
        ]
      },
      {
        "title": "54000471",
        "h": "tf.expand_dims",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.64",
        "r_prob": "0.98",
        "prob": "0.602112",
        "sentences": [
          "i think what you want here is tf.concat.",
          "as a side note , if you are working with tensors , you can use tf.expand_dims ( c2 , 0 ) to promote c2 ( if it is a placeholder / variable ) to a 2 - d tensor."
        ]
      },
      {
        "title": "48404009",
        "h": "tf.expand_dims",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.59",
        "r_prob": "0.99",
        "prob": "0.519849",
        "sentences": [
          "you can obtain the desired output with a combination of tf.concat , tf.tile and tf.expand_dims : ",
          "tf.expand_dims adds a dimension we can then concat on ",
          "finally , tf.concat stitches together the two tensors giving the desired result."
        ]
      },
      {
        "title": "62742291",
        "h": "tf.expand_dims",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.72",
        "r_prob": "0.62",
        "prob": "0.38390399999999997",
        "sentences": [
          "you can use tf.newaxis to effectively reshape your tensor and then use tf.concat ",
          "with tf.expand_dims."
        ]
      }
    ],
    "tf.tile": [
      {
        "title": "43372777",
        "h": "tf.tile",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7968",
        "sentences": [
          "you can use tf.concat () to concatenates the list of tensors , tf.tile () to creates a new tensor by replicating input multiples times , tf.reshape () "
        ]
      },
      {
        "title": "54902443",
        "h": "tf.concat",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.92",
        "r_prob": "0.99",
        "prob": "0.792396",
        "sentences": [
          "you can achieve this with tf.tile or tf.concat : "
        ]
      },
      {
        "title": "48404009",
        "h": "tf.concat",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.59",
        "t_prob": "0.78",
        "r_prob": "0.99",
        "prob": "0.455598",
        "sentences": [
          "you can obtain the desired output with a combination of tf.concat , tf.tile and tf.expand_dims : ",
          "tf.expand_dims adds a dimension we can then concat on ",
          "finally , tf.concat stitches together the two tensors giving the desired result."
        ]
      }
    ],
    "tf.stack": [
      {
        "title": "41079282",
        "h": "tf.stack",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.7789999999999999",
        "sentences": [
          "if instead you have a fixed smaller number of arrays ( loops in the context of the question ), you can do this via tf.stack (). ",
          "let ' s say your first loop produces array v1 , second v2 and third v3.",
          "you can use tf.stack () as follows , "
        ]
      },
      {
        "title": "59922177",
        "h": "tf.concat",
        "t": "tf.stack",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.65",
        "r_prob": "0.98",
        "prob": "0.59241",
        "sentences": [
          "the first tf.stack works because all the input tensors x , y , z have the same shape ( 2 ,). ",
          "in order to join them , you can use tf.concat but with adjusted tensor ' s shape : "
        ]
      },
      {
        "title": "53975093",
        "h": "tf.stack",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.82",
        "r_prob": "0.98",
        "prob": "0.5866279999999999",
        "sentences": [
          "tf.stack increases the rank of the tensor ( creating a new axis ) and combines them in the new axis.",
          "if you want to combine tensors along an existing axis , you should use tf.concat."
        ]
      },
      {
        "title": "44672272",
        "h": "tf.concat",
        "t": "tf.stack",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.51",
        "r_prob": "1.0",
        "prob": "0.34680000000000005",
        "sentences": [
          "edit : there exists a simpler solution using tf.stack instead of tf.concat."
        ]
      }
    ]
  },
  "tf.merge_all_": {},
  "pipelines": {},
  "stochastic gradient descent": {
    "batch gradient descent": [
      {
        "title": "46841168",
        "h": "batch gradient descent",
        "t": "stochastic gradient descent",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.66",
        "r_prob": "0.69",
        "prob": "0.35521199999999997",
        "sentences": [
          "there are mainly 3 types of gradient descent.",
          "specifically , ",
          "stochastic gradient descent.",
          "batch gradient descent.",
          "and you can set the batch_size according to your gradient descent method.",
          "for stochastic gradient descent , batch_size = 1 .. ",
          "for batch gradient descent , batch_size = training dataset size."
        ]
      },
      {
        "title": "38274597",
        "h": "batch gradient descent",
        "t": "stochastic gradient descent",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.28",
        "r_prob": "0.9",
        "prob": "0.19404000000000002",
        "sentences": [
          "training on the whole dataset at each iteration is called batch gradient descent.",
          "training on minibatches ( e.g.",
          "100 samples at a time ) is called stochastic gradient descent.",
          "batch gradient descent typically isn ' t feasible because it requires too much ram."
        ]
      },
      {
        "title": "42726174",
        "h": "batch gradient descent",
        "t": "stochastic gradient descent",
        "r": "S1",
        "h_prob": "0.74",
        "t_prob": "0.24",
        "r_prob": "1.0",
        "prob": "0.17759999999999998",
        "sentences": [
          "this is ok if your batch has size 1 ( stochastic gradient descent ), instead , since you want to do mini - batch gradient descent ( batch size > 1 ), you wanto to minimize the average error over the batch."
        ]
      }
    ],
    "batch size": [
      {
        "title": "38047478",
        "h": "batch size",
        "t": "stochastic gradient descent",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.36",
        "r_prob": "1.0",
        "prob": "0.3024",
        "sentences": [
          "switching from gradient descent to stochastic gradient descent you need to keep some things in mind."
        ]
      },
      {
        "title": "38274597",
        "h": "stochastic gradient descent",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.28",
        "t_prob": "0.96",
        "r_prob": "0.94",
        "prob": "0.252672",
        "sentences": [
          "100 samples at a time ) is called stochastic gradient descent."
        ]
      },
      {
        "title": "49007781",
        "h": "batch size",
        "t": "stochastic gradient descent",
        "r": "S2",
        "h_prob": "0.87",
        "t_prob": "0.28",
        "r_prob": "0.94",
        "prob": "0.228984",
        "sentences": [
          "keep also in mind that sgd ( stochastic gradient descent , mini - batch size = 1 ) tend to be very slow for big datasets ... give us some numbers."
        ]
      },
      {
        "title": "47498466",
        "h": "batch size",
        "t": "stochastic gradient descent",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.3",
        "r_prob": "0.88",
        "prob": "0.20592",
        "sentences": [
          "that is stochastic gradient descent with batch size 1."
        ]
      },
      {
        "title": "52470415",
        "h": "batch size",
        "t": "stochastic gradient descent",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.16",
        "r_prob": "0.61",
        "prob": "0.088816",
        "sentences": [
          "since i want a stochastic gradient descent ( batch size is one ) the following code fixed the problem : "
        ]
      }
    ]
  },
  "activations": {},
  "tf.keras.losses.binarycrossentropy": {},
  "tf.keras.losses.binary_crossentropy": {},
  "tf.feature_column.numeric_column": {},
  "tf.scalar_": {},
  "tf.metrics.auc": {},
  "tf.keras.layers.lambda": {},
  "tf.keras.layers.layer": {},
  "tf.image.random_contrast": {},
  "tf.image.random_brightness": {},
  "tf.nn.conv2d_transpose": {
    "tf.nn.conv2d": [
      {
        "title": "47354833",
        "h": "tf.nn.conv2d_transpose",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.54",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.5346000000000001",
        "sentences": [
          "actually , i ' ve never seen any tensorflow function that supports nhcw format.",
          "for example , tf.nn.conv2d and tf.nn.conv2d_transpose support nhwc ( current default ) and nchw format."
        ]
      },
      {
        "title": "38059483",
        "h": "tf.nn.conv2d_transpose",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.53",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.5141",
        "sentences": [
          "now , the error is a little misleading --- it talks about the ' out_backprop ' argument to ' conv2dcustombackpropinput '. ",
          "the key is that tf.nn.conv2d_transpose is actually just the gradient of tf.nn.conv2d , so tensorflow uses the same code internally ( conv2dcustombackpropinput ) to compute the gradient of tf.nn.conv2d and to compute tf.nn.conv2d_transpose.",
          "since tf.nn.conv2d_transpose is the backward ( gradient ) counterpart of tf.nn.conv2d , one way to see what the correct shapes should be is to use the corresponding forward operation : ",
          "in general , try using tf.nn.conv2d to get a feel for what the relationship between the tensor shapes is.",
          "since tf.nn.conv2d_transpose is its backward counterpart , it has the same relationship between input , output and filter shapes ( but with the roles of the input and output reversed .) "
        ]
      },
      {
        "title": "48265399",
        "h": "tf.nn.conv2d_transpose",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.5",
        "t_prob": "1.0",
        "r_prob": "1.0",
        "prob": "0.5",
        "sentences": [
          "use tf.nn.conv2d ( and tf.nn.conv2d_transpose correspondingly ). "
        ]
      }
    ]
  },
  "tf.nn.sparse_softmax_cross_entropy_with_logits": {
    "tf.nn.softmax_cross_entropy_with_logits": [
      {
        "title": "36088396",
        "h": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "indices of columns in the predictions matrix ). ",
          "if your problem is a single - class problem , then i assume that your y_ tensor is a one - hot encoding of the true labels for your examples ( for example because you also pass them to an op like tf.nn.softmax_cross_entropy_with_logits (). ",
          "( also , consider using tf.nn.sparse_softmax_cross_entropy_with_logits () as your loss function , because it may be more efficient.",
          "). "
        ]
      },
      {
        "title": "39831223",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9503999999999999",
        "sentences": [
          "use tf.nn.sparse_softmax_cross_entropy_with_logits instead of tf.nn.softmax_cross_entropy_with_logits."
        ]
      },
      {
        "title": "64213624",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.9306",
        "sentences": [
          "actually tf.nn.softmax_cross_entropy_with_logits does not impose the restriction that labels must be on - hot encoded , so you can go ahead and use non - one - hot label vectors.",
          "you might be confusing this with tf.nn.sparse_softmax_cross_entropy_with_logits which does impose this restriction."
        ]
      },
      {
        "title": "60262947",
        "h": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "0.98",
        "prob": "0.9219839999999999",
        "sentences": [
          "hence , specifying the 2.0 compatible calls for all the functions , we discussed above , if we migrate from 1.x to 2.x , for the benefit of the community.",
          "functions in 1.x : ",
          "tf.nn.softmax.",
          "tf.nn.softmax_cross_entropy_with_logits.",
          "tf.nn.sparse_softmax_cross_entropy_with_logits.",
          "for more information about migration from 1.x to 2.x , please refer this migration guide."
        ]
      },
      {
        "title": "38101834",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.8648",
        "sentences": [
          "i would just like to add 2 things to accepted answer that you can also find in tf documentation.",
          "first : ",
          "tf.nn.softmax_cross_entropy_with_logits ",
          "if they are not , the computation of the gradient will be incorrect.",
          "second : ",
          "tf.nn.sparse_softmax_cross_entropy_with_logits "
        ]
      },
      {
        "title": "34243720",
        "h": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.97",
        "r_prob": "0.96",
        "prob": "0.8008319999999999",
        "sentences": [
          "in contrast , tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function ( but it does it all together in a more mathematically careful way ). ",
          "the output of tf.nn.softmax_cross_entropy_with_logits on a shape [ 2 , 5 ] tensor is of shape [ 2 , 1 ] ( the first dimension is treated as the batch ). "
        ]
      },
      {
        "title": "47145069",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.7742",
        "sentences": [
          "thanks to maosi chen , i found the issue.",
          "it was because the ",
          "tf.nn.sparse_softmax_cross_entropy_with_logits ",
          "requires labels to have one less dimension than logits.",
          "however , it was also possible to use ",
          "tf.nn.softmax_cross_entropy_with_logits "
        ]
      }
    ]
  },
  "tf.random_normal": {},
  "tf.train.get_or_create_step": {},
  "tf.train.global_step": {},
  "tf.train.batch": {
    "tf.train.string_input_producer": [
      {
        "title": "50719652",
        "h": "tf.train.string_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "1.0",
        "r_prob": "1.0",
        "prob": "0.99",
        "sentences": [
          "when i had this issue with tf.train.string_input_producer () and tf.train.batch () initializing the local variables before i started the coordinator solved the problem."
        ]
      },
      {
        "title": "46010790",
        "h": "tf.train.string_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.8613",
        "sentences": [
          "tf.train.string_input_producer ([ filepath ]) returns a queue.",
          "if you want to reach a batch of element , you can use tf.train.batch with output of queue as input to tf.train.batch."
        ]
      },
      {
        "title": "38726143",
        "h": "tf.train.string_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.81",
        "sentences": [
          "the tf.train.string_input_producer creates a shuffled queue with the given keys.",
          "the tf.train.batch op create another queue that contains batches of data.",
          "because we created queues we need to take care and run the queuerunner objects before we start training.",
          "because we created queues we need to take care and run the queuerunner objects before we start training."
        ]
      }
    ],
    "tf.train.start_queue_runners": [
      {
        "title": "42909460",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.9507959999999999",
        "sentences": [
          "the first line calls tf.train.start_queue_runners () to start background threads for filling queues.",
          "the second line calls tf.train.batch (), which adds a new queue , which requires an additional background thread to be started to fill that queue , but that thread isn ' t started , so the program hangs.",
          "the solution is quite simple : reverse the two lines , so that tf.train.start_queue_runners () is called after tf.train.batch (). "
        ]
      },
      {
        "title": "39238304",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9408",
        "sentences": [
          "if you move the call to tf.train.start_queue_runners () after the calls to tf.train.batch (), then your program should no longer deadlock.",
          "why does this happen ? ",
          "the tf.train.batch () function internally creates queues to buffer the data as it is being batched , and in tensorflow the common way to populate these queues is to create a \" queue runner \", which is ( usually ) a background thread that moves elements into a queue.",
          "the tf.train.start_queue_runners () function starts background threads for all registered queue runners at the point when it is called , but if it is called before the queue runners are created , then those threads won ' t be started."
        ]
      },
      {
        "title": "41413058",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9405",
        "sentences": [
          "tl ; dr : you need to add tf.train.start_queue_runners ( sess ) after you run the init_op , and before you start the training loop.",
          "the tf.train.batch () function uses tensorflow queues to accumulate input data into batches.",
          "these queues are filled by background threads , which are created when you call tf.train.start_queue_runners (). "
        ]
      },
      {
        "title": "35346656",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.9021",
        "sentences": [
          "using tf.tfrecordreader ), and batching ( e.g.",
          "using tf.train.batch ()) pipelines implicitly create queues and queue runners.",
          "if you had to run tf.train.start_queue_runners () in your python program , then you will need to do the equivalent thing in your c ++ code , by forking threads to run the appropriate q.enqueue () ops."
        ]
      },
      {
        "title": "49506723",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.99",
        "r_prob": "0.98",
        "prob": "0.8828820000000001",
        "sentences": [
          "tf.train.batch creates own queue runners : ",
          "tensoflow has a function that starts all queue runners collected in the graph : tf.train.start_queue_runners."
        ]
      }
    ],
    "tf.train.slice_input_producer": [
      {
        "title": "41864611",
        "h": "tf.train.slice_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.873",
        "sentences": [
          "assuming you can get a list of filenames , image_paths and a numpy array of labels labels , you can bind them together and operate on individual examples with tf.train.slice_input_producer then batch them together using tf.train.batch."
        ]
      },
      {
        "title": "48991201",
        "h": "tf.train.slice_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.8526",
        "sentences": [
          "tf.train.batch simply groups upstream samples into batches , and nothing more.",
          "for example , if your training data fits into a tensor , you could use tf.train.slice_input_producer to produce samples.",
          "this function has arguments for shuffling and epochs."
        ]
      },
      {
        "title": "46053671",
        "h": "tf.train.slice_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.99",
        "r_prob": "0.95",
        "prob": "0.8182349999999999",
        "sentences": [
          "you can use tf.train.batch , but you also have to use a queue for filenames such as tf.train.slice_input_producer "
        ]
      },
      {
        "title": "41866982",
        "h": "tf.train.slice_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7968",
        "sentences": [
          "use tf.train.slice_input_producer to get a tensor for a single example ;. ",
          "load images from filenames ;. ",
          "batch them together using tf.train.batch to group them up .. "
        ]
      },
      {
        "title": "44350073",
        "h": "tf.train.slice_input_producer",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.96",
        "r_prob": "0.54",
        "prob": "0.47174400000000005",
        "sentences": [
          "set capacity = 55000 for tf.train.slice_input_producer.",
          "( 55000 is the size of mnist training set in my case ).",
          "set num_threads = 5 for tf.train.batch .. ",
          "set capacity = 500 for tf.train.batch .. ",
          "this was possible because titan x has enough memory space to preload entire dataset.",
          ". "
        ]
      }
    ]
  },
  "backend": {},
  "tf.contrib.framework.get_name_scope": {},
  "tf.image.resize_images": {},
  "tf.train.start_queue_runners": {
    "tf.train.string_input_producer": [
      {
        "title": "45807749",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.string_input_producer",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9602999999999999",
        "sentences": [
          "the call to tf.train.start_queue_runners () will start background threads for all input pipeline stages that have been defined up to that point.",
          "the call to input_pipeline () creates two new input pipeline stages ( in the calls to tf.train.string_input_producer () and tf.train.shuffle_batch ()). "
        ]
      },
      {
        "title": "38595382",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.string_input_producer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.94",
        "r_prob": "0.95",
        "prob": "0.8840699999999999",
        "sentences": [
          "you must call tf.train.start_queue_runners ( sess ) before you call train_data.eval () or train_labels.eval (). ",
          "the tf.train.start_queue_runners () call tells tensorflow to start fetching records into these buffers ; without calling it the buffers remain empty and eval () hangs indefinitely."
        ]
      },
      {
        "title": "44913933",
        "h": "tf.train.string_input_producer",
        "t": "tf.train.start_queue_runners",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.7896",
        "sentences": [
          "you can put even all examples in one file or have 10k per file.",
          "regarding shuffling : there are two types shuffling which serve different purposes and shuffle different things : ",
          "tf.train.string_input_producer shuffle : boolean.",
          "so if you have a few files [' file1 ', ' file2 ', ..., ' filen '] this randomly selects a file from this list.",
          "if case of false , the files follow one after each other .. ",
          "tf.train.shuffle_batch creates batches by randomly shuffling tensors.",
          "so it takes batch_size tensors from your queue ( you will need to create a queue with tf.train.start_queue_runners ) and shuffles them .. "
        ]
      },
      {
        "title": "34258214",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.string_input_producer",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.79",
        "r_prob": "0.61",
        "prob": "0.467443",
        "sentences": [
          "first , randomly shuffle the order in which you input your datafiles , by reading from them using a tf.train.string_input_producer with shuffle = true that feeds into whatever input method you use ( if you can put your examples into tf.example proto format , that ' s easy to use with parse_example ). ",
          "second , you need to mix at a finer granularity.",
          "keep in mind that the batch functions add a queuerunner to the queue_runners collection , so you need to run tf.train.start_queue_runners () "
        ]
      }
    ],
    "tf.train.batch": [
      {
        "title": "42909460",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.9507959999999999",
        "sentences": [
          "the first line calls tf.train.start_queue_runners () to start background threads for filling queues.",
          "the second line calls tf.train.batch (), which adds a new queue , which requires an additional background thread to be started to fill that queue , but that thread isn ' t started , so the program hangs.",
          "the solution is quite simple : reverse the two lines , so that tf.train.start_queue_runners () is called after tf.train.batch (). "
        ]
      },
      {
        "title": "39238304",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9408",
        "sentences": [
          "if you move the call to tf.train.start_queue_runners () after the calls to tf.train.batch (), then your program should no longer deadlock.",
          "why does this happen ? ",
          "the tf.train.batch () function internally creates queues to buffer the data as it is being batched , and in tensorflow the common way to populate these queues is to create a \" queue runner \", which is ( usually ) a background thread that moves elements into a queue.",
          "the tf.train.start_queue_runners () function starts background threads for all registered queue runners at the point when it is called , but if it is called before the queue runners are created , then those threads won ' t be started."
        ]
      },
      {
        "title": "41413058",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9405",
        "sentences": [
          "tl ; dr : you need to add tf.train.start_queue_runners ( sess ) after you run the init_op , and before you start the training loop.",
          "the tf.train.batch () function uses tensorflow queues to accumulate input data into batches.",
          "these queues are filled by background threads , which are created when you call tf.train.start_queue_runners (). "
        ]
      },
      {
        "title": "35346656",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.9021",
        "sentences": [
          "using tf.tfrecordreader ), and batching ( e.g.",
          "using tf.train.batch ()) pipelines implicitly create queues and queue runners.",
          "if you had to run tf.train.start_queue_runners () in your python program , then you will need to do the equivalent thing in your c ++ code , by forking threads to run the appropriate q.enqueue () ops."
        ]
      },
      {
        "title": "49506723",
        "h": "tf.train.start_queue_runners",
        "t": "tf.train.batch",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.99",
        "r_prob": "0.98",
        "prob": "0.8828820000000001",
        "sentences": [
          "tf.train.batch creates own queue runners : ",
          "tensoflow has a function that starts all queue runners collected in the graph : tf.train.start_queue_runners."
        ]
      }
    ]
  },
  "tf.print": {
    "tf.function": [
      {
        "title": "57704523",
        "h": "tf.print",
        "t": "tf.function",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.77",
        "r_prob": "1.0",
        "prob": "0.6237",
        "sentences": [
          "sometimes the way tf.function executes can cause us a little confusion - particularly when we mix in vanilla python operations such as print (). ",
          "we should remember that when we decorate a function with tf.function it ' s no longer just a python function.",
          "it behaves a little differently in order to enable fast and efficient use in tf.",
          "the first thing to note is that if we use tf.print () in place of print () then we get the expected output : ",
          "this means that the python only operations ( such as print () could get executed more than once ) but tf operations such as tf.print () will behave as you would normally expect.",
          "and i also want to know how tf.function woks when two functions call each other.",
          "in general , we need only decorate the \" outer \" function with tf.function (. fun () in your example ) but if you could call the inner function directly too then you are free to decorate that too."
        ]
      },
      {
        "title": "55303437",
        "h": "tf.print",
        "t": "tf.function",
        "r": "S1",
        "h_prob": "0.46",
        "t_prob": "0.84",
        "r_prob": "0.93",
        "prob": "0.35935200000000006",
        "sentences": [
          "inside the graph indicated by the decorator @ tf.function , you can use tf.print to print the values of your tensor."
        ]
      },
      {
        "title": "56307018",
        "h": "tf.print",
        "t": "tf.function",
        "r": "S1",
        "h_prob": "0.6",
        "t_prob": "0.7",
        "r_prob": "0.81",
        "prob": "0.3402",
        "sentences": [
          "what ' s the difference between tf.print and python print ?. ",
          "tf.print is a tensorflow construct , that prints on standard error by default and , more importantly , it produces an operation when evaluated.",
          "tf.function is able to capture the generated operation of tf.print and convert it to a graph node.",
          "therefore , tf.function is not able to convert it in its graph equivalent and executes it only during the function tracing.",
          "the above only apply when there ' s tf.function decorator ? ",
          "outside that , tf.print runs before python print ?. ",
          "yes.",
          "tf.print does not run before or after print.",
          "at any rate , i suggest you read the three articles linked since they cover in detail this and other peculiarities of tf.function."
        ]
      },
      {
        "title": "56900992",
        "h": "tf.function",
        "t": "tf.print",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.29",
        "r_prob": "1.0",
        "prob": "0.23199999999999998",
        "sentences": [
          "if you want to print in - graph , use tf.print."
        ]
      },
      {
        "title": "56306679",
        "h": "tf.print",
        "t": "tf.function",
        "r": "S1",
        "h_prob": "0.4",
        "t_prob": "0.59",
        "r_prob": "0.58",
        "prob": "0.13687999999999997",
        "sentences": [
          "that is why outside of your @ tf.function function , the output of the python print is a number ( tensorflow executes the graph directly and gives out the number to the normal print function ) and that is also why tf.print prints immediately.",
          "on the other hand , inside the @ tf.function function tensorflow will not execute the graph immediately."
        ]
      }
    ]
  },
  "tf.nn.elu": {},
  "tf.nn.avg_pool": {},
  "tf.nn.pool": {},
  "tf.data.textlinedataset": {},
  "tf.estimator.evalspec": {},
  "tf.estimator.runconfig": {},
  "tf.contrib.data.shuffle_and_repeat": {},
  "tf.data.": {
    "tf.data.dataset": [
      {
        "title": "59767370",
        "h": "tf.data.dataset",
        "t": "tf.data.",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.94",
        "r_prob": "0.99",
        "prob": "0.921294",
        "sentences": [
          "tf.data.csv returns a tf.data.dataset.",
          "if you would like to evaluate a tf.data.dataset , the method evaluatedataset can be used instead.",
          "evaluatedataset returns a promise."
        ]
      },
      {
        "title": "47946271",
        "h": "tf.data.dataset",
        "t": "tf.data.",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.87",
        "r_prob": "0.99",
        "prob": "0.835461",
        "sentences": [
          "if you implement the same pipeline using the tf.data.dataset api and using queues , the performance of the dataset version should be better than the queue - based version.",
          "however , there are a few performance best practices to observe in order to get the best performance.",
          "we have collected these in a performance guide for tf.data.",
          "adding dataset.prefetch ( 1 ) to the end of your pipeline will give you most of the benefit of prefetching , but you might need to tune this further.",
          "in a dataset pipeline , this would be equivalent to dataset.repeat ( num_epochs ). shuffle ( n ). "
        ]
      },
      {
        "title": "48713164",
        "h": "tf.data.",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.99",
        "r_prob": "0.92",
        "prob": "0.7741800000000001",
        "sentences": [
          "i advise you to read the tutorial by @ mrry on tf.data.",
          "on slide 42 he explains how to use tf.data.dataset.interleave () to read multiple tfrecord files at the same time."
        ]
      }
    ]
  },
  "tf.train.momentumoptimizer": {},
  "tf.train.exponentialmovingaverage": {},
  "tf.module": {},
  "tf.contrib.framework.load_variable": {},
  "tf.compat.v1.enable_v2_behavior": {},
  "tf.reshape": {
    "tf.concat": [
      {
        "title": "46439976",
        "h": "tf.concat",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.99",
        "r_prob": "0.97",
        "prob": "0.9218879999999999",
        "sentences": [
          "assuming the two tensors have the same shape in the outer ( none ) dimension and you want to alternate between rows of the two tensors , you can do this by adding a dimension with tf.expand_dims (), concatenating with tf.concat (), then reshaping with tf.reshape (): "
        ]
      },
      {
        "title": "45114096",
        "h": "tf.concat",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.902682",
        "sentences": [
          "in your case you want to merge axis 0 and axis 1 so you can do tf.concat ( tensor , axis = 0 ). ",
          "you can also use tf.reshape ( tensor , ( 28 * 256 , 397 )). "
        ]
      },
      {
        "title": "56833142",
        "h": "tf.concat",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.96",
        "r_prob": "0.97",
        "prob": "0.8753279999999999",
        "sentences": [
          "anyway , you can get what you need with tf.concat , tf.reshape , and tf.transpose.",
          "so you may need to use tf.transpose , interleave using concat and reshape , then transpose again to reorder the dimensions."
        ]
      },
      {
        "title": "36360943",
        "h": "tf.concat",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.79",
        "t_prob": "0.79",
        "r_prob": "0.99",
        "prob": "0.617859",
        "sentences": [
          "i am implementing seq2seq model too.",
          "the code line : ",
          "output = tf.reshape ( tf.concat ( 1 , outputs ), [- 1 , size ]) ",
          "loss = seq2seq.sequence_loss_by_example ([ logits ], [ tf.reshape ( self._targets , [- 1 ])], [ tf.ones ([ batch_size * num_steps ])]) "
        ]
      },
      {
        "title": "41079282",
        "h": "tf.reshape",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.75",
        "t_prob": "0.82",
        "r_prob": "0.91",
        "prob": "0.55965",
        "sentences": [
          "begin by reshaping it to dimensions ( 1 , vector_size ) using tf.reshape (). ",
          "if instead you have a fixed smaller number of arrays ( loops in the context of the question ), you can do this via tf.stack (). "
        ]
      }
    ],
    "tf.shape": [
      {
        "title": "40666375",
        "h": "tf.reshape",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9211999999999999",
        "sentences": [
          "to complete your tf.shape () call , try tensor2 = tf.reshape ( tensor , tf.tensorshape ([ num_rows * num_cols , 1 ])). ",
          "or you can directly do tensor2 = tf.reshape ( tensor , tf.tensorshape ([- 1 , 1 ])) where its first dimension can be inferred."
        ]
      },
      {
        "title": "50810041",
        "h": "tf.shape",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.86",
        "r_prob": "0.97",
        "prob": "0.734096",
        "sentences": [
          "dont tile , use broadcasting : t * tf.reshape ( params , [- 1 , 1 , 1 , tf.shape ( n_params )[ 1 ]]) "
        ]
      },
      {
        "title": "37477055",
        "h": "tf.reshape",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.93",
        "r_prob": "0.8",
        "prob": "0.6770400000000001",
        "sentences": [
          "you can do it easily with tf.reshape () without knowing the batch size.",
          "you can see it in tf.reshape (). ",
          "thanks @ kbrose.",
          "for the cases where more than 1 dimension are undefined , we can use tf.shape () with tf.reduce_prod () alternatively.",
          "tf.shape () returns a shape tensor which can be evaluated in runtime.",
          "the difference between tf.get_shape () and tf.shape () can be seen in the doc."
        ]
      }
    ],
    "tf.matmul": [
      {
        "title": "47969359",
        "h": "tf.reshape",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.97",
        "r_prob": "0.92",
        "prob": "0.8834759999999999",
        "sentences": [
          "this essentially casts a matrix of , say , rank 3 to one with rank 2 by \" stacking the matrices \" one on top of the other.",
          "you can use this : tf.reshape ( tf.matmul ( tf.reshape ( aijk ,[ i * j , k ]), bkl ),[ i , j , l ]) "
        ]
      },
      {
        "title": "46382856",
        "h": "tf.matmul",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.97",
        "r_prob": "0.94",
        "prob": "0.811502",
        "sentences": [
          "change model = tf.nn.softmax ( tf.matmul ( k , w )+ b ) to model = tf.nn.softmax ( tf.reshape ( tf.matmul ( k , w )+ b , [- 1 ])) ",
          "the output of tf.matmul ( k , w )+ b is a 2d array.",
          "tf.reshape change the size of tf.matmul ( k , w )+ b to [ 4 ]. "
        ]
      },
      {
        "title": "38885030",
        "h": "tf.reshape",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.88",
        "r_prob": "1.0",
        "prob": "0.7303999999999999",
        "sentences": [
          "as npe mentions in their comment , the tf.matmul () op expects both of its inputs to be two - dimensional tensors , but your arguments weights and weights2 are one - dimensional tensors.",
          "tf.reshape () as follows : "
        ]
      },
      {
        "title": "50167877",
        "h": "tf.reshape",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.64",
        "t_prob": "0.81",
        "r_prob": "0.94",
        "prob": "0.48729600000000006",
        "sentences": [
          "after reshaping conv4 at line fc1 = tf.reshape ( conv4 ,[ 256 ,- 1 ]), the shape of fc1 is ( 256 , 2048 ) and the weight matrix w_fc1 has shape ( 256 , 1024 ). "
        ]
      },
      {
        "title": "40910942",
        "h": "tf.matmul",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.49",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.4704",
        "sentences": [
          "i think you want tf.squeeze or tf.reshape.",
          "you also want tf.matmul instead of tf.mul in your example if you want to do matrix multiplication instead of elementwise multiplication."
        ]
      },
      {
        "title": "48012244",
        "h": "tf.reshape",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.47",
        "r_prob": "1.0",
        "prob": "0.44179999999999997",
        "sentences": [
          "therefore , you need to know the values of these variables and of all the intermediate tensors which take part in the calculation of cost , including the gradients ( which are tensors too and are implicitly added to the computational graph ). ",
          "w.",
          "b.",
          "tf.reshape ( x , [- 1 , n ]). ",
          "tf.matmul ( ..., w ). ",
          "tf.reduce_mean (..., axis = 1 ). ",
          "tf.reduce_mean ( sq_error , name =\" cost \" ). "
        ]
      }
    ],
    "tf.tensor": [
      {
        "title": "35375029",
        "h": "tf.tensor",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.855",
        "sentences": [
          "the reason for the error is that tf.reshape () expects a value that is convertible to a tf.tensor as its second argument.",
          "tensorflow will automatically convert a list of python numbers to a tf.tensor but will not automatically convert a mixed list of numbers and tensors ( such as a tf.placeholder ())— instead raising the somewhat unintuitive error message you saw."
        ]
      },
      {
        "title": "35464950",
        "h": "tf.reshape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.72",
        "r_prob": "0.97",
        "prob": "0.62856",
        "sentences": [
          "to elaborate on the faq entry you quoted , tensor.set_shape () is a pure - python function that improves the shape information for a given tf.tensor object.",
          "if you want to make a new tensor with that shape from the contents of t , you can use reshaped_t = tf.reshape ( t , ( 478 , 717 , 3 )). ",
          "this creates a new tf.tensor object in python ; the actual implementation of tf.reshape () does this using a shallow copy of the tensor buffer , so it is inexpensive in practice."
        ]
      },
      {
        "title": "34420827",
        "h": "tf.reshape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.55",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.5225",
        "sentences": [
          "this — not very helpful — error is raised when one of the values in the feed_dict argument to tf.session.run () is a tf.tensor object ( in this case , the result of tf.reshape ()). "
        ]
      }
    ],
    "tf.variable": [
      {
        "title": "46451497",
        "h": "tf.reshape",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.98",
        "r_prob": "0.92",
        "prob": "0.838488",
        "sentences": [
          "your error message shows exact reason why it is raised.",
          "using tf.reshape function.tf.reshape ( x , shape =[ 387 , 7 * 10 ]) will be works , and also change your w to right dimension to multiply.",
          "like , tf.variable ( tf.ones ([ 7 * 10 , 1 ])). "
        ]
      },
      {
        "title": "39715887",
        "h": "tf.reshape",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "0.74",
        "prob": "0.703444",
        "sentences": [
          "can you try output = tf.reshape ( output , [ batch_size , num_hidden ]) and weight = tf.variable ( tf.random_normal ([ num_hidden , num_classes ])) and let me know how that goes ? "
        ]
      },
      {
        "title": "61770185",
        "h": "tf.reshape",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.68",
        "r_prob": "0.67",
        "prob": "0.3052520000000001",
        "sentences": [
          "the problem is related to eager execution tf 2.0 -- any operations such as tf.reshape are run the moment they are encountered.",
          "build is only called a single time for a given model.",
          "now , what is happening is that you are creating a tensor weights2 , which is a reshaped version of the tf.variable weights but is not itself a tf.variable ( ops generally return tensors , not variables ). ",
          "this does not happen in the else case because here , weights2 is just another name referring to the actual tf.variable weights.",
          "this works because now the reshape operation is part of the model call graph , i.e."
        ]
      }
    ],
    "batch size": [
      {
        "title": "40353437",
        "h": "batch size",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.92",
        "r_prob": "0.97",
        "prob": "0.8210080000000001",
        "sentences": [
          "q1.",
          "10 is your batch size.",
          "q3.",
          "tf.reshape () with a single dimension \"- 1 \" leaves the dimension automatically calculated by the program so that the total size remains the same."
        ]
      },
      {
        "title": "37873120",
        "h": "batch size",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.76",
        "t_prob": "0.93",
        "r_prob": "0.92",
        "prob": "0.6502560000000002",
        "sentences": [
          "you can use - 1 for the none dimension.",
          "you can do it easily with tf.reshape () without knowing the batch size.",
          "you can see it in tf.reshape (). "
        ]
      },
      {
        "title": "37477055",
        "h": "batch size",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.76",
        "t_prob": "0.91",
        "r_prob": "0.94",
        "prob": "0.6501039999999999",
        "sentences": [
          "you can do it easily with tf.reshape () without knowing the batch size.",
          "you can see it in tf.reshape (). "
        ]
      }
    ],
    "tf.tile": [
      {
        "title": "43372777",
        "h": "tf.tile",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.7885",
        "sentences": [
          "you can use tf.concat () to concatenates the list of tensors , tf.tile () to creates a new tensor by replicating input multiples times , tf.reshape () "
        ]
      },
      {
        "title": "35367161",
        "h": "tf.tile",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7776000000000001",
        "sentences": [
          "you can achieve the effect of np.repeat () using a combination of tf.tile () and tf.reshape (): ",
          "you can simply compute jdx using tf.tile (): "
        ]
      },
      {
        "title": "44173148",
        "h": "tf.tile",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.76",
        "r_prob": "0.52",
        "prob": "0.35568000000000005",
        "sentences": [
          "you should use tf.tile , not concat.",
          "if you need your tensor to have a slightly different shape , read about the second parameter in tile function and also use tf.reshape "
        ]
      },
      {
        "title": "44490169",
        "h": "tf.reshape",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.44",
        "r_prob": "0.86",
        "prob": "0.32920799999999995",
        "sentences": [
          "sadly the same function is not still implemented in tf.",
          "you can implement it by combining tf.tile , tf.reshape , tf.squeeze."
        ]
      }
    ],
    "tf.squeeze": [
      {
        "title": "40910942",
        "h": "tf.squeeze",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7392",
        "sentences": [
          "i think you want tf.squeeze or tf.reshape."
        ]
      },
      {
        "title": "44490169",
        "h": "tf.squeeze",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.7",
        "t_prob": "0.87",
        "r_prob": "0.99",
        "prob": "0.60291",
        "sentences": [
          "sadly the same function is not still implemented in tf.",
          "you can implement it by combining tf.tile , tf.reshape , tf.squeeze."
        ]
      },
      {
        "title": "59986249",
        "h": "tf.squeeze",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.7",
        "r_prob": "1.0",
        "prob": "0.511",
        "sentences": [
          "however , please note that tf.squeeze ( w ) only squeezes the first layer in the case of a multilayer tensor , whereas tf.reshape ( w ,[- 1 ]) will flatten the entire tensor regardless of depth.",
          "tf.squeeze ( w ) will output "
        ]
      },
      {
        "title": "47658937",
        "h": "tf.squeeze",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.58",
        "t_prob": "0.87",
        "r_prob": "0.99",
        "prob": "0.49955399999999994",
        "sentences": [
          "why do you say tf.squeeze is not supported ? ",
          "in order to remove 1 dimensional axis from tensor , tf.squeeze is the correct operation.",
          "but you can achieve your desired work with tf.reshape as well though i will suggest you to make use of tf.squeeze."
        ]
      }
    ],
    "tf.transpose": [
      {
        "title": "56833142",
        "h": "tf.reshape",
        "t": "tf.transpose",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.6",
        "r_prob": "1.0",
        "prob": "0.576",
        "sentences": [
          "anyway , you can get what you need with tf.concat , tf.reshape , and tf.transpose.",
          "so you may need to use tf.transpose , interleave using concat and reshape , then transpose again to reorder the dimensions."
        ]
      },
      {
        "title": "40899688",
        "h": "tf.transpose",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.44",
        "t_prob": "0.94",
        "r_prob": "0.88",
        "prob": "0.36396799999999996",
        "sentences": [
          "tf.reshape ( a , shape =[- 1 ]) will \" unroll \" tensor a into vector using row - major order.",
          "if you want different order , you could tf.transpose first "
        ]
      },
      {
        "title": "57721028",
        "h": "tf.transpose",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.37",
        "t_prob": "0.6",
        "r_prob": "0.98",
        "prob": "0.21756",
        "sentences": [
          "you can make use of tf.transpose to shift your axis from nhwc to nchw ",
          "you may even make use of tf.reshape "
        ]
      }
    ]
  },
  "tf.matmul": {
    "tf.add": [
      {
        "title": "36520509",
        "h": "tf.add",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "the most basic way to write a linear activation in tensorflow is using tf.matmul () and tf.add () ( or the + operator ). "
        ]
      },
      {
        "title": "35711410",
        "h": "tf.matmul",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9023999999999999",
        "sentences": [
          "see the tensorflow api for tf.nn.softmax.",
          "the only thing that is equal between the two statements is the basic calculation.",
          "+ does the same thing tf.add does so tf.add ( tf.matmul ( x , weights ), biases ) is equal to tf.matmul ( x , weights ) + biases."
        ]
      },
      {
        "title": "51088806",
        "h": "tf.matmul",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.8840699999999999",
        "sentences": [
          "each time when you call apply_feature_extraction you put a new operation tf.add ( tf.matmul (...) to your graph.",
          "as a result your graph gets bloated."
        ]
      },
      {
        "title": "51245924",
        "h": "tf.add",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.96",
        "r_prob": "0.96",
        "prob": "0.857088",
        "sentences": [
          "the function add takes 2 arguments.",
          "in the line tf.add ( tf.matmul ( data , hidden_layer_1 [' weights ']) + hidden_layer_1 [' biases ']) you are trying to use the function add and using the + too.",
          "either do tf.add ( tf.matmul ( data , hidden_layer_1 [' weights ']), hidden_layer_1 [' biases ']) or tf.matmul ( data , hidden_layer_1 [' weights ']) + hidden_layer_1 [' biases ']. "
        ]
      },
      {
        "title": "48562852",
        "h": "tf.add",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.818235",
        "sentences": [
          "each layer should be : layer = tf.add ( tf.matmul ( x , weights [ h ]), biases [ h ). ",
          "you ' re also completely missing tf.add () on your output layer , and are trying to run a matrix multiplication with three inputs , one of which are your biases."
        ]
      },
      {
        "title": "54492280",
        "h": "tf.matmul",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.6365",
        "sentences": [
          "there is absolutely no difference between using tf.layers and defining your own layers by creating w and b matricies and then doing tf.matmul and tf.add."
        ]
      },
      {
        "title": "44142142",
        "h": "tf.matmul",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.89",
        "r_prob": "0.54",
        "prob": "0.418122",
        "sentences": [
          "the + operator in tf.matmul ( x , w ) + b is actually shorthand for tf.add ( tf.matmul ( x , w ), b ) ( operator overloading ). ",
          "the documentation for tf.add mentions that it supports broadcasting , which means that when you add a tensor with shape ( 10 ) to a tensor with shape ( 100 , 10 ), it ' s the equivalent of adding the ( 10 ) tensor to each row of the ( 100 , 10 ) tensor."
        ]
      }
    ],
    "tf.multiply": [
      {
        "title": "40672159",
        "h": "tf.matmul",
        "t": "tf.multiply",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.9216899999999999",
        "sentences": [
          "in addition to tf.reduce_sum ( tf.multiply ( x , y )), you can also do tf.matmul ( x , tf.reshape ( y , [- 1 , 1 ])). "
        ]
      },
      {
        "title": "47647175",
        "h": "tf.multiply",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.88",
        "r_prob": "1.0",
        "prob": "0.8096",
        "sentences": [
          "you are trying to do matrix multiplication.",
          "so you should make use of tf.matmul.",
          "the operation of tf.multiply does elementwise multiplication for which both the tensor ' s shapes must be same."
        ]
      },
      {
        "title": "34908326",
        "h": "tf.matmul",
        "t": "tf.multiply",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.86",
        "r_prob": "1.0",
        "prob": "0.8083999999999999",
        "sentences": [
          "the tf.matmul () op requires that both of its inputs are matrices ( i.e.",
          "a 1 - d tensor ), which is the source of the error.",
          "by contrast , the * operator ( an alias for tf.multiply ()) is a broadcasting element - wise multiplication.",
          "* this was true when i posted the answer at first , but now tf.matmul () also supports batched matrix multiplications.",
          "see the documentation for more details."
        ]
      },
      {
        "title": "47583685",
        "h": "tf.matmul",
        "t": "tf.multiply",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.79002",
        "sentences": [
          "what tf.multiply ( x , x ) does is essentially multiplying each element of the matrix with itself , like ",
          "just put it down with variable names [[ a b ] [ c d ]] instead of actual numbers and look at what does tf.matmul ( x , x ) and tf.multiply ( x , x ) do."
        ]
      },
      {
        "title": "55401848",
        "h": "tf.multiply",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.83",
        "r_prob": "0.99",
        "prob": "0.65736",
        "sentences": [
          "the tf.matmul operator performs a matrix multiplication , which means that each element in the resulting matrix is a sum of products ( which corresponds exactly to what you describe ). ",
          "x = [ 2 , 3 , 1 ] y = [ 3 , 1 , 2 ] ",
          "then the result would be : ",
          "tf.matmul ( x , y ) = 2 * 3 + 3 * 1 + 1 * 2 = 11 ",
          "there you can see the weighted sum.",
          "p.s : tf.multiply performs element - wise multiplication , which is not what we want here."
        ]
      },
      {
        "title": "46599680",
        "h": "tf.multiply",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.66",
        "r_prob": "0.99",
        "prob": "0.6011280000000001",
        "sentences": [
          "please use tf.matmul instead of tf.multiply in your pred equation.",
          "tf.multiply does a element wise multiplication hence , it will generate a matrix of same dimension as train_x , whereas tf.matmul will do a matrix multiplication and will generate the resultant matrix based on the actual matrix multiplication rule."
        ]
      },
      {
        "title": "47556275",
        "h": "tf.multiply",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.55",
        "r_prob": "0.97",
        "prob": "0.5174949999999999",
        "sentences": [
          "but , hope the following helps : ",
          "the * or tf.multiply does element - wise multiplication.",
          "use tf.matmul "
        ]
      },
      {
        "title": "47510518",
        "h": "tf.multiply",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.62",
        "t_prob": "0.75",
        "r_prob": "1.0",
        "prob": "0.46499999999999997",
        "sentences": [
          "and you did not include the squared error in the second sum.",
          "using * and tf.multiply is the same.",
          "( for matrix multiplication , you ought to use tf.matmul ) "
        ]
      },
      {
        "title": "43633250",
        "h": "tf.multiply",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.63",
        "r_prob": "0.7",
        "prob": "0.33957",
        "sentences": [
          "did you mean to use tf.matmul instead of the combination of tf.reduce_sum and tf.multiply ? "
        ]
      }
    ],
    "tf.nn.softmax": [
      {
        "title": "39383544",
        "h": "tf.nn.softmax",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.94",
        "r_prob": "0.99",
        "prob": "0.921294",
        "sentences": [
          "seems like your softmax function is applied to every distinct value in the output vector.",
          "try to transpose your output , i.e.",
          "change tf.nn.softmax ( tf.matmul ( x , w ) + b )) to tf.nn.softmax ( tf.transpose ( tf.matmul ( x , w ) + b ))). "
        ]
      },
      {
        "title": "33717109",
        "h": "tf.nn.softmax",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.94",
        "r_prob": "0.99",
        "prob": "0.921294",
        "sentences": [
          "assume you went though the first tutorial and calculated the accuracy of your model ( the model is this : y = tf.nn.softmax ( tf.matmul ( x , w ) + b )). "
        ]
      },
      {
        "title": "39828841",
        "h": "tf.nn.softmax",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.92",
        "r_prob": "0.99",
        "prob": "0.901692",
        "sentences": [
          "when you define your loss you are passing the operation stored in cross_entropy , which depends on y_ and y.y_ is a placeholder for your input whereas y is the result of y = tf.nn.softmax ( tf.matmul ( x , w ) + b ). "
        ]
      },
      {
        "title": "50080463",
        "h": "tf.nn.softmax",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.92",
        "r_prob": "0.99",
        "prob": "0.901692",
        "sentences": [
          "you need to use - y_ = tf.nn.softmax ( tf.matmul ( x , w )+ b ) ",
          "instead of : ",
          "y_ = tf.nn.sigmoid ( tf.matmul ( x , w )+ b ) "
        ]
      },
      {
        "title": "56149843",
        "h": "tf.nn.softmax",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.93",
        "r_prob": "0.97",
        "prob": "0.8930790000000001",
        "sentences": [
          "by definition , i mean something , like , ",
          "y = tf.nn.softmax ( tf.matmul ( x , w ) + b , name =' y '). "
        ]
      },
      {
        "title": "35711410",
        "h": "tf.nn.softmax",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.8929999999999999",
        "sentences": [
          "since softmax1 has no softmax calculation at all while softmax2 does.",
          "see the tensorflow api for tf.nn.softmax.",
          "+ does the same thing tf.add does so tf.add ( tf.matmul ( x , weights ), biases ) is equal to tf.matmul ( x , weights ) + biases."
        ]
      },
      {
        "title": "46382856",
        "h": "tf.nn.softmax",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.8811",
        "sentences": [
          "change model = tf.nn.softmax ( tf.matmul ( k , w )+ b ) to model = tf.nn.softmax ( tf.reshape ( tf.matmul ( k , w )+ b , [- 1 ])) ",
          "the output of tf.matmul ( k , w )+ b is a 2d array.",
          "in your case , [ 4 , 1 ]. ",
          "but the reduce_sum in tf.nn.softmax () is by default applied to the last axis.",
          "tf.reshape change the size of tf.matmul ( k , w )+ b to [ 4 ]. "
        ]
      },
      {
        "title": "43633250",
        "h": "tf.nn.softmax",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.63",
        "r_prob": "1.0",
        "prob": "0.6174",
        "sentences": [
          "the problem arises because you call tf.reduce_sum on the argument of tf.nn.softmax.",
          "as a result , the softmax function fails because a scalar is not a valid input argument.",
          "did you mean to use tf.matmul instead of the combination of tf.reduce_sum and tf.multiply ? "
        ]
      },
      {
        "title": "35279095",
        "h": "tf.nn.softmax",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.57",
        "r_prob": "1.0",
        "prob": "0.5642999999999999",
        "sentences": [
          "the tf.nn.softmax () operator converts the logits computed by tf.matmul ( x , w ) + b into a probability distribution across the different output classes , which is then compared to the fed - in value for y_.",
          "if nlabels = 1 , this acts as if there were only a single class , and the tf.nn.softmax () op would compute a probability of 1.0 for that class , leading to a cross - entropy of 0.0 , since tf.log ( 1.0 ) is 0.0 for all of the examples."
        ]
      }
    ],
    "tf.placeholder": [
      {
        "title": "42309513",
        "h": "tf.matmul",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.93",
        "r_prob": "0.99",
        "prob": "0.902286",
        "sentences": [
          "the main advantage of constraining the shape of a tf.placeholder () is that the ( possibly partial ) shape can be used to infer the shapes of tensors that are derived from it.",
          "... you would get a runtime error from the matmul kernel.",
          "by contrast , if you constrained the shapes in the tf.placeholder () calls as follows : ",
          "... you ' d get an error when you call tf.matmul (). ",
          "shape inference can also improve the performance at runtime."
        ]
      },
      {
        "title": "42537878",
        "h": "tf.placeholder",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.84",
        "r_prob": "0.99",
        "prob": "0.66528",
        "sentences": [
          "to fix this problem , use a different python variable name for the placeholder y and result of tf.matmul ( x , w ) + b."
        ]
      },
      {
        "title": "44735282",
        "h": "tf.matmul",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.69",
        "t_prob": "0.89",
        "r_prob": "0.99",
        "prob": "0.607959",
        "sentences": [
          "to be more detailed , in tensorflow , all the tensors or operations you define ( e.g.",
          "tf.placeholder or tf.matmul ) are defined in the tf.graph () youre working on.",
          "you might want to store them in python variable , as you did by doingx = tf.placeholder ` but that ' s not mandatory."
        ]
      }
    ],
    "tf.reshape": [
      {
        "title": "47969359",
        "h": "tf.reshape",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.97",
        "r_prob": "0.92",
        "prob": "0.8834759999999999",
        "sentences": [
          "this essentially casts a matrix of , say , rank 3 to one with rank 2 by \" stacking the matrices \" one on top of the other.",
          "you can use this : tf.reshape ( tf.matmul ( tf.reshape ( aijk ,[ i * j , k ]), bkl ),[ i , j , l ]) "
        ]
      },
      {
        "title": "46382856",
        "h": "tf.matmul",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.97",
        "r_prob": "0.94",
        "prob": "0.811502",
        "sentences": [
          "change model = tf.nn.softmax ( tf.matmul ( k , w )+ b ) to model = tf.nn.softmax ( tf.reshape ( tf.matmul ( k , w )+ b , [- 1 ])) ",
          "the output of tf.matmul ( k , w )+ b is a 2d array.",
          "tf.reshape change the size of tf.matmul ( k , w )+ b to [ 4 ]. "
        ]
      },
      {
        "title": "38885030",
        "h": "tf.reshape",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.88",
        "r_prob": "1.0",
        "prob": "0.7303999999999999",
        "sentences": [
          "as npe mentions in their comment , the tf.matmul () op expects both of its inputs to be two - dimensional tensors , but your arguments weights and weights2 are one - dimensional tensors.",
          "tf.reshape () as follows : "
        ]
      },
      {
        "title": "50167877",
        "h": "tf.reshape",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.64",
        "t_prob": "0.81",
        "r_prob": "0.94",
        "prob": "0.48729600000000006",
        "sentences": [
          "after reshaping conv4 at line fc1 = tf.reshape ( conv4 ,[ 256 ,- 1 ]), the shape of fc1 is ( 256 , 2048 ) and the weight matrix w_fc1 has shape ( 256 , 1024 ). "
        ]
      },
      {
        "title": "40910942",
        "h": "tf.matmul",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.49",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.4704",
        "sentences": [
          "i think you want tf.squeeze or tf.reshape.",
          "you also want tf.matmul instead of tf.mul in your example if you want to do matrix multiplication instead of elementwise multiplication."
        ]
      },
      {
        "title": "48012244",
        "h": "tf.reshape",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.47",
        "r_prob": "1.0",
        "prob": "0.44179999999999997",
        "sentences": [
          "therefore , you need to know the values of these variables and of all the intermediate tensors which take part in the calculation of cost , including the gradients ( which are tensors too and are implicitly added to the computational graph ). ",
          "w.",
          "b.",
          "tf.reshape ( x , [- 1 , n ]). ",
          "tf.matmul ( ..., w ). ",
          "tf.reduce_mean (..., axis = 1 ). ",
          "tf.reduce_mean ( sq_error , name =\" cost \" ). "
        ]
      }
    ],
    "tf.constant": [
      {
        "title": "52050884",
        "h": "tf.constant",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.9",
        "r_prob": "0.99",
        "prob": "0.86427",
        "sentences": [
          "according to documentation : ",
          "calling tf.constant () creates a single operation that produces a value , adds it to the default graph .. ",
          "calling tf.matmul ( x , y ) creates a single operation that multiplies the values of tf.tensor objects x and y , adds it to the default graph , and returns a tf.tensor that represents the result of the multiplication."
        ]
      },
      {
        "title": "43082103",
        "h": "tf.constant",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.57",
        "t_prob": "0.72",
        "r_prob": "1.0",
        "prob": "0.41039999999999993",
        "sentences": [
          "for example , input1 is converted to a constant tensor within tf.matmul for your first case , and directly within tf.constant for your second case."
        ]
      },
      {
        "title": "38448221",
        "h": "tf.constant",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.6",
        "t_prob": "0.8",
        "r_prob": "0.73",
        "prob": "0.3504",
        "sentences": [
          "your target is y_ = tf.constant ([ 1.0 , 2.0 , 3.0 ], dtype = tf.float64 ) has the shape ( 1 , 3 ). ",
          "the output of tf.matmul ( x , w ) has the shape ( 3 , 1 ). "
        ]
      }
    ],
    "tf.einsum": [
      {
        "title": "50918250",
        "h": "tf.einsum",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.88",
        "r_prob": "0.98",
        "prob": "0.7761600000000001",
        "sentences": [
          "here is a workaround replacing tf.matmul with tf.einsum."
        ]
      },
      {
        "title": "45216732",
        "h": "tf.einsum",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.7743",
        "sentences": [
          "as noted here , tf.einsum is just syntactic sugar for tf.matmul and tf.multiply , if you understand how backprop works in those , you can desugar to understand backprop here."
        ]
      },
      {
        "title": "46338763",
        "h": "tf.matmul",
        "t": "tf.einsum",
        "r": "S1",
        "h_prob": "0.63",
        "t_prob": "0.87",
        "r_prob": "1.0",
        "prob": "0.5481",
        "sentences": [
          "tf.einsum gives you the ability to do exactly what you need in concise and intuitive form : ",
          "so you avoid the tf.matmul requirement to have matching dimensions for batch and other outer dimensions : "
        ]
      }
    ],
    "tf.transpose": [
      {
        "title": "39383544",
        "h": "tf.matmul",
        "t": "tf.transpose",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.82",
        "r_prob": "0.98",
        "prob": "0.7553839999999999",
        "sentences": [
          "try to transpose your output , i.e.",
          "change tf.nn.softmax ( tf.matmul ( x , w ) + b )) to tf.nn.softmax ( tf.transpose ( tf.matmul ( x , w ) + b ))). "
        ]
      },
      {
        "title": "50132258",
        "h": "tf.transpose",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.74",
        "t_prob": "0.97",
        "r_prob": "0.97",
        "prob": "0.6962659999999999",
        "sentences": [
          "a one - dimensional array , and you are trying to multiply it with a two - dimensional array w1 of shape ( 2 , 3 ), which is not possible for matrix multiplication , as number of columns of first parameter must be equal to number of rows in second parameter.",
          "this will create a two - dimensional constant tensor of shape ( 2 , 1 ). ",
          "and , then multiply it as , ",
          "a = tf.matmul ( tf.transpose ( x ), w1 ) ",
          "tf.transpose () is used to create transpose of array x with shape ( 2 , 1 ) to shape ( 1 , 2 ). "
        ]
      },
      {
        "title": "44099690",
        "h": "tf.transpose",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.6499",
        "sentences": [
          "they will be updated by optimizer during training.",
          "we can use tf.matmul ( embed , tf.transpose ( nce_weights )) + nce_biases to get final output score."
        ]
      },
      {
        "title": "41941688",
        "h": "tf.transpose",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.63",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.5922",
        "sentences": [
          "you can use tf.matmul and tf.transpose "
        ]
      }
    ],
    "tf.reduce_sum": [
      {
        "title": "48172033",
        "h": "tf.reduce_sum",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.7110000000000001",
        "sentences": [
          "using tf.matmul ( x , x , transpose_b = true ) means that you are calculating x.",
          "is the matrix multiplication.",
          "tf.reduce_sum ( _ , axis = 1 ) takes the sum along 1st axis ( starting counting with 0 ) which means you are suming the rows : "
        ]
      },
      {
        "title": "41233901",
        "h": "tf.reduce_sum",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.76",
        "t_prob": "0.83",
        "r_prob": "0.98",
        "prob": "0.6181840000000001",
        "sentences": [
          "if you have an operation that can be parallelized internally , such as matrix multiplication ( tf.matmul ()) or a reduction ( e.g.",
          "tf.reduce_sum ()), tensorflow will execute it by scheduling tasks in a thread pool with intra_op_parallelism_threads threads.",
          "this configuration option therefore controls the maximum parallel speedup for a single operation."
        ]
      },
      {
        "title": "52943237",
        "h": "tf.reduce_sum",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.72",
        "t_prob": "0.66",
        "r_prob": "0.92",
        "prob": "0.437184",
        "sentences": [
          "here ' s one way to do it using implicit broadcasting and tf.reduce_sum : ",
          "and an alternative way using tf.matmul , tf.reshape and tf.transpose : "
        ]
      }
    ]
  },
  "tf.train.clusterspec": {},
  "tf.train.export_meta_graph": {},
  "tf.contrib.learn.linearregressor": {},
  "tf.contrib.learn.linearclassifier": {},
  "tf.contrib.learn.io": {},
  "tf.estimator.inputs": {},
  "tf.config.experimental_connect_to_host": {},
  "tf.config.experimental_connect_to_cluster": {},
  "tf.keras.model": {},
  "tf.keras.sequential": {},
  "tf.make_ndarray": {},
  "tf.keras.models.load_model": {},
  "tf.keras models": {},
  "dropout regularization": {},
  "tf.shape": {
    "tf.fill": [
      {
        "title": "35855219",
        "h": "tf.fill",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "if you are trying to create a tensor with a dynamic size and the same ( constant ) value for every element , you can use tf.fill () and tf.shape () to create an appropriately - shaped tensor."
        ]
      },
      {
        "title": "49054121",
        "h": "tf.fill",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.85",
        "r_prob": "0.99",
        "prob": "0.7152749999999999",
        "sentences": [
          "edit : ",
          "actually , replacing my_get_shape with tf.shape in the previous snippet works exacly the same.",
          "it seems that tf.shape should be the default ( being careful not to cram the graph with it ) unless you explicitly want to keep dimensions undefined.",
          "this class can , among other things , check whether values in a tensor are already known , which is true for example for tf.shape when the given tensor is static or for tf.fill ( and related like tf.ones ) with known values.",
          "it seems that , for example , for tf.shape , it is able to specifically pick known values and leave the rest as undefined."
        ]
      },
      {
        "title": "38832651",
        "h": "tf.fill",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.5821999999999999",
        "sentences": [
          "there is an alternative solution that uses tf.fill () like your initial version."
        ]
      },
      {
        "title": "48623392",
        "h": "tf.shape",
        "t": "tf.fill",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.4",
        "r_prob": "0.99",
        "prob": "0.34056000000000003",
        "sentences": [
          "you can use tf.shape in order to get the tensor shape as a tensor type.",
          "num2 = tf.fill ( num1.shape , tensor_val1 ) "
        ]
      }
    ],
    "tf.reshape": [
      {
        "title": "40666375",
        "h": "tf.reshape",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9211999999999999",
        "sentences": [
          "to complete your tf.shape () call , try tensor2 = tf.reshape ( tensor , tf.tensorshape ([ num_rows * num_cols , 1 ])). ",
          "or you can directly do tensor2 = tf.reshape ( tensor , tf.tensorshape ([- 1 , 1 ])) where its first dimension can be inferred."
        ]
      },
      {
        "title": "50810041",
        "h": "tf.shape",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.86",
        "r_prob": "0.97",
        "prob": "0.734096",
        "sentences": [
          "dont tile , use broadcasting : t * tf.reshape ( params , [- 1 , 1 , 1 , tf.shape ( n_params )[ 1 ]]) "
        ]
      },
      {
        "title": "37477055",
        "h": "tf.reshape",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.93",
        "r_prob": "0.8",
        "prob": "0.6770400000000001",
        "sentences": [
          "you can do it easily with tf.reshape () without knowing the batch size.",
          "you can see it in tf.reshape (). ",
          "thanks @ kbrose.",
          "for the cases where more than 1 dimension are undefined , we can use tf.shape () with tf.reduce_prod () alternatively.",
          "tf.shape () returns a shape tensor which can be evaluated in runtime.",
          "the difference between tf.get_shape () and tf.shape () can be seen in the doc."
        ]
      }
    ],
    "tf.ones": [
      {
        "title": "33711582",
        "h": "tf.shape",
        "t": "tf.ones",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9021",
        "sentences": [
          "the way to solve your problem is to use tf.pack operation : ",
          "o = tf.ones ( shape = tf.pack ([ tf.shape ( x )[ 0 ], 1 ])) "
        ]
      },
      {
        "title": "45018288",
        "h": "tf.shape",
        "t": "tf.ones",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.91",
        "r_prob": "0.75",
        "prob": "0.6279",
        "sentences": [
          "this should work , ",
          "tf.ones ([ tf.shape ( a )[ 0 ], 1 , 1 ]) * a ",
          "also using tf.tile , we can obtain the same : ",
          "tf.tile ( tf.expand_dims ( a , 0 ), [ tf.shape ( a )[ 0 ], 1 , 1 ]) "
        ]
      },
      {
        "title": "49054121",
        "h": "tf.ones",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.85",
        "r_prob": "0.87",
        "prob": "0.525045",
        "sentences": [
          "edit : ",
          "actually , replacing my_get_shape with tf.shape in the previous snippet works exacly the same.",
          "it seems that tf.shape should be the default ( being careful not to cram the graph with it ) unless you explicitly want to keep dimensions undefined.",
          "this class can , among other things , check whether values in a tensor are already known , which is true for example for tf.shape when the given tensor is static or for tf.fill ( and related like tf.ones ) with known values.",
          "it seems that , for example , for tf.shape , it is able to specifically pick known values and leave the rest as undefined."
        ]
      }
    ],
    "tf.tensor": [
      {
        "title": "55145960",
        "h": "tf.shape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.99",
        "r_prob": "0.99",
        "prob": "0.901692",
        "sentences": [
          "however , in your case x has an unknown first dimension.",
          "in order to use the actual tensor shape as a regular tf.tensor ( with value only known at runtime ), you can use tf.shape : "
        ]
      },
      {
        "title": "40936677",
        "h": "tf.tensor",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "0.94",
        "prob": "0.8843519999999999",
        "sentences": [
          "if you want to use the dynamic shape of imgs in subsequent code , you should use the tf.shape () operator to get the shape as a tf.tensor.",
          "for example , instead of imgs.get_shape ()[ 2 ], you can use tf.shape ( imgs )[ 2 ]. "
        ]
      },
      {
        "title": "56761873",
        "h": "tf.tensor",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.92",
        "r_prob": "0.95",
        "prob": "0.8302999999999999",
        "sentences": [
          "when running in graph mode use tf.shape.",
          "tf.tensor.shape fails automatic shape inference when runnning in graph mode."
        ]
      },
      {
        "title": "38237169",
        "h": "tf.shape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.87",
        "r_prob": "0.95",
        "prob": "0.80997",
        "sentences": [
          "if x has a variable batch size , the only way to get the actual shape is to use the tf.shape () operator.",
          "this operator returns a symbolic value in a tf.tensor , so it can be used as the input to other tensorflow operations , but to get a concrete python value for the shape , you need to pass it to session.run (). "
        ]
      },
      {
        "title": "40920490",
        "h": "tf.tensor",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.97",
        "r_prob": "0.89",
        "prob": "0.759704",
        "sentences": [
          "a value of none for the number of rows in your placeholder means that it can vary at runtime , so you must use tf.shape ( x ) to get the shape as a tf.tensor."
        ]
      },
      {
        "title": "42585542",
        "h": "tf.tensor",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.59",
        "t_prob": "0.99",
        "r_prob": "0.98",
        "prob": "0.572418",
        "sentences": [
          "tensorflow is often able to simplify the dataflow graph when shapes are fully known.",
          "for example , a call to tf.shape ( x ) returns a tf.tensor containing the true dynamic shape of a tensor x.",
          "as an extreme case , the xla compiler requires that all input tensor shapes be fully defined before code is generated , so that it can generate much more efficient kernel code where array bounds ( etc .) ",
          "xla recompiles the kernel code for each combination of input shapes , so using fixed - size tensors will avoid the recompilation overhead.",
          "however , the underlying memory allocators ( the bfc allocator for gpu memory , and tcmalloc or jemalloc for cpu memory ) tend to perform better if they have a static distribution of allocation requests ( since the requests can be satisfied from buffers that were recently freed ). "
        ]
      },
      {
        "title": "48268674",
        "h": "tf.tensor",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.75",
        "r_prob": "0.8",
        "prob": "0.522",
        "sentences": [
          "matrix1 is an instance of tf.tensor , which supports two ways to access the shape : matrix1.shape attribute and matrix1.get_shape () method.",
          "the result of tf.tensor evaluation , a , is a numpy ndarray , which has just a.shape attribute.",
          "historically , tf.tensor had only get_shape () method , shape was added later to make it similar to numpy.",
          "and one more note : in tensorflow , tensor shape can be dynamic ( like in your example ), in which case neither get_shape () nor shape will return a number.",
          "in this case , one can use tf.shape function to access it in runtime ( here ' s an example when it might be useful ). "
        ]
      },
      {
        "title": "43960046",
        "h": "tf.shape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.88",
        "r_prob": "0.59",
        "prob": "0.472472",
        "sentences": [
          "in tensorflow , a tensor has both a static ( inferred ) shape and a dynamic ( true ) shape.",
          "the static shape can be read using the tf.tensor.get_shape method : this shape is inferred from the operations that were used to create the tensor , and may be partially complete.",
          "if the static shape is not fully defined , the dynamic shape of a tensor t can be determined by evaluating tf.shape ( t ). ",
          "so tf.shape () returns you a tensor , will always have a size of shape =( n ,), and can be calculated in a session : "
        ]
      },
      {
        "title": "43840779",
        "h": "tf.shape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.88",
        "r_prob": "0.59",
        "prob": "0.472472",
        "sentences": [
          "in tensorflow , a tensor has both a static ( inferred ) shape and a dynamic ( true ) shape.",
          "the static shape can be read using the tf.tensor.get_shape method : this shape is inferred from the operations that were used to create the tensor , and may be partially complete.",
          "if the static shape is not fully defined , the dynamic shape of a tensor t can be determined by evaluating tf.shape ( t ). ",
          "so tf.shape () returns you a tensor , will always have a size of shape =( n ,), and can be calculated in a session : "
        ]
      },
      {
        "title": "44214625",
        "h": "tf.shape",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.74",
        "t_prob": "0.87",
        "r_prob": "0.68",
        "prob": "0.43778400000000006",
        "sentences": [
          "in tensorflow , a tensor has both a static ( inferred ) shape and a dynamic ( true ) shape.",
          "the static shape can be read using the tf.tensor.get_shape method : this shape is inferred from the operations that were used to create the tensor , and may be partially complete.",
          "if the static shape is not fully defined , the dynamic shape of a tensor t can be determined by evaluating tf.shape ( t ). ",
          "luckily the next few lines from the same faq tell you what to do : ",
          "the tf.tensor.set_shape method updates the static shape of a tensor object , and it is typically used to provide additional shape information when this cannot be inferred directly."
        ]
      }
    ],
    "batch size": [
      {
        "title": "42886208",
        "h": "batch size",
        "t": "tf.shape",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.99",
        "r_prob": "0.96",
        "prob": "0.864864",
        "sentences": [
          "tf.get_shape () and tf.shape () are slightly different.",
          "also see recomendations in tensorflow site about variable batch size."
        ]
      },
      {
        "title": "44733627",
        "h": "tf.shape",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.98",
        "t_prob": "0.9",
        "r_prob": "0.95",
        "prob": "0.8379",
        "sentences": [
          "use batch_size = tf.shape ( input )[ 0 ] to extract the batch dimension from a tensor called input , and store it in a tensor called batch_size."
        ]
      },
      {
        "title": "47925840",
        "h": "batch size",
        "t": "tf.shape",
        "r": "S2",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "0.71",
        "prob": "0.6679679999999999",
        "sentences": [
          "you can use tf.variable ( dims =[ tf.shape ( x )[ 0 ]], validate_shape = false ) to set a dynamic shape for batch size."
        ]
      },
      {
        "title": "38237169",
        "h": "batch size",
        "t": "tf.shape",
        "r": "S2",
        "h_prob": "0.62",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.6015240000000001",
        "sentences": [
          "if x has a variable batch size , the only way to get the actual shape is to use the tf.shape () operator."
        ]
      }
    ],
    "tf.tile": [
      {
        "title": "45018288",
        "h": "tf.shape",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.98",
        "r_prob": "0.86",
        "prob": "0.7753760000000001",
        "sentences": [
          "this should work , ",
          "tf.ones ([ tf.shape ( a )[ 0 ], 1 , 1 ]) * a ",
          "also using tf.tile , we can obtain the same : ",
          "tf.tile ( tf.expand_dims ( a , 0 ), [ tf.shape ( a )[ 0 ], 1 , 1 ]) "
        ]
      },
      {
        "title": "36042389",
        "h": "tf.shape",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.75",
        "r_prob": "1.0",
        "prob": "0.6900000000000001",
        "sentences": [
          "the most general solution is to use the tf.shape () op to get the run - time size of the placeholder , and the tf.tile () op to expand h to the appropriate size : "
        ]
      },
      {
        "title": "48190364",
        "h": "tf.tile",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.58",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.5627159999999999",
        "sentences": [
          "solution is to use a combination of tf.shape ( which returns the shape at runtime ) and tf.tile ( which accepts the dynamic shape ). "
        ]
      },
      {
        "title": "49550420",
        "h": "tf.tile",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.92",
        "r_prob": "0.56",
        "prob": "0.4430720000000001",
        "sentences": [
          "there is a function in tensorflow that returns dynamic shape : tf.shape.",
          "this function returns tensor that will evaluate to actual value of a shape.",
          "also there are two functions that you may find useful for your case : tf.pad and tf.tile."
        ]
      }
    ]
  },
  "tf.matrix_sol": {},
  "tf.matrix_solve_ls": {},
  "tf.contrib.data.make_saveable_from_iterator": {},
  "tf.data.iterator": {
    "tf.data.dataset": [
      {
        "title": "47396308",
        "h": "tf.data.iterator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "if you see the error message \" function ... is not defined \" when creating a tf.data.dataset or tf.data.iterator , upgrade to a newer version of tensorflow."
        ]
      },
      {
        "title": "49902781",
        "h": "tf.data.dataset",
        "t": "tf.data.iterator",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.9108",
        "sentences": [
          "you can express the entire pipeline using tf.data.dataset objects , which might make things slightly easier : ",
          "to use the values from the dataset , you can make a tf.data.iterator to get the next element as a pair of tf.tensor objects , then use these as the input to your model."
        ]
      },
      {
        "title": "50191742",
        "h": "tf.data.dataset",
        "t": "tf.data.iterator",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.855",
        "sentences": [
          "i would create 2 tf.data.dataset , one for training and one for validation subsets.",
          "once you have both datasets pipelines defined ( where you are able to define 2 different batch sizes ), you can join them in the graph by creating a single tf.data.iterator with a handler ( in my case , the tf.placeholder handle ). ",
          "edit : your current error seems to be due to trying to do a sess.run () on a tf.data.iterator.",
          "try to replace sess.run ( train_iter ) for sess.run ( train_iter.initializer ) ( and same for validation iterator ). "
        ]
      },
      {
        "title": "48127601",
        "h": "tf.data.iterator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.84",
        "prob": "0.798504",
        "sentences": [
          "in short , there is not a good way to get the size / length ; tf.data.dataset is built for pipelines of data , so has an iterator structure ( in my understanding and according to my read of the dataset ops code.",
          "from the programmer ' s guide : ",
          "a tf.data.iterator provides the main way to extract elements from a dataset.",
          "more generally though , why does this problem arise ? ",
          "if you are calling batch , you are also getting a tf.data.dataset , so whatever you are running on a batch you should be able to run on the whole dataset ; it will iterate through all the elements and calculate validation accuracy."
        ]
      }
    ]
  },
  "tf.uint": {},
  "tf.contrib.lookup.hashtable": {},
  "tf.argmax": {
    "tf.nn.softmax": [
      {
        "title": "34886873",
        "h": "tf.nn.softmax",
        "t": "tf.argmax",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9602999999999999",
        "sentences": [
          "since there is only one class for each example , the result of the tf.nn.softmax () op ( y_conv ) will be a batch_size x 1 matrix containing 1.0 in every element , and the tf.argmax () of that will containing 0 for every element , since there is only one value.",
          "similarly , applying tf.argmax () to y_train ( which is a batch_size x 1 matrix ) will yield 0 for every element , so the \" accuracy \" will be 100 %. "
        ]
      },
      {
        "title": "49943379",
        "h": "tf.nn.softmax",
        "t": "tf.argmax",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.9207000000000001",
        "sentences": [
          "it ' s happening because of correct_pred = tf.argmax ( pred , 1 ), it ' s giving you class with the highest probability after the softmax.",
          "now , you will get probability of each class for given images for example you might get [. 1 ,.05 ,.05 ,.6 ,.1 ,.1 ] for your 6 classes.",
          "also , add : ",
          "pred_ = tf.nn.softmax ( pred ) "
        ]
      },
      {
        "title": "51066076",
        "h": "tf.argmax",
        "t": "tf.nn.softmax",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.99",
        "r_prob": "0.97",
        "prob": "0.883476",
        "sentences": [
          "it has only 1 dimension , it ' s a vector ), while logits is a tensor of shape ( 100 , 1 ) ( it ' s a 2d matrix , for which one dimension happens to be of size 1 ). ",
          "trying to predict a number , not a class ), then logits contains this number , and \" classes \": tf.argmax ( input = logits , axis = 1 ) and \" probabilities \": tf.nn.softmax ( logits , name =\" softmax_tensor \") don ' t mean anything anymore , you can drop it : there is only 1 dimension in logits ( the batch dimension ). "
        ]
      }
    ],
    "tf.equal": [
      {
        "title": "47053144",
        "h": "tf.equal",
        "t": "tf.argmax",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.97",
        "r_prob": "0.98",
        "prob": "0.874552",
        "sentences": [
          "try checking the axis along which you need to find the maximum.",
          "probably it should be : ",
          "correct_pred = tf.equal ( tf.argmax ( prediction , axis = 1 ), tf.argmax ( y , axis = 1 )) "
        ]
      },
      {
        "title": "43428933",
        "h": "tf.equal",
        "t": "tf.argmax",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.97",
        "r_prob": "0.9",
        "prob": "0.81189",
        "sentences": [
          "for computation accuracy , i change ",
          "correct_pred = tf.equal ( tf.argmax ( self.score_ , 1 ), tf.argmax ( y , 1 )) ",
          "to ",
          "correct_pred = tf.equal ( tf.argmax ( self.score_ , 1 ), y )) "
        ]
      },
      {
        "title": "41863099",
        "h": "tf.argmax",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.82",
        "r_prob": "0.99",
        "prob": "0.77121",
        "sentences": [
          "evaluating tf.argmax ( pred , 1 ) gives a tensor whose evaluation will give array ([ 5 , 5 , 2 , 1 , 3 , 0 ]) ",
          "evaluating tf.argmax ( y , 1 ) gives a tensor whose evaluation will give array ([ 5 , 5 , 2 , 1 , 3 , 0 ]) ",
          "following our example , tf.equal ( tf.argmax ( pred , 1 ), tf.argmax ( y , 1 )) returns a tensor whose evaluation will givearray ( 1 , 1 , 1 , 1 , 1 , 1 ). ",
          "y_test_prediction can be obtained by executing pred = tf.argmax ( logits , 1 ) ",
          "the documentation for tf.argmax and tf.equal can be accessed by following the links below.",
          "tf.argmax () https :// www.tensorflow.org / api_docs / python / math_ops / sequence_comparison_and_indexing   argmax ",
          "tf.equal () https :// www.tensorflow.org / versions / master / api_docs / python / control_flow_ops / comparison_operators   equal "
        ]
      },
      {
        "title": "49582606",
        "h": "tf.argmax",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.91",
        "r_prob": "0.8",
        "prob": "0.6988800000000001",
        "sentences": [
          "if you are using sigmoids , your outputs will be each ( independently ) be between 0 and 1.",
          "in that case , you would remove the tf.argmax call and instead check if the preds and label are exactly the same vectors , which would look like tf.reduce_all ( tf.equal ( preds , label ), axis = 0 ). ",
          "for the latter , the code would look like tf.reduce_sum ( tf.equal ( preds , label ), axis = 0 ). "
        ]
      },
      {
        "title": "34080116",
        "h": "tf.argmax",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.84",
        "r_prob": "0.79",
        "prob": "0.650328",
        "sentences": [
          "in other words , the tensor defined with correct_prediction = tf.equal ( tf.argmax ( y , 1 ), tf.argmax ( y_ , 1 )) doesn ' t contain the list of booleans , it contains the instructions for computing it in a tensorflow graph.",
          "in order to get the actual values , you need to tell tensorflow to compute it in a graph.",
          "first , you need a tf.session variable.",
          "a simple way to get it for testing in a interactive shell is sess = tf.interactivesession (), followed by variable initialization : sess.run ( tf.initialize_all_variables ()). "
        ]
      },
      {
        "title": "51343481",
        "h": "tf.argmax",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.83",
        "r_prob": "0.85",
        "prob": "0.64906",
        "sentences": [
          "tf.equal ( tf.argmax ( prediction , 1 , name =\" argmax_pred \"), tf.argmax ( y , 1 , name =\" y_pred \"), name =\" correct_pred \"). ",
          "this means that when you try to run correct_pred in your android code , it will attempt to compute tf.equal ( tf.argmax ( prediction , 1 , name =\" argmax_pred \"), tf.argmax ( y , 1 , name =\" y_pred \"). "
        ]
      },
      {
        "title": "49614687",
        "h": "tf.argmax",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.8",
        "r_prob": "0.82",
        "prob": "0.63632",
        "sentences": [
          "replace your _ , c = sess.run ([ optimizer , cost ], ...) with _ , c , p = sess.run ([ optimizer , cost , predictions ], ...). ",
          "if you want to do it in tensorflow , move the correct prediction and accuracy computations up to where you compute cost , and change your sess.run line to : _ , c , a = sess.run ([ optimizer , cost , accuracy ], ...) "
        ]
      },
      {
        "title": "42414255",
        "h": "tf.argmax",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.71",
        "r_prob": "0.92",
        "prob": "0.62054",
        "sentences": [
          "mnist usually has one - hot coded output.",
          "in that case correct_prediction = tf.equal ( tf.argmax ( prediction , 1 ), tf.argmax ( y , 1 )) makes sense as tf.argmax converts the one - hot code to the actual class which is then compared by tf.equal."
        ]
      },
      {
        "title": "44571038",
        "h": "tf.equal",
        "t": "tf.argmax",
        "r": "S1",
        "h_prob": "0.62",
        "t_prob": "0.97",
        "r_prob": "0.78",
        "prob": "0.46909199999999995",
        "sentences": [
          "the best way i see is using a mask and tf.boolean_mask k times , with the i - th mask being given by tf.equal ( i , tf.argmax ( z , axis =- 1 )) "
        ]
      },
      {
        "title": "43025066",
        "h": "tf.equal",
        "t": "tf.argmax",
        "r": "S1",
        "h_prob": "0.44",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.41381999999999997",
        "sentences": [
          "so besically , correct_pred is just 1 number ( 0 or 1 ) because it is based on one picture ( so if tf.argmax ( model_op , 1 )= tf.argmax ( y , 1 ) then correct_pred = 1.",
          "otherwise , it equals 0 ). ",
          "the array correct_pred is just a 0 - 1 vector ( because it is a result of tf.equal ). "
        ]
      }
    ]
  },
  "tf.pad": {},
  "tf.pack": {},
  "tf.test.is_built_with_cuda": {},
  "tf.summary.histogram": {
    "tf.summary.scalar": [
      {
        "title": "54695716",
        "h": "tf.summary.histogram",
        "t": "tf.summary.scalar",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.9405",
        "sentences": [
          "generally , you just need to specify tf.summary.scalar (), tf.summary.histogram () or tf.summary.image () anywhere in the code."
        ]
      },
      {
        "title": "42967733",
        "h": "tf.summary.histogram",
        "t": "tf.summary.scalar",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.8918",
        "sentences": [
          "to perform your own logging in tensorboard , use something like tf.summary.scalar or tf.summary.histogram."
        ]
      },
      {
        "title": "49921758",
        "h": "tf.summary.histogram",
        "t": "tf.summary.scalar",
        "r": "S1",
        "h_prob": "0.72",
        "t_prob": "0.93",
        "r_prob": "0.74",
        "prob": "0.495504",
        "sentences": [
          "this gives you a scalar value.",
          "you can add it to tensorboard using tf.summary.scalar.",
          "the same as you do when using tf.summary.histogram to view values."
        ]
      }
    ]
  },
  "tf.summary.image": {},
  "tf.to_float": {},
  "tf.cast": {
    "tf.float32": [
      {
        "title": "48523652",
        "h": "tf.float32",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.96",
        "r_prob": "0.99",
        "prob": "0.836352",
        "sentences": [
          "cast labels_mask > 0 to float : tf.cast ( labels_mask > 0 , tf.float32 ). "
        ]
      },
      {
        "title": "50876919",
        "h": "tf.float32",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.98",
        "r_prob": "0.98",
        "prob": "0.835548",
        "sentences": [
          "use k.cast ( tensor , k.floatx ()).",
          "or tf.cast ( tensor , tf.float32 ). ",
          "if the model is already created , replicate it , adding the operation at the right place ( when in keras , use a lambda layer for that : lambda ( lambda x : k.cast ( x , k.floatx ()))), and transfer its weights ( newmodel.set_weights ( oldmodel.get_weights ())). "
        ]
      },
      {
        "title": "34599220",
        "h": "tf.float32",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.91",
        "r_prob": "1.0",
        "prob": "0.8281000000000001",
        "sentences": [
          "you can cast the values to floats and compute the sum on them : tf.reduce_sum ( tf.cast ( myothertensor , tf.float32 )) "
        ]
      },
      {
        "title": "59294838",
        "h": "tf.float32",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.7979999999999999",
        "sentences": [
          "the problem is in your loss function ( obviously ). ",
          "particularly , the following operation.",
          "y_pred = tf.cast ( y_pred > 0.5 , tf.float32 ) "
        ]
      },
      {
        "title": "50048350",
        "h": "tf.float32",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.97",
        "r_prob": "0.83",
        "prob": "0.7406919999999999",
        "sentences": [
          "but if you do need to cast it ( or in general combine operations in a lambda layer ), i ' d wrap that in a python lambda : k.layers.lambda ( lambda v : tf.cast ( tf.spectral.whatever ( v ), tf.float32 )) "
        ]
      },
      {
        "title": "49626923",
        "h": "tf.float32",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.9",
        "r_prob": "0.76",
        "prob": "0.60192",
        "sentences": [
          "it wrong to use tf.cast (..., tf.int32 ) and actually , there is no need to use tf.cast (..., tf.float32 ) because it ' s already been tf.float32.",
          "p.s."
        ]
      }
    ],
    "tf.int": [
      {
        "title": "51714563",
        "h": "tf.int",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.776",
        "sentences": [
          "note that the error states a valueerror regarding integers , int64 and int32 , so the conversion at question should be int not float.",
          "new_var = tf.cast ( old_var , tf.int32 ) "
        ]
      },
      {
        "title": "51813781",
        "h": "tf.cast",
        "t": "tf.int",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.79",
        "r_prob": "0.83",
        "prob": "0.583573",
        "sentences": [
          "correct = tf.nn.in_top_k ( logits , tf.cast ( y , tf.int32 ), 1 ) "
        ]
      },
      {
        "title": "45053430",
        "h": "tf.cast",
        "t": "tf.int",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.52",
        "r_prob": "0.86",
        "prob": "0.406952",
        "sentences": [
          "this issue can arise when your labels were tf.int16 before it was saved in bytes in the tfrecords.",
          "so when you read , as tf.int8 it has twice the numbers you expect.",
          "so you can make sure your labels are written properly by : label = tf.cast ( y [ i ], tf.int8 ) in your tfrecords conversion code."
        ]
      }
    ]
  },
  "tf.nn.in_top_k": {},
  "tf.keras.applicat": {},
  "tf.scatter_nd": {},
  "tf.variables": {},
  "tf.train.sequenceexample": {},
  "tf.keras.preprocessing.text.tokenizer": {},
  "tf.keras.tokeni": {},
  "tf.contrib.eager.metrics": {},
  "tf.keras.applications.mobilenetv2": {},
  "tf.scan": {
    "tf.while_loop": [
      {
        "title": "46735740",
        "h": "tf.while_loop",
        "t": "tf.scan",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.92",
        "r_prob": "0.99",
        "prob": "0.892584",
        "sentences": [
          "the model._step () method will only be called once per model object constructed.",
          "the tf.scan () function , like the tf.while_loop () function it wraps , will call their given function ( s ) only once to build a graph with a loop in it , and then the same graph will be used for each iteration of the loop."
        ]
      },
      {
        "title": "42960494",
        "h": "tf.while_loop",
        "t": "tf.scan",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.85",
        "r_prob": "0.99",
        "prob": "0.7321049999999999",
        "sentences": [
          "as a worst - case fallback , tf.scan or tf.while_loop with a tensorarray may be somewhat faster."
        ]
      },
      {
        "title": "37551496",
        "h": "tf.while_loop",
        "t": "tf.scan",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.73",
        "r_prob": "1.0",
        "prob": "0.7153999999999999",
        "sentences": [
          "tensorflow does support cyclic computation graphs.",
          "the tf.while_loop () function allows you to specify a while loop with arbitrary subgraphs for the condition and the body of the loop , and the runtime will execute the loop in parallel.",
          "the tf.scan () function is a higher - level api that is similar to theano ' s theano.scan () function."
        ]
      },
      {
        "title": "54906367",
        "h": "tf.while_loop",
        "t": "tf.scan",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.67",
        "r_prob": "1.0",
        "prob": "0.5762",
        "sentences": [
          "or tf.scan ( which is implemented using tf.while_loop and tf.tensorarray ): "
        ]
      }
    ]
  },
  "tf.nn.max_pool": {
    "tf.nn.conv2d": [
      {
        "title": "53678910",
        "h": "tf.nn.max_pool",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "the default value of padding in these versions of conv2d are as follows : ",
          "tf.nn.conv2d , tf.nn.max_pool no default value."
        ]
      },
      {
        "title": "47973905",
        "h": "tf.nn.max_pool",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.9207000000000001",
        "sentences": [
          "since convolution itself is linear , inserting any linear operation can be done by doing convolution via tf.nn.conv2d first and then that operation.",
          "the ( max ( x ) + min ( x )) / 2 on each 5x5 patch can be realized by ( tf.nn.max_pool ( x ) - tf.nn.max_pool (- x )) * 0.5."
        ]
      },
      {
        "title": "47354833",
        "h": "tf.nn.max_pool",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "0.71",
        "prob": "0.6958709999999999",
        "sentences": [
          "actually , i ' ve never seen any tensorflow function that supports nhcw format.",
          "for example , tf.nn.conv2d and tf.nn.conv2d_transpose support nhwc ( current default ) and nchw format.",
          "tf.nn.max_pool supports nhwc , nchw and nchw_vect_c ( the last one is the most performant tensor format for cudnn6 ' s quantized convolution , similar to nchw ). "
        ]
      }
    ]
  },
  "tf.float32": {
    "tf.placeholder": [
      {
        "title": "46892382",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.912285",
        "sentences": [
          "your placeholder : 0 is tfkids = tf.placeholder ( tf.float32 , [ pop_size , dna_size ]). ",
          "your kids variable , instead , has shape = ( 10 ). "
        ]
      },
      {
        "title": "35382604",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "0.95",
        "prob": "0.9030699999999999",
        "sentences": [
          "it sounds like you have defined input_y — which i am assuming is a tf.placeholder ()— as having type tf.int32.",
          "either change this to tf.float32 or add a cast : tf.cast ( input_y , tf.float32 ) or tf.to_float ( input_y ). "
        ]
      },
      {
        "title": "54201739",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.9025",
        "sentences": [
          "from your code : ",
          "x_state = tf.placeholder ( tf.float32 , shape =[ none , input_height , input_width , input_channels ]) ",
          "if you want to feed arrays in nchw format , change x_state to tf.placeholder ( tf.float32 , shape =[ none , input_channels , input_height , input_width ]) "
        ]
      },
      {
        "title": "42970364",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.902286",
        "sentences": [
          "load your test image from disk into a numpy array , vectorize and reshape it to be of size [ 1 , 784 ] because this is the shape of your input placeholder defined here : x = tf.placeholder ( tf.float32 , shape =[ none , 784 ]). "
        ]
      },
      {
        "title": "46566863",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "0.9",
        "prob": "0.87318",
        "sentences": [
          "in your first code snippet , you basically define y = tf.placeholder ( tf.float32 ) + tf.variable ([ 1.0 , 1.0 , 1.0 ], tf.float32 ). ",
          "in your second example , you define y = tf.placeholder ( tf.float32 ) + tf.assign ( tf.variable ([ 1.0 , 1.0 , 1.0 ], tf.float32 ), [ 4.0 , 4.0 , 4.0 ]). ",
          "so , no matter which value you assign to c , the computation graph contains the assign operation and will always assign [ 4.0 , 4.0 , 4.0 ] to it before computing the sum."
        ]
      },
      {
        "title": "43817958",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.8648",
        "sentences": [
          "please carefully check the datatype you feed \" x_train / y_train \" and the tensor \" x / y_label \" you defined by ' tf.placeholder (...)' ",
          "and the reason is x_train in my code is \" np.float64 \", but what i defined by tf.placeholder () is tf.float32."
        ]
      },
      {
        "title": "43606151",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.96",
        "r_prob": "0.96",
        "prob": "0.857088",
        "sentences": [
          "one probable reason could be the lack of numpy like versatility in tensorflow yet.",
          "define a placeholder in your graph , ",
          "derivative = tf.placeholder ( tf.float32 ,[ none , num_features ]) "
        ]
      },
      {
        "title": "42933116",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.96",
        "r_prob": "0.98",
        "prob": "0.856128",
        "sentences": [
          "you ' ve correctly observed the problem.",
          "the keep_prob = tf.placeholder ( tf.float32 ) tensor is unconnected to the graph that you import with tf.train.import_meta_graph (), so feeding that tensor has no effect on the inference."
        ]
      },
      {
        "title": "44276422",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.96",
        "r_prob": "0.99",
        "prob": "0.85536",
        "sentences": [
          "i.e.",
          "i created keep_prob = tf.placeholder ( tf.float32 ,) in the training file.",
          "i created another keep_prob = tf.placeholder ( tf.float32 ,) in testing file , thinking it would be the same , but it was not.",
          "i modified my code in the training file by adding the label : keep_prob = tf.placeholder ( tf.float32 , name =\" keep_prob \") "
        ]
      },
      {
        "title": "50498669",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.98",
        "r_prob": "0.94",
        "prob": "0.838292",
        "sentences": [
          "in a tensorflow model you can define a placeholder such as x = tf.placeholder ( tf.float32 ), then you will use x in your model."
        ]
      },
      {
        "title": "51872436",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.95",
        "r_prob": "0.97",
        "prob": "0.81092",
        "sentences": [
          "most of objects in tensorflow can be found with a string.",
          "when you invoke tf.placeholder ( tf.float32 ), tensorflow will do the following : ",
          "create a node with the placeholder op.",
          "add this node to default graph.",
          "you can set a name for any node , say tf.placeholder ( tf.float32 , name =' myplaceholder '), if you don ' t specify a node name , tensorflow will generate one , you can use print x.op to see the name of the op."
        ]
      },
      {
        "title": "42325639",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.8066519999999999",
        "sentences": [
          "let ' s create one : ",
          "x = tf.placeholder ( tf.float32 , [ none , 784 ]), ",
          "b = tf.variable ( tf.zeros ([ 10 ])) "
        ]
      },
      {
        "title": "48038945",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.93",
        "r_prob": "0.99",
        "prob": "0.7825949999999999",
        "sentences": [
          "x = tf.placeholder ( tf.float32 , [ none , 784 ]) "
        ]
      },
      {
        "title": "40881670",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.96",
        "r_prob": "0.92",
        "prob": "0.7507199999999999",
        "sentences": [
          "in the graph , i ' d suggest to move keep_prob = tf.placeholder ( tf.float32 ) outside of the model function to make it global."
        ]
      },
      {
        "title": "48263700",
        "h": "tf.float32",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.8",
        "r_prob": "0.98",
        "prob": "0.6115200000000001",
        "sentences": [
          "so change this line of code y_ = tf.placeholder ( tf.float32 , [ none , 1 ]). ",
          "replace this 1 with number of output classes."
        ]
      }
    ],
    "tf.variable": [
      {
        "title": "46566863",
        "h": "tf.float32",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.93",
        "r_prob": "0.99",
        "prob": "0.902286",
        "sentences": [
          "in your first code snippet , you basically define y = tf.placeholder ( tf.float32 ) + tf.variable ([ 1.0 , 1.0 , 1.0 ], tf.float32 ). ",
          "in your second example , you define y = tf.placeholder ( tf.float32 ) + tf.assign ( tf.variable ([ 1.0 , 1.0 , 1.0 ], tf.float32 ), [ 4.0 , 4.0 , 4.0 ]). ",
          "so , no matter which value you assign to c , the computation graph contains the assign operation and will always assign [ 4.0 , 4.0 , 4.0 ] to it before computing the sum."
        ]
      },
      {
        "title": "44284879",
        "h": "tf.float32",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.87",
        "r_prob": "0.98",
        "prob": "0.76734",
        "sentences": [
          "you should see tensorflow as a graph , with : ",
          "w = tf.variable ([. 3 ], tf.float32 ) b = tf.variable ([-. 3 ], tf.float32 ) b = tf.variable ([-. 3 ], tf.float32 ) linear_model = w * x + b "
        ]
      },
      {
        "title": "42899188",
        "h": "tf.float32",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.7200000000000001",
        "sentences": [
          "from the documentation of tf.variable : ",
          "if e.g.",
          "you call tf.variable ([. 3 ], tf.float64 ) the resulting tensor has the same dtype as when calling tf.variable ([. 3 ], tf.float32 ). ",
          "in fact , i believe that both calls tf.variable ([. 3 ], tf.float32 ) and tf.variable ([. 3 ], tf.float64 ) are equivalent , as the second argument to tf.variable is a boolean and thus tf.floatx is being converted to a boolean which always returns true."
        ]
      },
      {
        "title": "42095734",
        "h": "tf.float32",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.65",
        "r_prob": "0.54",
        "prob": "0.23868000000000006",
        "sentences": [
          "every time model.evaluate get called , it will run the model_fn again.",
          "so if i create several variables using tf.variable and update the metric score of each evaluation batch to a variable using assign_add , then i can successfully get the accumulated metric score for all evaluation batches.",
          "and because every time model_fn get called , the variables get reset to 0 by the tf.variable ( 0 ., dtype = tf.float32 ) call , previous evaluation will not affect the result of next evaluation."
        ]
      }
    ],
    "tf.int32": [
      {
        "title": "49626923",
        "h": "tf.int32",
        "t": "tf.float32",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.88",
        "r_prob": "1.0",
        "prob": "0.8623999999999999",
        "sentences": [
          "it wrong to use tf.cast (..., tf.int32 ) and actually , there is no need to use tf.cast (..., tf.float32 ) because it ' s already been tf.float32.",
          "p.s."
        ]
      },
      {
        "title": "46654346",
        "h": "tf.float32",
        "t": "tf.int32",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.825858",
        "sentences": [
          "first of all , i would recommend quantifying the performance overhead of feeding x and y each time you initialize the iterator.",
          "for primitive types like tf.int32 and tf.float32 it is often possible to feed a value without copying any data , and in that case the overhead will be negligible."
        ]
      },
      {
        "title": "47020866",
        "h": "tf.int32",
        "t": "tf.float32",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.66",
        "r_prob": "1.0",
        "prob": "0.5148",
        "sentences": [
          "if you don ' t specify it and don ' t have any initializer , the dtype will be tf.float32 by default and the loading of tf.int32 will fail."
        ]
      }
    ],
    "tf.cast": [
      {
        "title": "48523652",
        "h": "tf.float32",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.96",
        "r_prob": "0.99",
        "prob": "0.836352",
        "sentences": [
          "cast labels_mask > 0 to float : tf.cast ( labels_mask > 0 , tf.float32 ). "
        ]
      },
      {
        "title": "50876919",
        "h": "tf.float32",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.98",
        "r_prob": "0.98",
        "prob": "0.835548",
        "sentences": [
          "use k.cast ( tensor , k.floatx ()).",
          "or tf.cast ( tensor , tf.float32 ). ",
          "if the model is already created , replicate it , adding the operation at the right place ( when in keras , use a lambda layer for that : lambda ( lambda x : k.cast ( x , k.floatx ()))), and transfer its weights ( newmodel.set_weights ( oldmodel.get_weights ())). "
        ]
      },
      {
        "title": "34599220",
        "h": "tf.float32",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.91",
        "r_prob": "1.0",
        "prob": "0.8281000000000001",
        "sentences": [
          "you can cast the values to floats and compute the sum on them : tf.reduce_sum ( tf.cast ( myothertensor , tf.float32 )) "
        ]
      },
      {
        "title": "59294838",
        "h": "tf.float32",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.7979999999999999",
        "sentences": [
          "the problem is in your loss function ( obviously ). ",
          "particularly , the following operation.",
          "y_pred = tf.cast ( y_pred > 0.5 , tf.float32 ) "
        ]
      },
      {
        "title": "50048350",
        "h": "tf.float32",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.97",
        "r_prob": "0.83",
        "prob": "0.7406919999999999",
        "sentences": [
          "but if you do need to cast it ( or in general combine operations in a lambda layer ), i ' d wrap that in a python lambda : k.layers.lambda ( lambda v : tf.cast ( tf.spectral.whatever ( v ), tf.float32 )) "
        ]
      },
      {
        "title": "49626923",
        "h": "tf.float32",
        "t": "tf.cast",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.9",
        "r_prob": "0.76",
        "prob": "0.60192",
        "sentences": [
          "it wrong to use tf.cast (..., tf.int32 ) and actually , there is no need to use tf.cast (..., tf.float32 ) because it ' s already been tf.float32.",
          "p.s."
        ]
      }
    ],
    "tf.constant": [
      {
        "title": "60066750",
        "h": "tf.float32",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.82",
        "r_prob": "0.99",
        "prob": "0.5926139999999999",
        "sentences": [
          "the problem is that i set the shape in tf.constant ( value , shape = outputs [ key ]. shape ). ",
          "i should have only used tf.constant ( value , dtype = tf.float32 ). "
        ]
      },
      {
        "title": "51996168",
        "h": "tf.float32",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.69",
        "r_prob": "1.0",
        "prob": "0.5864999999999999",
        "sentences": [
          "b1 = tf.variable ( tf.constant ( 0.1 , tf.float32 , [ k ])) "
        ]
      },
      {
        "title": "47469754",
        "h": "tf.float32",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.49",
        "r_prob": "1.0",
        "prob": "0.43119999999999997",
        "sentences": [
          "def score_converter_fn ( logits ): cr = logit_scale cr = tf.constant ([[ cr ]], tf.float32 ) print ( logit_scale ) print ( logits ) scaled_logits = tf.divide ( logits , cr , name =' scale_logits ')   change logit_scale return tf_score_converter_fn ( scaled_logits , name =' convert_scores ') score_converter_fn.__name__ = '% s_with_logit_scale ' % ( tf_score_converter_fn.__name__ ) return score_converter_fn "
        ]
      }
    ]
  },
  "tf.keras.layers.stackedrnncells": {},
  "tf.keras.layers.rnn": {},
  "batch gradient descent": {
    "stochastic gradient descent": [
      {
        "title": "46841168",
        "h": "batch gradient descent",
        "t": "stochastic gradient descent",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.66",
        "r_prob": "0.69",
        "prob": "0.35521199999999997",
        "sentences": [
          "there are mainly 3 types of gradient descent.",
          "specifically , ",
          "stochastic gradient descent.",
          "batch gradient descent.",
          "and you can set the batch_size according to your gradient descent method.",
          "for stochastic gradient descent , batch_size = 1 .. ",
          "for batch gradient descent , batch_size = training dataset size."
        ]
      },
      {
        "title": "38274597",
        "h": "batch gradient descent",
        "t": "stochastic gradient descent",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.28",
        "r_prob": "0.9",
        "prob": "0.19404000000000002",
        "sentences": [
          "training on the whole dataset at each iteration is called batch gradient descent.",
          "training on minibatches ( e.g.",
          "100 samples at a time ) is called stochastic gradient descent.",
          "batch gradient descent typically isn ' t feasible because it requires too much ram."
        ]
      },
      {
        "title": "42726174",
        "h": "batch gradient descent",
        "t": "stochastic gradient descent",
        "r": "S1",
        "h_prob": "0.74",
        "t_prob": "0.24",
        "r_prob": "1.0",
        "prob": "0.17759999999999998",
        "sentences": [
          "this is ok if your batch has size 1 ( stochastic gradient descent ), instead , since you want to do mini - batch gradient descent ( batch size > 1 ), you wanto to minimize the average error over the batch."
        ]
      }
    ]
  },
  "width": {
    "batch size": [
      {
        "title": "60723277",
        "h": "batch size",
        "t": "width",
        "r": "S2",
        "h_prob": "0.96",
        "t_prob": "0.88",
        "r_prob": "0.99",
        "prob": "0.836352",
        "sentences": [
          "elements in convolution layer represent in this order batchsize , height , width , channels ",
          "and none are placeholders for batch size."
        ]
      },
      {
        "title": "40353437",
        "h": "batch size",
        "t": "width",
        "r": "S2",
        "h_prob": "0.92",
        "t_prob": "0.9",
        "r_prob": "0.99",
        "prob": "0.8197200000000001",
        "sentences": [
          "the shapes are ( batch_size , height , width , channel ). ",
          "q1.",
          "10 is your batch size."
        ]
      },
      {
        "title": "62021250",
        "h": "batch size",
        "t": "width",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.92",
        "r_prob": "0.99",
        "prob": "0.710424",
        "sentences": [
          "( n_imgs , width , height , channels ) "
        ]
      },
      {
        "title": "56185441",
        "h": "batch size",
        "t": "width",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.91",
        "r_prob": "0.94",
        "prob": "0.7099819999999999",
        "sentences": [
          "1.channels_last ( nhwc ) - dimensions are ordered as ( batch_size , height , width , channel ). ",
          "2.channels_first ( nchw ) - dimensions are ordered as batch_size , channels , height , width ). ",
          "batch size dimension.",
          "this is possible becuase of the batch size dimension ",
          "width dimension."
        ]
      },
      {
        "title": "48510497",
        "h": "batch size",
        "t": "width",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.89",
        "r_prob": "0.88",
        "prob": "0.66572",
        "sentences": [
          "the first dimension is the batch size."
        ]
      },
      {
        "title": "44865605",
        "h": "batch size",
        "t": "width",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.58",
        "r_prob": "1.0",
        "prob": "0.5277999999999999",
        "sentences": [
          "1 x feat_nb x height x width , where height > 1 or width > 1 ), then the bn still works fine even when the batch size is equal to one."
        ]
      },
      {
        "title": "53225466",
        "h": "batch size",
        "t": "width",
        "r": "S2",
        "h_prob": "0.69",
        "t_prob": "0.77",
        "r_prob": "0.99",
        "prob": "0.525987",
        "sentences": [
          "the graph has to accept a variable batch size by setting the shape to ( none , width , heihgt , channels ). "
        ]
      },
      {
        "title": "41399856",
        "h": "batch size",
        "t": "width",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.75",
        "r_prob": "0.73",
        "prob": "0.498225",
        "sentences": [
          "in tensorflow , the in - memory representation of a tensor is row - major order ( or \" c \" ordering , because that is the representation of arrays in the c programming language ). ",
          "stands for the unknown batch size , minus 1 ): ",
          "reshape your data to match its true order (\" batch \", \" channels \", \" height \", \" width \"): "
        ]
      },
      {
        "title": "46008803",
        "h": "batch size",
        "t": "width",
        "r": "S2",
        "h_prob": "0.9",
        "t_prob": "0.49",
        "r_prob": "1.0",
        "prob": "0.441",
        "sentences": [
          "you could also do tf.reduce_mean ( x , axis =[ 1 , 2 ]), specially if your height and width are not defined."
        ]
      },
      {
        "title": "42339182",
        "h": "batch size",
        "t": "width",
        "r": "S2",
        "h_prob": "0.82",
        "t_prob": "0.48",
        "r_prob": "0.96",
        "prob": "0.3778559999999999",
        "sentences": [
          "like in your case : you have a batch size that can be anything."
        ]
      },
      {
        "title": "57573631",
        "h": "batch size",
        "t": "width",
        "r": "S2",
        "h_prob": "0.74",
        "t_prob": "0.5",
        "r_prob": "1.0",
        "prob": "0.37",
        "sentences": [
          "basically with the first array had an output of scores , with the dimensions being batch size * height * width * numkeypoints.",
          "then second array contains offsets , again batch size * height * width * offsets."
        ]
      }
    ],
    "convolutional layer": [
      {
        "title": "40706157",
        "h": "width",
        "t": "convolutional layer",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.89",
        "r_prob": "0.71",
        "prob": "0.511839",
        "sentences": [
          "in general the input for a convolutional layer is ( numsamples , numchannels , width , height ). "
        ]
      },
      {
        "title": "57401878",
        "h": "width",
        "t": "convolutional layer",
        "r": "S1",
        "h_prob": "0.72",
        "t_prob": "0.77",
        "r_prob": "0.86",
        "prob": "0.476784",
        "sentences": [
          "therefore , the input has the shape [ time , feature_size ] or if you are processing a batch [ batch_size , time , feature_size ]. ",
          "in your case , the input has a shape [ batch_size , number_of_frames , height , width , num_channels ]. ",
          "then , you use a convolutional layer , to learn spatial dependencies between the pixels in each video frame.",
          "therefore , for each video frame , the convolutional layer is going to provide you a tensor with shape [ activation_map_width , activation_map_height , number_of_filters ]. "
        ]
      },
      {
        "title": "44865605",
        "h": "convolutional layer",
        "t": "width",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.58",
        "r_prob": "0.95",
        "prob": "0.42977999999999994",
        "sentences": [
          "according to my experiments in pytorch , if convolutional layer before the bn outputs more than one value ( i.e.",
          "1 x feat_nb x height x width , where height > 1 or width > 1 ), then the bn still works fine even when the batch size is equal to one."
        ]
      }
    ]
  },
  "action": {},
  "tf.textlinereader": {},
  "tf.sparse_tensor_dense_matmul": {},
  "tf.auto_re": {},
  "tf.sigmoid_cross_entropy_with_logits": {},
  "loss function": {
    "learning rate": [
      {
        "title": "60954965",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.89",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.8277000000000001",
        "sentences": [
          "such a messy loss trajectory would usually mean that the learning rate is too high for the given smoothness of the loss function.",
          "an alternative interpretation is that the loss function is not at all predictive of the success at the given task."
        ]
      },
      {
        "title": "44949185",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.93",
        "r_prob": "0.99",
        "prob": "0.7457670000000001",
        "sentences": [
          "if your model is not converging it means that the optimizer is stuck in a local minima in your loss function.",
          "another strategy employed often is the learning rate decay , which reduces your learning rate by a factor every several epochs."
        ]
      },
      {
        "title": "47692350",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.6992",
        "sentences": [
          "note that your loss function is ",
          "l = \\ sum ( wx + b - y )^ 2 ",
          "now , your loss is diverging because learning rate is more than inverse of hessian which there will be roughly 1 /( 2 * 2900 ). "
        ]
      },
      {
        "title": "35106509",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.85",
        "r_prob": "1.0",
        "prob": "0.6885",
        "sentences": [
          "if you are getting nan values , it is probably because your learning rate is high relative to your loss function."
        ]
      },
      {
        "title": "49943047",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.7",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.651",
        "sentences": [
          "you can keep an eye on loss function and do early stopping if you think you can adjust to better learning rate."
        ]
      },
      {
        "title": "40434284",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.69",
        "t_prob": "0.92",
        "r_prob": "0.96",
        "prob": "0.6094080000000001",
        "sentences": [
          "too high of a learning rate."
        ]
      },
      {
        "title": "55838335",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.72",
        "t_prob": "0.9",
        "r_prob": "0.93",
        "prob": "0.6026400000000001",
        "sentences": [
          "i have tried to increase layers , play with the learning rate , changing the loss function , changing the optimizer , scaling the data , normalizing the data , but nothing helped me to solve this problem."
        ]
      },
      {
        "title": "52049396",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.73",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.5985999999999999",
        "sentences": [
          "your loss function looks ok , although i would just use : tf.reduce_sum ( tf.square ( heatmaps - heat_ground_truth , 2 ) , name =' heat_loss '). ",
          "then use the same loss function."
        ]
      },
      {
        "title": "46482301",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.8",
        "r_prob": "0.91",
        "prob": "0.5678400000000001",
        "sentences": [
          "one major problem with your estimator is the loss function.",
          "since you use tf.reduce_sum , the loss grows with the number of samples , which you have to compensate by using a smaller learning rate."
        ]
      },
      {
        "title": "63493601",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.8",
        "t_prob": "0.91",
        "r_prob": "0.78",
        "prob": "0.5678400000000001",
        "sentences": [
          "compile defines the loss function , the optimizer and the metrics.",
          "you need a compiled model to train ( because training uses the loss function and the optimizer ). "
        ]
      },
      {
        "title": "54031459",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.73",
        "r_prob": "0.99",
        "prob": "0.549252",
        "sentences": [
          "choose the right loss function binary crossentropy might lead your network in the direction of optimizing for all labels , now if you have an unbalanced amount of labels in your image , it might draw your network to just give back either white , gray or black image predictions.",
          "reduce learning rate if your learning rate is too high you might converge in non - sufficient optima , which also tend to optimize for gray , black or white predictions only."
        ]
      },
      {
        "title": "48893698",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.72",
        "r_prob": "0.87",
        "prob": "0.53244",
        "sentences": [
          "if you really do have a reason for trying to generate a model that creates tries to match word2vec , then looking at your loss function here are a few suggestions.",
          "if the learning rate is too small then you may not actually be able to move enough in the right direction."
        ]
      },
      {
        "title": "49924566",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.9",
        "r_prob": "0.86",
        "prob": "0.48762000000000005",
        "sentences": [
          "imagine here you are over - correcting the jumping - around and it ' s not jumping around enough to further minimize the loss function .. ",
          "when to reduce epochs "
        ]
      },
      {
        "title": "47996024",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.88",
        "r_prob": "0.64",
        "prob": "0.47872000000000003",
        "sentences": [
          "compile defines the loss function , the optimizer and the metrics.",
          "you need a compiled model to train ( because training uses the loss function and the optimizer ). ",
          "loss function.",
          "optimizer / learning rate.",
          "metrics."
        ]
      },
      {
        "title": "36577144",
        "h": "learning rate",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.69",
        "t_prob": "0.68",
        "r_prob": "1.0",
        "prob": "0.4692",
        "sentences": [
          "the extra layer made the gradients too unstable , and that lead to the loss function quickly devolving to nan.",
          "also , decreasing the learning rate may help."
        ]
      }
    ],
    "activation function": [
      {
        "title": "60575553",
        "h": "activation function",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.95",
        "r_prob": "0.98",
        "prob": "0.75411",
        "sentences": [
          "the second of all , you are using the activation ' sigmoid ', but your loss function + metric is mse.",
          "if it is indeed a regression , then the activation function should be ' linear ' at the last step."
        ]
      },
      {
        "title": "49580867",
        "h": "activation function",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.88",
        "r_prob": "0.87",
        "prob": "0.6354479999999999",
        "sentences": [
          "you can set the classifier ' s optimizer and the activation function in the hidden layers , but i don ' t think you can define a custom loss function."
        ]
      },
      {
        "title": "53847486",
        "h": "activation function",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.88",
        "t_prob": "0.89",
        "r_prob": "0.57",
        "prob": "0.446424",
        "sentences": [
          "if it is a classification task you must use ' categorical_crossentropy ' as the loss function ( instead of ' sparse_categorical_crossentropy ' which you are currently using ) and use ' softmax ' as the activation function of last layer."
        ]
      },
      {
        "title": "55634744",
        "h": "activation function",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.9",
        "r_prob": "0.51",
        "prob": "0.39015",
        "sentences": [
          "use ' sigmoid ' as the activation function of last layer .. ",
          "use ' binary_crossentropy ' as the loss function .. "
        ]
      },
      {
        "title": "45546361",
        "h": "activation function",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.67",
        "t_prob": "0.9",
        "r_prob": "0.6",
        "prob": "0.36180000000000007",
        "sentences": [
          "many autoencoder use mse as their loss function.",
          "i would suggest using tanh as your intermediate activation function."
        ]
      }
    ],
    "batch size": [
      {
        "title": "51032558",
        "h": "batch size",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.89",
        "t_prob": "0.84",
        "r_prob": "1.0",
        "prob": "0.7475999999999999",
        "sentences": [
          "i have simply put your loss function as it is.",
          "n is the batch size."
        ]
      },
      {
        "title": "57515658",
        "h": "batch size",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.92",
        "r_prob": "0.95",
        "prob": "0.55062",
        "sentences": [
          "similar to martino ' s answer , but will infer shape from input ( setting it to a fixed batch size did not work for me ). "
        ]
      },
      {
        "title": "48893698",
        "h": "batch size",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.72",
        "r_prob": "0.74",
        "prob": "0.484848",
        "sentences": [
          "if you really do have a reason for trying to generate a model that creates tries to match word2vec , then looking at your loss function here are a few suggestions.",
          "this should represent the dimension along which to compute the cosine distance and i think that the 0th dimension would be the wrong dimension ( as this likely should be the batch size."
        ]
      },
      {
        "title": "62585482",
        "h": "batch size",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.58",
        "t_prob": "0.88",
        "r_prob": "0.58",
        "prob": "0.29603199999999996",
        "sentences": [
          "the batch size has two purposes ( that i know of , there could be more ): "
        ]
      }
    ],
    "neural network": [
      {
        "title": "52902870",
        "h": "neural network",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.93",
        "t_prob": "0.75",
        "r_prob": "0.98",
        "prob": "0.68355",
        "sentences": [
          "however , probably the better way is to integrate this additional objective in the loss function during training.",
          "this way you can trade off between your additional requirement and fitting the weights of your neural network."
        ]
      },
      {
        "title": "63712487",
        "h": "neural network",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.91",
        "r_prob": "0.84",
        "prob": "0.64974",
        "sentences": [
          "in a nutshell , when you are using the option from_logits = true , you are telling the loss function that your neural network output is not normalized."
        ]
      },
      {
        "title": "60098915",
        "h": "neural network",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.74",
        "t_prob": "0.88",
        "r_prob": "0.96",
        "prob": "0.6251519999999999",
        "sentences": [
          "by the way , if you are taking gradients of the loss function of your neural network with respect to the parameters of you network , the time to compute the gradients will be o ( 1 ) the cost of computing the loss itself."
        ]
      },
      {
        "title": "42284733",
        "h": "neural network",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.87",
        "t_prob": "0.87",
        "r_prob": "0.76",
        "prob": "0.575244",
        "sentences": [
          "then , you have to define a loss function and a neural network.",
          "for the loss function , you can simply use the negative log - probability."
        ]
      },
      {
        "title": "55158267",
        "h": "neural network",
        "t": "loss function",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.78",
        "r_prob": "0.59",
        "prob": "0.38656799999999997",
        "sentences": [
          "a metric function is similar to a loss function , except that the results from evaluating a metric are not used when training the model."
        ]
      }
    ],
    "tf.reduce_mean": [
      {
        "title": "45287445",
        "h": "tf.reduce_mean",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.94",
        "t_prob": "0.83",
        "r_prob": "0.87",
        "prob": "0.6787739999999999",
        "sentences": [
          "add regularization to your loss function for all the weights ej : ",
          "cross_entropy = tf.reduce_mean ( tf.nn.softmax_cross_entropy_with_logits ( labels = y_true , logits = y_conv )+ beta * tf.nn.l2_loss ( w_conv1 ) + beta * tf.nn.l2_loss ( w_conv2 ) + beta * tf.nn.l2_loss ( w_fc1 )+ beta * tf.nn.l2_loss ( w_fc2 )) "
        ]
      },
      {
        "title": "45160944",
        "h": "tf.reduce_mean",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.73",
        "t_prob": "0.9",
        "r_prob": "0.96",
        "prob": "0.6307200000000001",
        "sentences": [
          "the answer to this question depends on your loss function.",
          "if loss_element is your loss function for one element of the batch , then , the loss of your batch will be some function of all your individual losses."
        ]
      },
      {
        "title": "41001210",
        "h": "tf.reduce_mean",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.92",
        "t_prob": "0.81",
        "r_prob": "0.72",
        "prob": "0.536544",
        "sentences": [
          "following modified code works.",
          "the main problem was loss function , which should be loss = 0.5 * tf.reduce_mean ( tf.square ( tf.transpose ( logits ) - tftrainy )) "
        ]
      }
    ],
    "indices": [
      {
        "title": "64126418",
        "h": "indices",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.6",
        "t_prob": "0.83",
        "r_prob": "0.86",
        "prob": "0.42827999999999994",
        "sentences": [
          "according to the model.fit () reference , this parameter should be a dict : optional dictionary mapping class indices ( integers ) to a weight ( float ) value , used for weighting the loss function ( during training only ). "
        ]
      },
      {
        "title": "59493502",
        "h": "indices",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.59",
        "t_prob": "0.83",
        "r_prob": "0.87",
        "prob": "0.42603899999999995",
        "sentences": [
          "class_weight : optional dictionary mapping class indices ( integers ) to a weight ( float ) value , used for weighting the loss function ( during training only ). "
        ]
      },
      {
        "title": "57936450",
        "h": "indices",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.59",
        "t_prob": "0.83",
        "r_prob": "0.87",
        "prob": "0.42603899999999995",
        "sentences": [
          "class_weight : optional dictionary mapping class indices ( integers ) to a weight ( float ) value , used for weighting the loss function ( during training only ). ",
          "one way could be to increase the loss value for classes with low samples."
        ]
      },
      {
        "title": "52972521",
        "h": "indices",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.58",
        "t_prob": "0.83",
        "r_prob": "0.78",
        "prob": "0.375492",
        "sentences": [
          "class_weight : optional dictionary mapping class indices ( integers ) to a weight ( float ) value , used for weighting the loss function ( during training only ). ",
          "sample_weight : optional numpy array of weights for the training samples , used for weighting the loss function ( during training only ). ",
          "if you have a weight for each sample , you can pass the numpy array as sample_weight to achieve the same effect without writing your own loss function."
        ]
      }
    ],
    "squar": [
      {
        "title": "51576032",
        "h": "squar",
        "t": "loss function",
        "r": "S1",
        "h_prob": "0.57",
        "t_prob": "0.91",
        "r_prob": "0.8",
        "prob": "0.41496",
        "sentences": [
          "you ' ll want to use ' mean_squared_error ' as a loss function , and track mse as a metric instead of accuracy."
        ]
      },
      {
        "title": "51094115",
        "h": "squar",
        "t": "loss function",
        "r": "S1",
        "h_prob": "0.55",
        "t_prob": "0.67",
        "r_prob": "0.84",
        "prob": "0.30954000000000004",
        "sentences": [
          "the ' mean_squared_error ' loss function is probably expecting to receive a ( batch_sz x n_labels ) matrix of labels , but you are passing a ( batch_sz x 1 x n_labels ) matrix of labels , specifically with labels.shape =( 32 , 1 , 4 ). "
        ]
      },
      {
        "title": "47061112",
        "h": "squar",
        "t": "loss function",
        "r": "S1",
        "h_prob": "0.48",
        "t_prob": "0.79",
        "r_prob": "0.71",
        "prob": "0.26923199999999997",
        "sentences": [
          "you have used mean_squared_error ( mse ) loss function.",
          "formula for mse ",
          "mse must be low for a good model.",
          "which is pretty good.",
          "in keras there is another loss function named mean_absolute_percentage_error.",
          "you can compile the model with mean_absolute_percentage_error as loss function if you want to know the percentage error of the model with train and test."
        ]
      }
    ]
  },
  "tf.sparse.sparse_dense_matmul": {},
  "tf.decode_csv": {
    "tf.read_file": [
      {
        "title": "35530400",
        "h": "tf.read_file",
        "t": "tf.decode_csv",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.38",
        "r_prob": "1.0",
        "prob": "0.3496",
        "sentences": [
          "the tf.read_file () op reads the entire contents of the given file into a single string , whereas tf.decode_csv () op expects each element of its input to be a single record ( i.e.",
          "one line ). "
        ]
      },
      {
        "title": "34401816",
        "h": "tf.decode_csv",
        "t": "tf.read_file",
        "r": "S1",
        "h_prob": "0.23",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.20700000000000002",
        "sentences": [
          "we ' ll use tf.decode_csv () to parse the text , tf.read_file () to load the jpeg data as a string , tf.image.decode_jpeg () to parse it into a dense tensor , and finally tf.train.batch () to build the parsed data into a batch of images."
        ]
      },
      {
        "title": "47373052",
        "h": "tf.read_file",
        "t": "tf.decode_csv",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.19",
        "r_prob": "1.0",
        "prob": "0.1881",
        "sentences": [
          "tf.read_file () needs a scalar input ( i.e ., just one string ), but the results of tf.decode_csv are coming back in a \" rank 1 \" context , i.e ., a 1 - d list."
        ]
      }
    ]
  },
  "tf.train.shuffle_batch_join": {},
  "tf.keras.backend.categorical_crossentropy": {},
  "tf.tensor_scatter_nd_update": {},
  "where": {},
  "clou": {},
  "multi class classification": {},
  "tf.nn.sigmoid )": {},
  "tf.image.resize_image_with_pad": {},
  "tf.make_tensor_proto": {},
  "tf.math.count_nonzero": {},
  "tf.math.unsorted_segment_mean": {},
  "tf.contrib.quantize.create_training_graph": {},
  "tf.estimator": {},
  "tf.keras.backend.tile": {},
  "tf.keras.backend rou": {},
  "tf.metrics are": {},
  "tf.metrics.precision": {},
  "tf.contrib.learn.dnnregressor": {},
  "tf.python.keras": {},
  "tf.keras.metrics.binarycrossentropy": {},
  "integer": {},
  "dim": {
    "batch size": [
      {
        "title": "60531920",
        "h": "batch size",
        "t": "dim",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.65",
        "r_prob": "1.0",
        "prob": "0.5525",
        "sentences": [
          "notice that it ' s very important that you understand where your batch size is , and that a layer cannot have weights with sizes based on the batch size ( unless you define your inputs with batch_shape or batch_input_shape instead of shape -- this will force you to use a fixed batch size in the model ). "
        ]
      },
      {
        "title": "48141688",
        "h": "batch size",
        "t": "dim",
        "r": "S2",
        "h_prob": "0.88",
        "t_prob": "0.59",
        "r_prob": "0.91",
        "prob": "0.472472",
        "sentences": [
          "the difference is in convention that input_shape does not contain the batch size , while batch_input_shape is the full input shape including the batch size."
        ]
      },
      {
        "title": "53102892",
        "h": "batch size",
        "t": "dim",
        "r": "S2",
        "h_prob": "0.65",
        "t_prob": "0.67",
        "r_prob": "1.0",
        "prob": "0.43550000000000005",
        "sentences": [
          "from the keras documentation , the input shape should be ( batch_size , timesteps , input_dim ). "
        ]
      }
    ]
  },
  "tf.image.flip_left_right": {},
  "tf.image.flip_up_down": {},
  "tf.nn.conv2d": {
    "tf.nn.max_pool": [
      {
        "title": "53678910",
        "h": "tf.nn.max_pool",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "the default value of padding in these versions of conv2d are as follows : ",
          "tf.nn.conv2d , tf.nn.max_pool no default value."
        ]
      },
      {
        "title": "47973905",
        "h": "tf.nn.max_pool",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.9207000000000001",
        "sentences": [
          "since convolution itself is linear , inserting any linear operation can be done by doing convolution via tf.nn.conv2d first and then that operation.",
          "the ( max ( x ) + min ( x )) / 2 on each 5x5 patch can be realized by ( tf.nn.max_pool ( x ) - tf.nn.max_pool (- x )) * 0.5."
        ]
      },
      {
        "title": "47354833",
        "h": "tf.nn.max_pool",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "0.71",
        "prob": "0.6958709999999999",
        "sentences": [
          "actually , i ' ve never seen any tensorflow function that supports nhcw format.",
          "for example , tf.nn.conv2d and tf.nn.conv2d_transpose support nhwc ( current default ) and nchw format.",
          "tf.nn.max_pool supports nhwc , nchw and nchw_vect_c ( the last one is the most performant tensor format for cudnn6 ' s quantized convolution , similar to nchw ). "
        ]
      }
    ],
    "tf.expand_dims": [
      {
        "title": "35569977",
        "h": "tf.expand_dims",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "perhaps i ' m missing a subtlety here , but it appears that you could apply a sobel filter to an image using tf.expand_dims () and tf.nn.conv2d (), as follows : "
        ]
      },
      {
        "title": "35559980",
        "h": "tf.expand_dims",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.95",
        "r_prob": "0.93",
        "prob": "0.84816",
        "sentences": [
          "the inputs () function returns a tensor of shape 180 x 180 x 3 , but tf.nn.conv2d () expects a 4 - d tensor of shape batch_size x height x width x num_channels.",
          "using image = tf.expand_dims ( image , 0 )). "
        ]
      },
      {
        "title": "57507094",
        "h": "tf.expand_dims",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.87",
        "r_prob": "1.0",
        "prob": "0.7917000000000001",
        "sentences": [
          "with ",
          "img = tf.expand_dims ( img.astype (' float32 '), 0 ) ",
          "the dimention of tf.nn.conv2d input shoul be 4 , ( batch_size , image_hight , image_with , image_channels ). ",
          "you where missing the batch_size , tf.expand_dims just add that dimention ( with a batch_size of 1 since you only have one image ). "
        ]
      }
    ],
    "tf.layers.conv2d": [
      {
        "title": "47170433",
        "h": "tf.layers.conv2d",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9503999999999999",
        "sentences": [
          "and the output is ... kernels are exactly the same.",
          "the docs , btw : tf.nn.conv2d and tf.layers.conv2d."
        ]
      },
      {
        "title": "50030458",
        "h": "tf.nn.conv2d",
        "t": "tf.layers.conv2d",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9311999999999999",
        "sentences": [
          "i ' d recommend setting it up a little bit differently.",
          "instead of using tf.layers.conv2d , i would explicitly make the weights using calls to tf.get_variable () and then use these weights with calls to tf.nn.conv2d (). ",
          "references here for variable sharing and here for tf.nn.conv2d.",
          "hopefully that helps ! "
        ]
      },
      {
        "title": "53678910",
        "h": "tf.nn.conv2d",
        "t": "tf.layers.conv2d",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "0.95",
        "prob": "0.9216899999999999",
        "sentences": [
          "the default value of padding in these versions of conv2d are as follows : ",
          "tf.nn.conv2d , tf.nn.max_pool no default value.",
          "tf.layers.conv2d , tf.layers.max_pool2d."
        ]
      },
      {
        "title": "43587561",
        "h": "tf.nn.conv2d",
        "t": "tf.layers.conv2d",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.94",
        "r_prob": "0.99",
        "prob": "0.921294",
        "sentences": [
          "tf.nn.conv2d (...) is the core , low - level convolution functionality provided by tensorflow.",
          "note , that in current tensorflow versions , parts of layers are now in core , too , e.g.",
          "tf.layers.conv2d.",
          "the difference is simply , that tf.nn.conv2d is an op , that does convolution , nothing else.",
          "tf.layers.conv2d does more , e.g.",
          "without knowing your use case : most likely you want to use tf.layers.conv2d."
        ]
      },
      {
        "title": "52533491",
        "h": "tf.nn.conv2d",
        "t": "tf.layers.conv2d",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.96",
        "prob": "0.9125759999999999",
        "sentences": [
          "for tf.layers.conv2d and tf.nn.conv2d you can pass an additional parameter called name.",
          "in all cases your weights and biases are named like for example my_conv1 / weights and my_conv1 / bias."
        ]
      },
      {
        "title": "45308609",
        "h": "tf.layers.conv2d",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.8256",
        "sentences": [
          "there is a slight difference in the parameters.",
          "for tf.nn.conv2d : ",
          "for tf.layers.conv2d : ",
          "i would use tf.nn.conv2d when loading a pretrained model ( example code : https :// github.com / ry / tensorflow - vgg16 ), and tf.layers.conv2d for a model trained from scratch."
        ]
      },
      {
        "title": "47321605",
        "h": "tf.nn.conv2d",
        "t": "tf.layers.conv2d",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.87",
        "r_prob": "0.97",
        "prob": "0.818583",
        "sentences": [
          "as others mentioned the parameters are different especially the \" filter ( s )\". ",
          "tf.nn.conv2d takes a tensor as a filter , which means you can specify the weight decay ( or maybe other properties ) like the following in cifar10 code.",
          "i ' m not quite sure how to set weight decay in tf.layers.conv2d since it only take an integer as filters.",
          "maybe using kernel_constraint ? ",
          "on the other hand , tf.layers.conv2d handles activation and bias automatically while you have to write additional codes for these if you use tf.nn.conv2d."
        ]
      },
      {
        "title": "42785422",
        "h": "tf.nn.conv2d",
        "t": "tf.layers.conv2d",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.95",
        "r_prob": "0.78",
        "prob": "0.7335900000000001",
        "sentences": [
          "take a look here : tensorflow > tf.layers.conv2d ",
          "and here : tensorflow > conv2d ",
          "as you can see the arguments to the layers version are : ",
          "tf.layers.conv2d ( inputs , filters , kernel_size , strides =( 1 , 1 ), padding =' valid ', data_format =' channels_last ', dilation_rate =( 1 , 1 ), activation = none , use_bias = true , kernel_initializer = none , bias_initializer = tf.zeros_initializer (), kernel_regularizer = none , bias_regularizer = none , activity_regularizer = none , trainable = true , name = none , reuse = none ) ",
          "and the nn version : ",
          "tf.nn.conv2d ( input , filter , strides , padding , use_cudnn_on_gpu = none , data_format = none , name = none ) "
        ]
      },
      {
        "title": "49526130",
        "h": "tf.layers.conv2d",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.6052000000000001",
        "sentences": [
          "below are two methods , one which uses a named tf.layers.conv2d and a variable scope to force kernel reuse for each channel , and another which uses tf.nn.conv2d and a declared kernel to achieve the same result ( by simply passing the same filter in each time ). "
        ]
      }
    ],
    "tf.nn.conv2d_transpose": [
      {
        "title": "47354833",
        "h": "tf.nn.conv2d_transpose",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.54",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.5346000000000001",
        "sentences": [
          "actually , i ' ve never seen any tensorflow function that supports nhcw format.",
          "for example , tf.nn.conv2d and tf.nn.conv2d_transpose support nhwc ( current default ) and nchw format."
        ]
      },
      {
        "title": "38059483",
        "h": "tf.nn.conv2d_transpose",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.53",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.5141",
        "sentences": [
          "now , the error is a little misleading --- it talks about the ' out_backprop ' argument to ' conv2dcustombackpropinput '. ",
          "the key is that tf.nn.conv2d_transpose is actually just the gradient of tf.nn.conv2d , so tensorflow uses the same code internally ( conv2dcustombackpropinput ) to compute the gradient of tf.nn.conv2d and to compute tf.nn.conv2d_transpose.",
          "since tf.nn.conv2d_transpose is the backward ( gradient ) counterpart of tf.nn.conv2d , one way to see what the correct shapes should be is to use the corresponding forward operation : ",
          "in general , try using tf.nn.conv2d to get a feel for what the relationship between the tensor shapes is.",
          "since tf.nn.conv2d_transpose is its backward counterpart , it has the same relationship between input , output and filter shapes ( but with the roles of the input and output reversed .) "
        ]
      },
      {
        "title": "48265399",
        "h": "tf.nn.conv2d_transpose",
        "t": "tf.nn.conv2d",
        "r": "S1",
        "h_prob": "0.5",
        "t_prob": "1.0",
        "r_prob": "1.0",
        "prob": "0.5",
        "sentences": [
          "use tf.nn.conv2d ( and tf.nn.conv2d_transpose correspondingly ). "
        ]
      }
    ]
  },
  "tf.boolean_mask": {},
  "tf.import_meta_graph": {},
  "gradient clipping": {},
  "learning rate decay": {
    "learning rate": [
      {
        "title": "44949185",
        "h": "learning rate decay",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.56",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.45360000000000006",
        "sentences": [
          "another strategy employed often is the learning rate decay , which reduces your learning rate by a factor every several epochs."
        ]
      },
      {
        "title": "58637810",
        "h": "learning rate decay",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.44",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.396",
        "sentences": [
          "in such case , you can either lower down the learning rate or use learning rate decay as well."
        ]
      },
      {
        "title": "51669193",
        "h": "learning rate",
        "t": "learning rate decay",
        "r": "S2",
        "h_prob": "0.77",
        "t_prob": "0.35",
        "r_prob": "1.0",
        "prob": "0.26949999999999996",
        "sentences": [
          "the sudden step down is caused by the learning rate decay happening at 40k steps ( you can find this parameter in hyper_parameters.py ). "
        ]
      },
      {
        "title": "51648360",
        "h": "learning rate",
        "t": "learning rate decay",
        "r": "S2",
        "h_prob": "0.77",
        "t_prob": "0.35",
        "r_prob": "0.99",
        "prob": "0.26680499999999996",
        "sentences": [
          "as discussed in the comments of the question , this seems to be a problem of the high learning rate decay.",
          "essentially , with every episode you multiply your learning rate by some factor j , which means that your learning rate after n episodes / epochs will be equal to lr = initial_lr * j ^ n.",
          "in your case specifically , a higher value for the decay ( i.e."
        ]
      },
      {
        "title": "64149336",
        "h": "learning rate",
        "t": "learning rate decay",
        "r": "S2",
        "h_prob": "0.61",
        "t_prob": "0.39",
        "r_prob": "0.96",
        "prob": "0.228384",
        "sentences": [
          "a similar implementation of adaptive learning rate decay ( think keras ' functions to decay learning rate on plateaus ) is right next door to the code for afo .. "
        ]
      },
      {
        "title": "43949035",
        "h": "learning rate",
        "t": "learning rate decay",
        "r": "S2",
        "h_prob": "0.77",
        "t_prob": "0.32",
        "r_prob": "0.89",
        "prob": "0.21929600000000002",
        "sentences": [
          "additionally , one needs to be careful with learning rate decay here."
        ]
      }
    ]
  },
  "tf.contrib": {},
  "tf.autograph.to_graph": {},
  "tf.unique_with_counts": {},
  "tf.segment_sum": {},
  "tf.sequen": {},
  "tf.keras.backend.resize_images": {},
  "tf.image.resize": {},
  "tf.nn.rnn_cell.dropoutwrapper": {},
  "tf.image.random_flip_left_right": {},
  "tf.diag_part": {},
  "tf.linalg.tensor_diag_part": {},
  "tf.stati": {},
  "tf.nn.depthwise_conv2d": {},
  "tf.add_to_collections": {},
  "tf.contrib.image.angles_to_projective_transforms": {},
  "tf.contrib.image.rotate": {},
  "tf.compat.v1.metrics.auc": {},
  "tf.keras.metrics.auc": {},
  "tf.contrib.learn.infer_real_valued_columns_from_input": {},
  "tf.contrib.tpu.tpuestimator": {},
  "tf.initializers.variables": {},
  "tf.estimator.dnnestimator": {},
  "tf.estimator.multihead": {},
  "tf.contrib.graph_editor": {},
  "tf.summary.merge_all": {
    "tf.summary.merge": [
      {
        "title": "46304709",
        "h": "tf.summary.merge",
        "t": "tf.summary.merge_all",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.71",
        "r_prob": "0.87",
        "prob": "0.592992",
        "sentences": [
          "i replaced tf.summary.merge_all with tf.summary.merge ([ summary_var1 , summary_var2 ]) "
        ]
      },
      {
        "title": "48946182",
        "h": "tf.summary.merge_all",
        "t": "tf.summary.merge",
        "r": "S1",
        "h_prob": "0.63",
        "t_prob": "0.86",
        "r_prob": "1.0",
        "prob": "0.5418",
        "sentences": [
          "if you have multiple summaries merged into a single tf.summary object ( e.g.",
          "made with tf.summary.merge / tf.summary.merge_all ), then you would have to filter the value field : "
        ]
      },
      {
        "title": "47723110",
        "h": "tf.summary.merge_all",
        "t": "tf.summary.merge",
        "r": "S1",
        "h_prob": "0.62",
        "t_prob": "0.9",
        "r_prob": "0.97",
        "prob": "0.5412600000000001",
        "sentences": [
          "instead of merging all summaries by merged_summary = tf.summary.merge_all (), you can merge the ops that you wanted like merged_summary_group1 = tf.summary.merge ([ op1 , op2 , ...]). "
        ]
      }
    ]
  },
  "tf.sparset": {},
  "tf.train.replica_device_setter": {
    "tf.device": [
      {
        "title": "38666008",
        "h": "tf.train.replica_device_setter",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "1.0",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.96",
        "sentences": [
          "higher - level constructs like tf.train.replica_device_setter () wrap tf.device () to specify common policies such as \" shard the variables across parameter servers , and otherwise put all ops on the worker device ,\" and we use this extensively in distributed training.",
          "in practice we have found that a small set of annotations usually yields a more efficient placement than the dynamic placer will determine , but improving the placement algorithm remains an area of active research."
        ]
      },
      {
        "title": "37733117",
        "h": "tf.device",
        "t": "tf.train.replica_device_setter",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9409",
        "sentences": [
          "if a model has long , independent computation paths , then you can split the model across multiple gpus and have each compute a part of it.",
          "start up multiple copies of the model , train them , and then synchronize their learning ( the gradients applied to their weights & biases ). ",
          "but to tl ; dr that source : in a multi - gpu setup , it ' s often best to synchronously update the model by storing the weights on the cpu ( well , in its attached dram ). ",
          "but in a multi - machine setup , we often use a separate \" parameter server \" that stores and propagates the weight updates."
        ]
      },
      {
        "title": "40979990",
        "h": "tf.train.replica_device_setter",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "0.96",
        "prob": "0.940896",
        "sentences": [
          "if you split your code across multiple devices , tensorflow will add the appropriate communication , and this extends to devices in different processes.",
          "the with tf.device ( tf.train.replica_device_setter (...)): block tells tensorflow to put each variable on a different ps task by setting its device to \"/ job : ps / task :{ i }\" ( for different values of { i }, chosen in a round - robin fashion ). "
        ]
      },
      {
        "title": "40712445",
        "h": "tf.train.replica_device_setter",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.9216899999999999",
        "sentences": [
          "the device placement for nodes happens only once.",
          "you can control the device placement with directive such as tf.device or tf.train.replica_device_setter.",
          "when you use tf.device , a device function would be pushed to a stack and the following nodes would call the device function in the stack to get a device assignment."
        ]
      },
      {
        "title": "41482827",
        "h": "tf.train.replica_device_setter",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.9025",
        "sentences": [
          "instead of achieving this by calling tf.device manually everywhere , a helper function named tf.train.replica_device_setter is used that sets a tf.variable ' s device to a parameter server , and the other operations to a worker."
        ]
      },
      {
        "title": "46064309",
        "h": "tf.train.replica_device_setter",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.97",
        "r_prob": "0.96",
        "prob": "0.88464",
        "sentences": [
          "there ' s magic happening in tf.train.replica_device_setter which takes place of the manual with tf.device annotations and has the effect of automatically assigning variables across devices."
        ]
      },
      {
        "title": "38794027",
        "h": "tf.train.replica_device_setter",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.97",
        "r_prob": "0.89",
        "prob": "0.854667",
        "sentences": [
          "this definition is outside the scope of the with tf.device ( tf.train.replica_device_setter (...)): block above , so no device is assigned to global_step.",
          "fortunately , the solution is simple.",
          "you can either define global_step inside the with tf.device ( tf.train.replica_device_setter (...)): block above , or add a small with tf.device (\"/ job : ps / task : 0 \"): block as follows : "
        ]
      }
    ]
  },
  "tf.get_collection": {
    "tf.add_to_collection": [
      {
        "title": "50068241",
        "h": "tf.add_to_collection",
        "t": "tf.get_collection",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.92",
        "r_prob": "0.93",
        "prob": "0.81282",
        "sentences": [
          "you can add the state tensor to a graph collection , which is basically a key value store to track tensors , using tf.add_to_collection and retrieve it later using tf.get_collection."
        ]
      },
      {
        "title": "43059794",
        "h": "tf.add_to_collection",
        "t": "tf.get_collection",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.78",
        "r_prob": "1.0",
        "prob": "0.7254",
        "sentences": [
          "while variables must add to collections before retrieving the collection.",
          "tf.add_to_collection (\" vars \", w1 ) tf.add_to_collection (\" vars \", b1 ) ... then all_vars = tf.get_collection (' vars ') "
        ]
      },
      {
        "title": "44655199",
        "h": "tf.add_to_collection",
        "t": "tf.get_collection",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.56",
        "r_prob": "0.95",
        "prob": "0.49476000000000003",
        "sentences": [
          "another option is to display the graph using tensorboard or jupyter notebook and the show_graph command.",
          "if you want to set it up in the future so that you can retrieve it by name , you need to add it to a collection using tf.add_to_collection : ",
          "you might also be able to get by just naming it and then specifying its scope in tf.get_collection instead , but i am not sure."
        ]
      },
      {
        "title": "43909969",
        "h": "tf.add_to_collection",
        "t": "tf.get_collection",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.48",
        "r_prob": "0.99",
        "prob": "0.42292799999999997",
        "sentences": [
          "to get back a handle on your weights , ",
          "you can use their names in the tf graph : w1 = get_variable ( name =' w1 '). ",
          "the problem is that you ' ll have to pay close attention to your name scopes , and make sure that you don ' t have multiple variables of the same name ( in which case tf adds ' _1 ' to one of their names , so you might get the wrong one ). ",
          "when building the graph , before saving it , do for instance : tf.add_to_collection (' weights ', w1 ) and tf.add_to_collection (' weights ', w2 ), and in your restoring code : [ w1 , w2 ] = tf.get_collection (' weights1 '). "
        ]
      }
    ]
  },
  "training conver": {},
  "tf.nn.dropout": {
    "tf.layers.dropout": [
      {
        "title": "47916567",
        "h": "tf.layers.dropout",
        "t": "tf.nn.dropout",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.8366",
        "sentences": [
          "well , i cannot make a comment ... ",
          "the only thing came to my mind is that tf.nn.dropout uses keep_prob while tf.layers.dropout uses dropout rate.",
          "please check if this rate is too high."
        ]
      },
      {
        "title": "49177382",
        "h": "tf.layers.dropout",
        "t": "tf.nn.dropout",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.7546",
        "sentences": [
          "there are two primary ways to perform dropout in tensorflow : ",
          "tf.nn.dropout ( low - level ). ",
          "tf.layers.dropout ( high - level , uses tf.nn.dropout under the hood ). "
        ]
      },
      {
        "title": "48085077",
        "h": "tf.layers.dropout",
        "t": "tf.nn.dropout",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7392",
        "sentences": [
          "however , for evaluation ( test ) phase they are completely different.",
          "tf.nn.dropout will still do random dropping while tf.layers.dropout won ' t drop anything ( transparent layer ). ",
          "in most cases it make sense to use tf.layers.dropout."
        ]
      },
      {
        "title": "44404530",
        "h": "tf.layers.dropout",
        "t": "tf.nn.dropout",
        "r": "S1",
        "h_prob": "0.76",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.722",
        "sentences": [
          "a quick glance through tensorflow / python / layers / core.py and tensorflow / python / ops / nn_ops.py reveals that tf.layers.dropout is a wrapper for tf.nn.dropout.",
          "the only differences in the two functions are : ",
          "the tf.nn.dropout has parameter keep_prob : \" probability that each element is kept \" tf.layers.dropout has parameter rate : \" the dropout rate \" thus , keep_prob = 1 - rate as defined here.",
          "the tf.layers.dropout has training parameter : \" whether to return the output in training mode ( apply dropout ) or in inference mode ( return the input untouched ). "
        ]
      },
      {
        "title": "48549654",
        "h": "tf.nn.dropout",
        "t": "tf.layers.dropout",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.7209000000000001",
        "sentences": [
          "apart from the answers from @ nikpod and @ salvador dali ",
          "the tf.nn.dropout scaled the weights by 1 ./ keep prob during training phase , while tf.layers.dropout scaled the weights by 1 ./( 1 - rate ). "
        ]
      },
      {
        "title": "49606249",
        "h": "tf.nn.dropout",
        "t": "tf.layers.dropout",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.73",
        "r_prob": "1.0",
        "prob": "0.7081",
        "sentences": [
          "tf.layers.dropout uses tf.nn.dropout function internally.",
          "tf.nn.dropout might be useful if you just want to use a higher level abstraction and do not want to control many facets of the dropout.",
          "2 ) https :// www.tensorflow.org / api_docs / python / tf / nn / dropout ",
          "tf.layers.dropout is a wrapper around tf.nn.dropout and there ' s a slight difference in terms that tf.layers uses \" rate of dropout \" while tf.nn \" uses the probability to keep the inputs \". ",
          "though a direct relation can be established between them.",
          "also there ' s an extra argument \" training \" in tf.layers.dropout which is used to control whether to return the output in training mode ( apply dropout ) or in inference mode ( return the input untouched ). "
        ]
      }
    ]
  },
  "tf.nn.conv_2d": {},
  "tf.config.experimental.list_physical_devices": {},
  "tf.config.list_physical_devices": {},
  "tf.contrib.layers.layer_norm": {},
  "tf.compat.v1.placeholder": {},
  "tf.comp": {},
  "tf.string_join": {},
  "tf.split": {
    "tf.concat": [
      {
        "title": "39301914",
        "h": "tf.concat",
        "t": "tf.split",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "1.0",
        "r_prob": "0.99",
        "prob": "0.9702",
        "sentences": [
          "in the mean time , you can work around it by manually adding tf.split () or tf.slice (), and tf.concat () operations to partition the tensor for transfer."
        ]
      },
      {
        "title": "57549853",
        "h": "tf.concat",
        "t": "tf.split",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.86",
        "r_prob": "1.0",
        "prob": "0.774",
        "sentences": [
          ". ",
          "if your network has a lot of memory heavy operations : tf.slice , tf.split , tf.concat , or a lot of elementwise operations ( e.g , tf.exp ( tf.exp ( a + b - c ) ), there is little that can be done by tensorrt since the fused kernel is not implemented ( cannot fuse two consecutive exp ops ) or there is little to optimize for memory operations.",
          "launching a cuda kernel would incur overhead ( say 0.1 ms ). "
        ]
      },
      {
        "title": "57452253",
        "h": "tf.concat",
        "t": "tf.split",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.76",
        "r_prob": "0.99",
        "prob": "0.6320159999999999",
        "sentences": [
          "you can use tensor manipulation such as tf.split and tf.concat."
        ]
      },
      {
        "title": "45585174",
        "h": "tf.split",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.44",
        "t_prob": "0.73",
        "r_prob": "1.0",
        "prob": "0.3212",
        "sentences": [
          "the error is due to tensorflow version , syntax of tf.split is changed in the newer version.",
          "there is another same problem with tf.concat "
        ]
      }
    ]
  },
  "tf.losses.add_loss": {},
  "tf.losses.get_total_loss": {},
  "tf.runmetadata": {},
  "tf.reduce_all": {
    "tf.equal": [
      {
        "title": "42320470",
        "h": "tf.equal",
        "t": "tf.reduce_all",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.7871999999999999",
        "sentences": [
          "the problem arises because tf.equal () is an elementwise operation and it returns a tensor with the same shape as its arguments.",
          "the easiest way to fix your expression is to use tf.reduce_all () to aggregate the results of tf.equal () down to a scalar before computing the tf.logical_and (), as follows : "
        ]
      },
      {
        "title": "49582606",
        "h": "tf.reduce_all",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.91",
        "r_prob": "0.92",
        "prob": "0.7785960000000001",
        "sentences": [
          "if you are using sigmoids , your outputs will be each ( independently ) be between 0 and 1.",
          "in that case , you would remove the tf.argmax call and instead check if the preds and label are exactly the same vectors , which would look like tf.reduce_all ( tf.equal ( preds , label ), axis = 0 ). ",
          "for the latter , the code would look like tf.reduce_sum ( tf.equal ( preds , label ), axis = 0 ). "
        ]
      },
      {
        "title": "41877446",
        "h": "tf.reduce_all",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.7209000000000001",
        "sentences": [
          "the tf.equal () operator is an elementwise operator.",
          "therefore , sess.run ( tf.equal ( y , y )) in your program will return the array [ true , true , true ]. ",
          "* note however that tf.equal ( x , y ) will broadcast its arguments if they have different shapes , so you might get the unexpected result that two tensors with different shapes are \" equal \" using this program."
        ]
      }
    ]
  },
  "tf.keras.backend.set_learning_phase": {},
  "tf.contrib.cudnn_rnn": {},
  "tf.train.write_graph": {
    "tf.train.saver": [
      {
        "title": "38950073",
        "h": "tf.train.write_graph",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.98",
        "prob": "0.931588",
        "sentences": [
          "by calling saver.save () on a tf.train.saver object ) contain only the weights , and any other variables defined in the same program.",
          "note that calling saver.save () also produces a file containing a metagraphdef , which contains a graph and details of how to associate the weights from a checkpoint with that graph.",
          "see the tutorial for more details.",
          "tf.train.write_graph () only writes the graph structure ; not the weights.",
          "a frozen graph can be loaded using tf.import_graph_def (). ",
          "in this case , the weights are ( typically ) embedded in the graph , so you don ' t need to load a separate checkpoint."
        ]
      },
      {
        "title": "40396570",
        "h": "tf.train.write_graph",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.8543999999999999",
        "sentences": [
          "a.",
          "the graphdef (*. pb ) using tf.train.write_graph : https :// www.tensorflow.org / versions / r0.11 / api_docs / cc / index.html ",
          "b.",
          "the weights (*. ckpt ) using tf.train.saver : https :// www.tensorflow.org / versions / r0.11 / api_docs / python / state_ops.html   saver "
        ]
      },
      {
        "title": "34516450",
        "h": "tf.train.saver",
        "t": "tf.train.write_graph",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.87",
        "r_prob": "0.99",
        "prob": "0.809622",
        "sentences": [
          "then you will be able to run a step by feeding a value for ( i ), and fetching the value for ( ii ). ",
          "one final concern is how to represent the model parameters in your exported graph.",
          "there are several ways to do this , including shipping a tensorflow checkpoint ( written by a tf.train.saver ) as part of your app , and running the restore ops to reload it."
        ]
      },
      {
        "title": "57490232",
        "h": "tf.train.write_graph",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.7979999999999999",
        "sentences": [
          "or is this whole approach wrong ? ",
          "a : the main problem is that tf.train.write_graph saves the tensorflow graph , but not the weights of your model.",
          "q : do i need to do use tf.train.saver ? ",
          "in addition to saving the graph ( which is only necessary if your subsequent scripts do not explicitly recreate it ), you should use tf.train.saver to save the weights of your model : ",
          "calling saver.save also saves a metagraphdef which can then be used to restore the graph , so it is not necessary for you to use tf.train.write_graph.",
          "to restore the weights , simply use saver.restore : ",
          "if you want to generate a single.pb file , please use the former tf.train.saver approach.",
          "save and restore in tensorflow .. ",
          "stackoverflow answer to tensorflow saving into / loading a graph from a file .. "
        ]
      }
    ]
  },
  "tf.einsum": {
    "tf.matmul": [
      {
        "title": "50918250",
        "h": "tf.einsum",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.88",
        "r_prob": "0.98",
        "prob": "0.7761600000000001",
        "sentences": [
          "here is a workaround replacing tf.matmul with tf.einsum."
        ]
      },
      {
        "title": "45216732",
        "h": "tf.einsum",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.7743",
        "sentences": [
          "as noted here , tf.einsum is just syntactic sugar for tf.matmul and tf.multiply , if you understand how backprop works in those , you can desugar to understand backprop here."
        ]
      },
      {
        "title": "46338763",
        "h": "tf.matmul",
        "t": "tf.einsum",
        "r": "S1",
        "h_prob": "0.63",
        "t_prob": "0.87",
        "r_prob": "1.0",
        "prob": "0.5481",
        "sentences": [
          "tf.einsum gives you the ability to do exactly what you need in concise and intuitive form : ",
          "so you avoid the tf.matmul requirement to have matching dimensions for batch and other outer dimensions : "
        ]
      }
    ]
  },
  "tf.variable": {
    "tf.get_variable": [
      {
        "title": "55415113",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.941094",
        "sentences": [
          "i believe that can ' t be done.",
          "as the main aim for tf.get_variable () is to search for a variable with the same name first and if it didn ' t find it it creates a new one.",
          "so if you just want to create a new variable use tf.variable () instead "
        ]
      },
      {
        "title": "63075230",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "0.96",
        "prob": "0.9125759999999999",
        "sentences": [
          "if you define a variable with a name that has been defined before , then tensorflow throws an exception.",
          "hence , it is convenient to use the tf.get_variable () function instead of tf.variable (). "
        ]
      },
      {
        "title": "41501486",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.93",
        "r_prob": "0.98",
        "prob": "0.902286",
        "sentences": [
          "when you simply write x + 300 , you are not creating a tf.variable.",
          "you need to explicitly use tf.get_variable () or tf.variable () to create a variable which can be saved."
        ]
      },
      {
        "title": "49660916",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.96",
        "r_prob": "0.94",
        "prob": "0.8482559999999998",
        "sentences": [
          "instantiating variables ",
          "tf.variable () constructer prefixes variable name with current name_scope and variable_scope ",
          "tf.get_variable () constructor ignores name_scope and only prefixes name with the current variable_scope "
        ]
      },
      {
        "title": "39104281",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.98",
        "r_prob": "0.96",
        "prob": "0.837312",
        "sentences": [
          "the problem is that you should be using tf.get_variable () to create your variables , instead of tf.variable (), if you are reusing a scope."
        ]
      },
      {
        "title": "37115316",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.88",
        "r_prob": "0.91",
        "prob": "0.776776",
        "sentences": [
          "you might want to use tf.get_variable () instead of ' tf.variable `. ",
          "name_scope is originally used for managing operation names ( such as add , matmul ), because tf.variable is actually an operation and its operation name will be \" inherited \" by variables created by it , so the name of name_scope rather than variable_scope is used as prefix.",
          "but if you want to use tf.variable , you can also directly use name_scope in with statement : ",
          "if name has been used before , it will be made unique by calling self.unique_name ( name ). "
        ]
      },
      {
        "title": "52535494",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.88",
        "r_prob": "0.96",
        "prob": "0.768768",
        "sentences": [
          "this is because tf.variable is a low level method which stores created variable in globals ( or locals ) collection while tf.get_variable keeps account of the variable it has created by storing them in a variable store.",
          "when you first call tf.variable , the variable created is not added to the variable store letting think that no variable with name \" test \" has been created.",
          "so , when you later call tf.get_variable (\" test \") it will look at the variable store , see that no variable with name \" test \" is in it.",
          "it will thus call tf.variable , which will create a variable with an incremented name \" test_1 \" stored in the variable store under the key \" test \". "
        ]
      },
      {
        "title": "37102908",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.8",
        "r_prob": "0.95",
        "prob": "0.7372",
        "sentences": [
          "i ' d recommend to always use tf.get_variable (...) -- it will make it way easier to refactor your code if you need to share variables at any time , e.g.",
          "in a multi - gpu setting ( see the multi - gpu cifar example ). ",
          "there is no downside to it.",
          "pure tf.variable is lower - level ; at some point tf.get_variable () did not exist so some code still uses the low - level way."
        ]
      },
      {
        "title": "41525110",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.89",
        "r_prob": "0.91",
        "prob": "0.704613",
        "sentences": [
          "call tf.initialize_all_variables ()) after you declared all variables.",
          "creating a variable via tf.get_variable or tf.variable places it in global_variables collection ( unless otherwise specified with collections kwarg ). "
        ]
      },
      {
        "title": "49672443",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.87",
        "r_prob": "0.94",
        "prob": "0.6951299999999999",
        "sentences": [
          "the use of variables in tensorflow can be sometimes confusing.",
          "when you do my_var = tf.variable (...) or my_var = tf.get_variable (...), my_var is a tf.variable object than can hold values for the duration of a session.",
          "to change the value , we could do my_var = tf.assign ( my_var / 2 ). "
        ]
      },
      {
        "title": "51238155",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.84",
        "r_prob": "0.97",
        "prob": "0.635544",
        "sentences": [
          "oh , you should never use tf.variable unless you have a very good reason.",
          "you should use tf.get_variable instead to avoid issues."
        ]
      },
      {
        "title": "46774528",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.91",
        "r_prob": "0.72",
        "prob": "0.635544",
        "sentences": [
          "tf.variable accepts an initial value upon creation ( a constant ), this explains deterministic results when you use it.",
          "tf.get_variable is slightly different : it has an initializer argument , by default none , which is interpreted like this : "
        ]
      },
      {
        "title": "50975398",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.81",
        "r_prob": "0.81",
        "prob": "0.5708070000000001",
        "sentences": [
          "however that won ' t solve your problem.",
          "in the case of variables , calling tf.variable will always create a new variable , whereas calling tf.get_variable will reuse it if it already exists."
        ]
      },
      {
        "title": "43730032",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.64",
        "t_prob": "0.93",
        "r_prob": "0.92",
        "prob": "0.5475840000000001",
        "sentences": [
          "you can create non - trainable variables in two different ways : ",
          "tf.variable ( a , trainable = false ). ",
          "tf.get_variable (\" a \", a , trainable = false ). "
        ]
      },
      {
        "title": "44711222",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.72",
        "t_prob": "0.59",
        "r_prob": "0.99",
        "prob": "0.4205519999999999",
        "sentences": [
          "the first parameter of tf.variable is the initial value of the variable , so in the upper statement x is a variable with value [ len ( _element_list ), 4 ], and it ' s rank of shape is 1.",
          "the second parameter of tf.get_variable is the shape of variable , so the rank of shape of variable x is 2."
        ]
      },
      {
        "title": "44076283",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.62",
        "r_prob": "0.87",
        "prob": "0.36139800000000005",
        "sentences": [
          "first is that tf.variable will always create a new variable , whereas tf.get_variable gets an existing variable with specified parameters from the graph , and if it doesn ' t exist , creates a new one.",
          "tf.variable requires that an initial value be specified."
        ]
      },
      {
        "title": "37534656",
        "h": "tf.get_variable",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.53",
        "t_prob": "0.75",
        "r_prob": "0.82",
        "prob": "0.32595",
        "sentences": [
          "it is a mechanism in tensorflow that allows for sharing variables accessed in different parts of the code without passing references to the variable around.",
          "the method tf.get_variable can be used with the name of the variable as the argument to either create a new variable with such name or retrieve the one that was created before.",
          "this is different from using the tf.variable constructor which will create a new variable every time it is called ( and potentially add a suffix to the variable name if a variable with such name already exists ). ",
          "variable scope , created using tf.variable_scope.",
          "both scopes have the same effect on all operations as well as variables created using tf.variable , i.e ., the scope will be added as a prefix to the operation or variable name.",
          "the only way to place a variable accessed using tf.get_variable in a scope is to use a variable scope , as in the following example : "
        ]
      },
      {
        "title": "47534542",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.84",
        "r_prob": "0.53",
        "prob": "0.324996",
        "sentences": [
          "as of now , it is advised to make use of tf.get_variable and avoid tf.variable as much as possible."
        ]
      },
      {
        "title": "45867685",
        "h": "tf.variable",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.5",
        "t_prob": "0.49",
        "r_prob": "0.79",
        "prob": "0.19355",
        "sentences": [
          "tf provides many nicely packaged ones .. ",
          "additionally in the past , tf.get_variable seems to be preferred over tf.variable when used from within control flow."
        ]
      }
    ],
    "tf.placeholder": [
      {
        "title": "44371483",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9408",
        "sentences": [
          "the tensors v1 : 0 and v2 : 0 were created from tf.placeholder () ops , whereas only tf.variable objects are added to the \" variables \" ( or \" trainable_variables \") collections.",
          "there is no general collection to which tf.placeholder () ops are added , so your options are : ",
          "add the tf.placeholder () ops to a collection ( using tf.add_to_collection () when constructing the original graph.",
          "use [ x for x in tf.get_default_graph (). get_operations () if x.type == \" placeholderv2 \"] to get a list of placeholder ops after you import the metagraph."
        ]
      },
      {
        "title": "43536220",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.9405",
        "sentences": [
          "tf.constant () and tf.placeholder () are nodes in the graph ( ops or operations ). ",
          "on the other hand tf.variable () is a class."
        ]
      },
      {
        "title": "40533137",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9216",
        "sentences": [
          "to change the value that your tensorflow program uses in the loop , you have two main choices : ( 1 ) using a tf.placeholder () to feed in a value , or ( 2 ) using a tf.variable to store the value between steps , and tf.variable.assign () to update it."
        ]
      },
      {
        "title": "46369727",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.94",
        "r_prob": "0.98",
        "prob": "0.8659279999999999",
        "sentences": [
          "actually , it ' s pretty straightforward.",
          "a tf.variable constructor has an argument trainable.",
          "when you specify this kind of dependency : loss = sigmod ( theta * x ), a tf.variable is created under the hood and of course it ' s trainable.",
          "a tf.placeholder only adds a node to the graph."
        ]
      },
      {
        "title": "35749825",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "0.91",
        "prob": "0.856128",
        "sentences": [
          "you have to change your placeholder is_train = tf.placeholder ( tf.int32 ) for a tf.variable : is_train = tf.variable ( true , name =' training '). "
        ]
      },
      {
        "title": "36240769",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.816",
        "sentences": [
          "unfortunately you can ' t currently use the feed / tf.placeholder () mechanism to pass the result of one tensorflow graph to another."
        ]
      },
      {
        "title": "46566863",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.99",
        "r_prob": "0.86",
        "prob": "0.791802",
        "sentences": [
          "in your first code snippet , you basically define y = tf.placeholder ( tf.float32 ) + tf.variable ([ 1.0 , 1.0 , 1.0 ], tf.float32 ). ",
          "in your second example , you define y = tf.placeholder ( tf.float32 ) + tf.assign ( tf.variable ([ 1.0 , 1.0 , 1.0 ], tf.float32 ), [ 4.0 , 4.0 , 4.0 ]). ",
          "so , no matter which value you assign to c , the computation graph contains the assign operation and will always assign [ 4.0 , 4.0 , 4.0 ] to it before computing the sum."
        ]
      },
      {
        "title": "40649185",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.92",
        "r_prob": "0.99",
        "prob": "0.7377480000000001",
        "sentences": [
          "subclasses of tf.train.optimizer ) operate on tf.variable objects because they need to be able to assign new values to those objects , and in tensorflow only variables support an assign operation.",
          "if you use a tf.placeholder (), there ' s nothing to update , because the value of a placeholder is immutable within each step.",
          "instead of feeding a tf.placeholder (), you could first assign a fed - in value to a variable and then optimize with respect to it : ",
          "you could use the lower - level tf.gradients () function to get the gradient of the loss with respect to a placeholder in a single step.",
          "ps.",
          "the code in your question , where you define a tf.variable ( tf.placeholder (...), ...) is just defining a variable whose initial value is fed by the placeholder."
        ]
      },
      {
        "title": "57098377",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.81",
        "r_prob": "0.99",
        "prob": "0.7056720000000001",
        "sentences": [
          "maybe you should take a look at this question : what ' s the difference between tf.placeholder and tf.variable ? "
        ]
      },
      {
        "title": "57959532",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.88",
        "r_prob": "0.81",
        "prob": "0.691416",
        "sentences": [
          "if you want to change values within tensorflow data structures it is best to either pass values to a tf.placeholder or use a tf.variable."
        ]
      },
      {
        "title": "45016895",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.84",
        "r_prob": "0.99",
        "prob": "0.640332",
        "sentences": [
          "other approach to do same is to define variables tf.variable and in this case you have to provide an initial value when you declare it.",
          "conclusion : ",
          "use tf.variable for trainable variables such as weights ( w ) and biases ( b ) for your model or when initial values are required in general.",
          "tf.placeholder allows you to create operations and build computation graph , without needing the data."
        ]
      },
      {
        "title": "42718577",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.95",
        "r_prob": "0.7",
        "prob": "0.60515",
        "sentences": [
          "in second case there ' s a feed list , first step 1 and 2 will be added , next 3 and 4 ( a and b ). ",
          "relevant reads : ",
          "tf.placeholder doc.",
          ". ",
          "tf.variable doc.",
          ". ",
          "variable vs placeholder .. "
        ]
      },
      {
        "title": "43693704",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.69",
        "r_prob": "0.98",
        "prob": "0.601818",
        "sentences": [
          "the most obvious difference between the tf.variable and the tf.placeholder is that ",
          "placeholder is a function , variable is a class ( hence an uppercase ). ",
          "when you use tf in a distributed environment , variables are stored in a special place ( parameter server ) and are shared between the workers .. "
        ]
      },
      {
        "title": "45003763",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.69",
        "r_prob": "0.98",
        "prob": "0.601818",
        "sentences": [
          "the most obvious difference between the tf.variable and the tf.placeholder is that ",
          "placeholder is a function , variable is a class ( hence an uppercase ). ",
          "when you use tf in a distributed environment , variables are stored in a special place ( parameter server ) and are shared between the workers .. "
        ]
      },
      {
        "title": "45366164",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.84",
        "r_prob": "0.82",
        "prob": "0.599256",
        "sentences": [
          "it ' s never a requirement unless you are using a declared tf.variable or tf.placeholder from within your tensorflow session run."
        ]
      },
      {
        "title": "42718405",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.74",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.5846",
        "sentences": [
          "a little background : ",
          "above the you just initialized x and y using tf.placeholder (), and b and w as tf.variable (). ",
          "is it ok to keep passing the same x and y in every time ? ",
          "for every step x and y will be different i.e."
        ]
      },
      {
        "title": "55372251",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.72",
        "r_prob": "0.99",
        "prob": "0.555984",
        "sentences": [
          "you just need to modify the code a little bit.",
          "the value of tf.variable should not be tf.placeholder , otherwise it will cause your initialization error when running sess.run ( tf.global_variables_initializer ()). "
        ]
      },
      {
        "title": "35904439",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.73",
        "r_prob": "0.88",
        "prob": "0.5267679999999999",
        "sentences": [
          "the easiest way to achieve this would be to create a tf.variable of the appropriate type and shape by initializing it from a tf.placeholder (), then use the feed mechanism to pass in the value."
        ]
      },
      {
        "title": "35688187",
        "h": "tf.placeholder",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.66",
        "r_prob": "0.76",
        "prob": "0.471504",
        "sentences": [
          "since embedding can be very large , you should only use this approach for toy examples.",
          "create w as a tf.variable and initialize it from the numpy array via a tf.placeholder (): ",
          "this avoid storing a copy of embedding in the graph , but it does require enough memory to keep two copies of the matrix in memory at once ( one for the numpy array , and one for the tf.variable ). "
        ]
      },
      {
        "title": "42991444",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.63",
        "r_prob": "0.83",
        "prob": "0.44446499999999994",
        "sentences": [
          "it ' s unclear to me why you needed to change x to a tf.variable when you are continuing to feed a value for it.",
          "there are two workarounds ( not counting the case where you could just revert x to being tf.placeholder () as in the working code ): ",
          "this latter version would allow you to perform gradient descent on the contents of the image ( which might be why you ' d want to store it in a variable in the first place ). "
        ]
      },
      {
        "title": "51694027",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.65",
        "r_prob": "1.0",
        "prob": "0.44200000000000006",
        "sentences": [
          "define a placeholder lookup_ids = tf.placeholder ([ 10 ]). ",
          "define a embedding layer embeddings = tf.variable ([ 100 , 10 ],...). "
        ]
      },
      {
        "title": "36703529",
        "h": "tf.variable",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.64",
        "t_prob": "0.76",
        "r_prob": "0.9",
        "prob": "0.43776",
        "sentences": [
          "the difference is that with tf.variable you have to provide an initial value when you declare it.",
          "with tf.placeholder you don ' t have to provide an initial value and you can specify it at run time with the feed_dict argument inside session.run "
        ]
      }
    ],
    "tf.global_variables_initializer": [
      {
        "title": "44434099",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9309999999999999",
        "sentences": [
          "a more complete description is given here.",
          "only after running tf.global_variables_initializer () in a session will your variables hold the values you told them to hold when you declare them ( tf.variable ( tf.zeros (...)), tf.variable ( tf.random_normal (...)),...). ",
          "from the tf doc : ",
          "calling tf.variable () adds several ops to the graph : "
        ]
      },
      {
        "title": "48760159",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.91",
        "r_prob": "0.95",
        "prob": "0.786695",
        "sentences": [
          "you can define xx as a tf.variable instead , giving it a default value ( which will be used whenever xx is not fed with another value ). ",
          "a few things to notice : ",
          "don ' t forget to initialize the values for xx , by using , e.g ., tf.global_variables_initializer ().. "
        ]
      },
      {
        "title": "43773554",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.97",
        "r_prob": "0.79",
        "prob": "0.758637",
        "sentences": [
          "failedpreconditionerror : attempting to use uninitialized value is one of the most frequent errors related to tensorflow.",
          "this exception is most commonly raised when running an operation that reads a tf.variable before it has been initialized.",
          "in your case the error even explains what variable was not initialized : attempting to use uninitialized value variable_1.",
          "one of the tf tutorials explains a lot about variables , their creation / initialization / saving / loading ",
          "basically to initialize the variable you have 3 options : ",
          "initialize all global variables with tf.global_variables_initializer (). "
        ]
      },
      {
        "title": "43583960",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.97",
        "r_prob": "0.79",
        "prob": "0.758637",
        "sentences": [
          "this exception is most commonly raised when running an operation that reads a tf.variable before it has been initialized.",
          "in your case the error even explains what variable was not initialized : attempting to use uninitialized value variable_1.",
          "one of the tf tutorials explains a lot about variables , their creation / initialization / saving / loading ",
          "basically to initialize the variable you have 3 options : ",
          "initialize all global variables with tf.global_variables_initializer (). "
        ]
      },
      {
        "title": "45897756",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.71",
        "r_prob": "1.0",
        "prob": "0.6887",
        "sentences": [
          "however , now i ' m facing a new problem when trying to use the network copy.",
          "the new variables in the copy are not initialized , and trying to run tf.global_variables_initializer () does not help.",
          "or , if that weren ' t possible , at least a reliable way to get the initializer ops without having to find the tf.variable objects of the original network."
        ]
      },
      {
        "title": "57441807",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.68",
        "r_prob": "0.99",
        "prob": "0.659736",
        "sentences": [
          "therefore , all of the tf.variable in your tf graph will be initialized randomly.",
          "for example , in the code bellow , the tf.variable will be initialized with values from a normal distribution.",
          "you initialized those tf.variable with a the values you wish.",
          "as for the distinction between tf.global_variables_initializer and tf.constant_initializer , they are something completely different : ",
          "tf.global_variables_initializer is an operation that you execute to initialize all variables in your graph.",
          "tf.constant_initializer on the other hand , is just an initializer that you pass to the tf.variable of your graph.",
          "then , when a tf.session runs the operation tf.global_variables_initializer , the tf graph will use the tf.constant_initializer to initialize the corresponding tf.variable with the constant values provided."
        ]
      },
      {
        "title": "47271489",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.65",
        "r_prob": "0.99",
        "prob": "0.61776",
        "sentences": [
          "btw , if you want to initialize only a single tensor ( for e.g.",
          "tf.variable ) that hasn ' t been initialized using tf.global_variables_initializer (), then you can use your_tensor.initializer in the sess.run () as in the following example : "
        ]
      },
      {
        "title": "55372251",
        "h": "tf.variable",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.96",
        "r_prob": "0.8",
        "prob": "0.59904",
        "sentences": [
          "you just need to modify the code a little bit.",
          "the value of tf.variable should not be tf.placeholder , otherwise it will cause your initialization error when running sess.run ( tf.global_variables_initializer ()). "
        ]
      },
      {
        "title": "57438488",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.61",
        "r_prob": "1.0",
        "prob": "0.5978",
        "sentences": [
          "apparently there is , after running tf.global_variables_initializer () the variables got reinitialized.",
          "thus , the assertion fails.",
          "can a tf.variable be used in multiple sessions ? "
        ]
      },
      {
        "title": "45003763",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.69",
        "r_prob": "0.81",
        "prob": "0.48065399999999997",
        "sentences": [
          "the most obvious difference between the tf.variable and the tf.placeholder is that ",
          "initialization of the variables is done with sess.run ( tf.global_variables_initializer ()). ",
          "placeholder is a function , variable is a class ( hence an uppercase ). ",
          "when you use tf in a distributed environment , variables are stored in a special place ( parameter server ) and are shared between the workers .. "
        ]
      },
      {
        "title": "43693704",
        "h": "tf.global_variables_initializer",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.69",
        "r_prob": "0.81",
        "prob": "0.48065399999999997",
        "sentences": [
          "the most obvious difference between the tf.variable and the tf.placeholder is that ",
          "initialization of the variables is done with sess.run ( tf.global_variables_initializer ()). ",
          "placeholder is a function , variable is a class ( hence an uppercase ). ",
          "when you use tf in a distributed environment , variables are stored in a special place ( parameter server ) and are shared between the workers .. "
        ]
      }
    ],
    "tf.identity": [
      {
        "title": "41965889",
        "h": "tf.variable",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9215",
        "sentences": [
          "the tf.identity () operation is stateless.",
          "when you have a tf.variable called a , the value of tf.identity ( a ) will always be the same as the value of a.",
          "if you want b to remember a previous value of a , you should create b as a tf.variable as well : "
        ]
      },
      {
        "title": "34877802",
        "h": "tf.identity",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.74",
        "r_prob": "0.98",
        "prob": "0.6671840000000001",
        "sentences": [
          "tf.identity is useful when you want to explicitly transport tensor between devices ( like , from gpu to a cpu ). ",
          "a default behavior is that the send / recv nodes are added implicitly when the operation happens on a different device but you can imagine some situations ( especially in a multi - threaded / distributed settings ) when it might be useful to fetch the value of the variable multiple times within a single execution of the session.run.",
          "tf.identity allows for more control with regard to when the value should be read from the source device.",
          "also , please note that in the implementation of tf.variable link , the identity op is added in the constructor , which makes sure that all the accesses to the variable copy the data from the source only once.",
          "edit : updated answer after the question was edited.",
          "in addition , tf.identity can be used used as a dummy node to update a reference to the tensor."
        ]
      },
      {
        "title": "33717784",
        "h": "tf.variable",
        "t": "tf.identity",
        "r": "S1",
        "h_prob": "0.75",
        "t_prob": "0.96",
        "r_prob": "0.88",
        "prob": "0.6335999999999999",
        "sentences": [
          "this will create you a copy : v2 = tf.variable ( v1 ). ",
          "you can also use identity op : v2 = tf.identity ( v1 ) ( which i think is a proper way of doing it .. "
        ]
      }
    ],
    "tf.constant": [
      {
        "title": "43536220",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.97",
        "prob": "0.9030699999999999",
        "sentences": [
          "tf.constant () and tf.placeholder () are nodes in the graph ( ops or operations ). ",
          "on the other hand tf.variable () is a class."
        ]
      },
      {
        "title": "37854244",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.9",
        "r_prob": "0.99",
        "prob": "0.7306199999999999",
        "sentences": [
          "variables are tensors which you can update ( with var.assign ()). ",
          "technically speaking , tf.variable is not a subclass of tf.tensor though.",
          "tf.constant is just the most basic tensor , which contains a fixed value given when you create it."
        ]
      },
      {
        "title": "56480077",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.92",
        "r_prob": "0.84",
        "prob": "0.726432",
        "sentences": [
          "if you declare something with tf.constant () you won ' t be able to change the value in future.",
          "but , tf.variable () let ' s you change the variable in future."
        ]
      },
      {
        "title": "63435824",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.94",
        "r_prob": "0.99",
        "prob": "0.725868",
        "sentences": [
          "what was wrong : you cannot convert a tf.variable to a numpy array using graph execution and need to use a tf.constant instead."
        ]
      },
      {
        "title": "54977745",
        "h": "tf.variable",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.6480000000000001",
        "sentences": [
          "tf.operation represents a graph node and performs computation on tensors.",
          "tf.constant returns a special kind of tf.operation which takes 0 tensors as input and produces 0 tensors as output since it performs no computation.",
          "while tf.variable is in fact a nested operation ( or subgraph ) consists of 3 nodes.",
          "to my understanding , when users call this method , say , a = tf.constant ([ 1 , 2 , 3 ], name =' input_a '), two things will happen in different aspects : ",
          "maybe this node is just like a pointer pointing to the corresponding device memory with its value , waiting for other callable nodes to find it .. "
        ]
      },
      {
        "title": "35688187",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.66",
        "r_prob": "1.0",
        "prob": "0.6204",
        "sentences": [
          "simply create w as a tf.constant () that takes embedding as its value : ",
          "since embedding can be very large , you should only use this approach for toy examples.",
          "create w as a tf.variable and initialize it from the numpy array via a tf.placeholder (): ",
          "this avoid storing a copy of embedding in the graph , but it does require enough memory to keep two copies of the matrix in memory at once ( one for the numpy array , and one for the tf.variable ). "
        ]
      },
      {
        "title": "45267542",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.63",
        "r_prob": "0.98",
        "prob": "0.543312",
        "sentences": [
          "if you don ' t want train this weights , you can define them as tf.constant not tf.variable "
        ]
      },
      {
        "title": "44220922",
        "h": "tf.constant",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.55",
        "t_prob": "0.95",
        "r_prob": "0.87",
        "prob": "0.45457499999999995",
        "sentences": [
          "however , you declared a as a tf.constant in the first line -- and you can ' t assign a new value to a constant.",
          "if you replace your first line with a = tf.variable ( 1 ) then you have a program that will execute successfully."
        ]
      },
      {
        "title": "56480281",
        "h": "tf.variable",
        "t": "tf.constant",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.43",
        "r_prob": "0.71",
        "prob": "0.28698199999999996",
        "sentences": [
          "if you declare a tf.variable , you can change it ' s value later on if you want to.",
          "if the first layers are defined as tf.constant , you can ' t do that."
        ]
      }
    ],
    "tf.float32": [
      {
        "title": "46566863",
        "h": "tf.float32",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.93",
        "r_prob": "0.99",
        "prob": "0.902286",
        "sentences": [
          "in your first code snippet , you basically define y = tf.placeholder ( tf.float32 ) + tf.variable ([ 1.0 , 1.0 , 1.0 ], tf.float32 ). ",
          "in your second example , you define y = tf.placeholder ( tf.float32 ) + tf.assign ( tf.variable ([ 1.0 , 1.0 , 1.0 ], tf.float32 ), [ 4.0 , 4.0 , 4.0 ]). ",
          "so , no matter which value you assign to c , the computation graph contains the assign operation and will always assign [ 4.0 , 4.0 , 4.0 ] to it before computing the sum."
        ]
      },
      {
        "title": "44284879",
        "h": "tf.float32",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.87",
        "r_prob": "0.98",
        "prob": "0.76734",
        "sentences": [
          "you should see tensorflow as a graph , with : ",
          "w = tf.variable ([. 3 ], tf.float32 ) b = tf.variable ([-. 3 ], tf.float32 ) b = tf.variable ([-. 3 ], tf.float32 ) linear_model = w * x + b "
        ]
      },
      {
        "title": "42899188",
        "h": "tf.float32",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.7200000000000001",
        "sentences": [
          "from the documentation of tf.variable : ",
          "if e.g.",
          "you call tf.variable ([. 3 ], tf.float64 ) the resulting tensor has the same dtype as when calling tf.variable ([. 3 ], tf.float32 ). ",
          "in fact , i believe that both calls tf.variable ([. 3 ], tf.float32 ) and tf.variable ([. 3 ], tf.float64 ) are equivalent , as the second argument to tf.variable is a boolean and thus tf.floatx is being converted to a boolean which always returns true."
        ]
      },
      {
        "title": "42095734",
        "h": "tf.float32",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.65",
        "r_prob": "0.54",
        "prob": "0.23868000000000006",
        "sentences": [
          "every time model.evaluate get called , it will run the model_fn again.",
          "so if i create several variables using tf.variable and update the metric score of each evaluation batch to a variable using assign_add , then i can successfully get the accumulated metric score for all evaluation batches.",
          "and because every time model_fn get called , the variables get reset to 0 by the tf.variable ( 0 ., dtype = tf.float32 ) call , previous evaluation will not affect the result of next evaluation."
        ]
      }
    ],
    "tf.trainable_variables": [
      {
        "title": "38580555",
        "h": "tf.trainable_variables",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.9016000000000001",
        "sentences": [
          "since tf.trainable_variables () returns a list of tf.variable objects , you should be able to pass its result straight to session.run (): "
        ]
      },
      {
        "title": "46369727",
        "h": "tf.variable",
        "t": "tf.trainable_variables",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.97",
        "r_prob": "0.95",
        "prob": "0.8662099999999999",
        "sentences": [
          "actually , it ' s pretty straightforward.",
          "a tf.variable constructor has an argument trainable.",
          "so at any moment you can invoke tf.trainable_variables () to see the variables that are going to be updated through backprop.",
          "a tf.placeholder only adds a node to the graph."
        ]
      },
      {
        "title": "43075138",
        "h": "tf.trainable_variables",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.86",
        "r_prob": "0.94",
        "prob": "0.6952239999999998",
        "sentences": [
          "the easiest way to turn a variable name into a tf.variable object is to filter tf.trainable_variables (), matching on the name : ",
          "once you have a tf.variable object , you can use its load () method to assign a new weight : "
        ]
      },
      {
        "title": "43730032",
        "h": "tf.variable",
        "t": "tf.trainable_variables",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.98",
        "r_prob": "0.74",
        "prob": "0.674436",
        "sentences": [
          "you can create non - trainable variables in two different ways : ",
          "tf.variable ( a , trainable = false ). ",
          "tf.get_variable (\" a \", a , trainable = false ). ",
          "also there is no easy way to check whether the variable is trainable ( you need to check whether the name of your variable is in the list of tf.trainable_variables () "
        ]
      },
      {
        "title": "37624060",
        "h": "tf.variable",
        "t": "tf.trainable_variables",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.72",
        "r_prob": "0.94",
        "prob": "0.629424",
        "sentences": [
          "most tensorflow tensors ( tf.tensor objects ) are immutable , so you cannot simply assign a value to them.",
          "however , if you created the tensor as a tf.variable , you can assign a value to it by calling variable.assign (). ",
          "the code you have unnecessarily converts a tf.variable object ( from the list of tf.trainable_variables ()) into a string name."
        ]
      },
      {
        "title": "48460190",
        "h": "tf.trainable_variables",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.8",
        "r_prob": "0.71",
        "prob": "0.51688",
        "sentences": [
          "the distinction between trainable variables and non - trainable variables is used to let optimizers know which variables they can act upon.",
          "when defining a tf.variable (), setting trainable = true ( the default ) automatically adds the variable to the graphkeys.trainable_variables collection.",
          "during training , an optimizer gets the content of that collection via tf.trainable_variables () and applies the training to all of them."
        ]
      },
      {
        "title": "36193677",
        "h": "tf.variable",
        "t": "tf.trainable_variables",
        "r": "S1",
        "h_prob": "0.66",
        "t_prob": "0.97",
        "r_prob": "0.64",
        "prob": "0.409728",
        "sentences": [
          "in tensorflow , trained weights are represented by tf.variable objects.",
          "if you created a tf.variable — e.g.",
          "if you do not currently have a pointer to the tf.variable , you can get a list of the trainable variables in the current graph by calling tf.trainable_variables (). ",
          "this function returns a list of all trainable tf.variable objects in the current graph , and you can select the one that you want by matching the v.name property."
        ]
      }
    ],
    "tf.reshape": [
      {
        "title": "46451497",
        "h": "tf.reshape",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.98",
        "r_prob": "0.92",
        "prob": "0.838488",
        "sentences": [
          "your error message shows exact reason why it is raised.",
          "using tf.reshape function.tf.reshape ( x , shape =[ 387 , 7 * 10 ]) will be works , and also change your w to right dimension to multiply.",
          "like , tf.variable ( tf.ones ([ 7 * 10 , 1 ])). "
        ]
      },
      {
        "title": "39715887",
        "h": "tf.reshape",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "0.74",
        "prob": "0.703444",
        "sentences": [
          "can you try output = tf.reshape ( output , [ batch_size , num_hidden ]) and weight = tf.variable ( tf.random_normal ([ num_hidden , num_classes ])) and let me know how that goes ? "
        ]
      },
      {
        "title": "61770185",
        "h": "tf.reshape",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.68",
        "r_prob": "0.67",
        "prob": "0.3052520000000001",
        "sentences": [
          "the problem is related to eager execution tf 2.0 -- any operations such as tf.reshape are run the moment they are encountered.",
          "build is only called a single time for a given model.",
          "now , what is happening is that you are creating a tensor weights2 , which is a reshaped version of the tf.variable weights but is not itself a tf.variable ( ops generally return tensors , not variables ). ",
          "this does not happen in the else case because here , weights2 is just another name referring to the actual tf.variable weights.",
          "this works because now the reshape operation is part of the model call graph , i.e."
        ]
      }
    ],
    "tf.tensor": [
      {
        "title": "48796671",
        "h": "tf.variable",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.99",
        "r_prob": "0.87",
        "prob": "0.8354609999999999",
        "sentences": [
          "the problem is that the y_out argument to sess.run () is none , whereas it must be a tf.tensor ( or tensor - like object , such as a tf.variable ) or a tf.operation."
        ]
      },
      {
        "title": "37854244",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.8190000000000001",
        "sentences": [
          "variables are tensors which you can update ( with var.assign ()). ",
          "technically speaking , tf.variable is not a subclass of tf.tensor though."
        ]
      },
      {
        "title": "35931354",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.88",
        "r_prob": "0.99",
        "prob": "0.810216",
        "sentences": [
          "in tensorflow a tf.variable can be used anywhere a tf.tensor ( of the same element type and shape ) is expected."
        ]
      },
      {
        "title": "36240769",
        "h": "tf.variable",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.98",
        "r_prob": "0.97",
        "prob": "0.8080099999999999",
        "sentences": [
          "tl ; dr : you can ' t feed a tf.tensor object ( viz."
        ]
      },
      {
        "title": "37624060",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.93",
        "r_prob": "0.86",
        "prob": "0.687828",
        "sentences": [
          "most tensorflow tensors ( tf.tensor objects ) are immutable , so you cannot simply assign a value to them.",
          "however , if you created the tensor as a tf.variable , you can assign a value to it by calling variable.assign (). ",
          "the code you have unnecessarily converts a tf.variable object ( from the list of tf.trainable_variables ()) into a string name."
        ]
      },
      {
        "title": "37706972",
        "h": "tf.variable",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.79",
        "r_prob": "0.97",
        "prob": "0.674344",
        "sentences": [
          "* with the exception of tf.variable objects , using the variable.assign () etc.",
          "methods.",
          "however , rnn.rnn () likely returns a tf.tensor object that does not support this method."
        ]
      },
      {
        "title": "61735399",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.64",
        "r_prob": "0.99",
        "prob": "0.614592",
        "sentences": [
          "regarding how to implement your tftimeseries class , tf.variable and tf.costant might be interesting classes.",
          "here is their documentation : tf.variable , tf.costant "
        ]
      },
      {
        "title": "57957877",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.77",
        "r_prob": "0.81",
        "prob": "0.592515",
        "sentences": [
          "i believe the problem here is that you have previously manipulated your data sets in numpy formats , but now are using tensorflow foormats such as tf.variable or tf.tensor.",
          "this line ",
          "but if data is a tf.variable : "
        ]
      },
      {
        "title": "44486260",
        "h": "tf.variable",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.63",
        "t_prob": "0.91",
        "r_prob": "1.0",
        "prob": "0.5733",
        "sentences": [
          "here is a couple of workarounds : ",
          "tf.variable.",
          "tf.tensor cannot be changed , but tf.variable can."
        ]
      },
      {
        "title": "46375813",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.82",
        "r_prob": "0.71",
        "prob": "0.5647339999999998",
        "sentences": [
          "you can read more about variable initialization here , which says unlike tf.tensor objects , a tf.variable exists outside the context of a single session.run call.",
          "so before you can use a variable , it must be initialized."
        ]
      },
      {
        "title": "39513635",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.63",
        "r_prob": "0.98",
        "prob": "0.55566",
        "sentences": [
          "the four examples you gave will all give the same result , and generate the same graph ( if you ignore that some of the operation names in the graph are different ). ",
          "tensorflow will convert many different python objects into tf.tensor objects when they are passed as arguments to tensorflow operators , such as tf.add () here.",
          "the + operator is just a simple wrapper on tf.add (), and the overload is used when either the left - hand or right - hand argument is a tf.tensor ( or tf.variable ). ",
          "in that case , you may wish to convert the array to a tf.tensor once ",
          "creating a tf.constant () explicitly allows you to set its name property , which can be useful for tensorboard debugging and graph visualization."
        ]
      },
      {
        "title": "51381606",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.83",
        "r_prob": "0.6",
        "prob": "0.47807999999999995",
        "sentences": [
          "the error is because t1 is a tf.variable object , while t1 [ 1 ] is a tf.tensor.",
          "as it happens , tf.tensor can ' t be mutated ( it ' s read only ) whereas tf.variable can be ( read as well as write ) see here."
        ]
      },
      {
        "title": "44284879",
        "h": "tf.variable",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.84",
        "r_prob": "0.61",
        "prob": "0.445788",
        "sentences": [
          "you should see tensorflow as a graph , with : ",
          "w = tf.variable ([. 3 ], tf.float32 ) b = tf.variable ([-. 3 ], tf.float32 ) b = tf.variable ([-. 3 ], tf.float32 ) linear_model = w * x + b "
        ]
      },
      {
        "title": "63840420",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.61",
        "t_prob": "0.79",
        "r_prob": "0.85",
        "prob": "0.409615",
        "sentences": [
          "in the code you point to , initial_weights is only a collection of values ( tf.tensor objects ), and model_weights is a reference to the model ' s variables ( tf.variable objects ). "
        ]
      },
      {
        "title": "42792589",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.51",
        "r_prob": "0.88",
        "prob": "0.39045599999999997",
        "sentences": [
          "a tf.tensor in tensorflow is a read - only value — in fact , a symbolic expression for computing a read - only value — so you cannot in general assign values to it.",
          "( the main exceptions are tf.variable objects .) "
        ]
      },
      {
        "title": "37572852",
        "h": "tf.tensor",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.76",
        "t_prob": "0.61",
        "r_prob": "0.77",
        "prob": "0.356972",
        "sentences": [
          "each operation assign or assignadd has two inputs and no output : ",
          "a tf.variable : the variable to which we assign a value.",
          "here you can see a bidirectional edge because the operation reads the old value from x and then writes back the new value.",
          "a tf.tensor : the value assigned to the variable ( or added to the variable ). ",
          "in the operation tf.add ( x , y ): the graph reads the value of x as an input."
        ]
      }
    ],
    "tf.assign": [
      {
        "title": "44300480",
        "h": "tf.variable",
        "t": "tf.assign",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.95",
        "r_prob": "0.9",
        "prob": "0.82935",
        "sentences": [
          "from the docs ( emphasis mine ): ",
          "calling tf.variable () adds several ops to the graph : ",
          "a variable op that holds the variable value .. ",
          "an initializer op that sets the variable to its initial value.",
          "this is actually a tf.assign op .. ",
          "the ops for the initial value , such as the zeros op for the biases variable in the example are also added to the graph .. "
        ]
      },
      {
        "title": "44929824",
        "h": "tf.variable",
        "t": "tf.assign",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.8",
        "r_prob": "0.96",
        "prob": "0.7296",
        "sentences": [
          "tf.variable objects cannot be used as loop variables in a while loop , as loop variables are implemented differently.",
          "so either create your variable outside the loop and update it yourself with tf.assign in each iteration or manually keep track of the updates as you do with loop variables ( by returning their updated values from the loop lambdas , and in your case using the value from the inner loop as the new value for the outer loop ). "
        ]
      },
      {
        "title": "51122680",
        "h": "tf.variable",
        "t": "tf.assign",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.98",
        "r_prob": "0.92",
        "prob": "0.72128",
        "sentences": [
          "tf.variable can be used to create the same type of code.",
          "the main properties of tf.variable i understand is this.",
          "reference is this ",
          "tf.variable has to be initialized before it is used by using tf.assign or executing an initializer or by loading its saved state from a file."
        ]
      },
      {
        "title": "44220922",
        "h": "tf.assign",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.95",
        "r_prob": "0.74",
        "prob": "0.66082",
        "sentences": [
          "in the line tf.assign ( a , b ) you are instructing tensorflow to assign the value of b to the variable a.",
          "if you replace your first line with a = tf.variable ( 1 ) then you have a program that will execute successfully."
        ]
      },
      {
        "title": "58235644",
        "h": "tf.variable",
        "t": "tf.assign",
        "r": "S1",
        "h_prob": "0.63",
        "t_prob": "0.86",
        "r_prob": "0.98",
        "prob": "0.530964",
        "sentences": [
          "tf.assign * functions are available as methods on tf.variable in tf 2.0.",
          "note that unlike tf.assign in tf 1.x , tf.variable.assing_sub will execute the assignment eagerly."
        ]
      },
      {
        "title": "42659635",
        "h": "tf.variable",
        "t": "tf.assign",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.73",
        "r_prob": "0.71",
        "prob": "0.46128699999999995",
        "sentences": [
          "having tf.variable objects as loop variables for while loops is not supported , and will behave in weird nondeterministic ways.",
          "always use tf.assign and friends to update the value of a tf.variable."
        ]
      }
    ],
    "tf.add": [
      {
        "title": "36113230",
        "h": "tf.add",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.8188000000000001",
        "sentences": [
          "yes , - and + resolve to tf.sub ad tf.add.",
          "if you look at the tensorflow code you will see that these operators on tf.variable are overloaded with the tf."
        ]
      },
      {
        "title": "56833985",
        "h": "tf.variable",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.83",
        "r_prob": "0.98",
        "prob": "0.7971319999999998",
        "sentences": [
          "as the value of op is always changing , you can use tf.variable () to store its value after every iteration.",
          "here , tf.variable () is initialized with a zero tensor at the start.",
          "update : so , you want to initialize prevop with tf.add ( inp1 , 10 ) and then update it with values of op , which is tf.add ( tf.multiply ( prevop , inp2 ), inp1 ). "
        ]
      },
      {
        "title": "53915891",
        "h": "tf.add",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.91",
        "r_prob": "0.81",
        "prob": "0.6265350000000001",
        "sentences": [
          "this is resetting x back to its original value ( 0 ). ",
          "your code to increase x replaces ' x ' with a tf.add operation and then your summary value is no longer tracing a tf.variable but an addition operation."
        ]
      },
      {
        "title": "39513635",
        "h": "tf.add",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.7",
        "t_prob": "0.63",
        "r_prob": "0.98",
        "prob": "0.43217999999999995",
        "sentences": [
          "the four examples you gave will all give the same result , and generate the same graph ( if you ignore that some of the operation names in the graph are different ). ",
          "the + operator is just a simple wrapper on tf.add (), and the overload is used when either the left - hand or right - hand argument is a tf.tensor ( or tf.variable ). "
        ]
      },
      {
        "title": "37572852",
        "h": "tf.variable",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.61",
        "t_prob": "0.62",
        "r_prob": "0.94",
        "prob": "0.35550799999999994",
        "sentences": [
          "each operation assign or assignadd has two inputs and no output : ",
          "a tf.variable : the variable to which we assign a value.",
          "here you can see a bidirectional edge because the operation reads the old value from x and then writes back the new value.",
          "a tf.tensor : the value assigned to the variable ( or added to the variable ). ",
          "the variable x appears once in the graph in the big block named x , but is used twice : ",
          "in the operation tf.add ( x , y ): the graph reads the value of x as an input."
        ]
      }
    ],
    "tf.name_scope": [
      {
        "title": "55376814",
        "h": "tf.name_scope",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.9",
        "r_prob": "0.99",
        "prob": "0.77517",
        "sentences": [
          "to control variable naming users can use tf.name_scope + tf.variable.",
          "to control variable naming users can use tf.name_scope + tf.variable.",
          "indeed , tf.name_scope still exists in tensorflow 2.0 , so you can just do : "
        ]
      },
      {
        "title": "37115316",
        "h": "tf.name_scope",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.88",
        "r_prob": "0.71",
        "prob": "0.443608",
        "sentences": [
          "you might want to use tf.get_variable () instead of ' tf.variable `. ",
          "when we do with tf.variable_scope (\" name \"), this implicitly opens a tf.name_scope (\" name \"). ",
          "name_scope is originally used for managing operation names ( such as add , matmul ), because tf.variable is actually an operation and its operation name will be \" inherited \" by variables created by it , so the name of name_scope rather than variable_scope is used as prefix.",
          "but if you want to use tf.variable , you can also directly use name_scope in with statement : ",
          "if name has been used before , it will be made unique by calling self.unique_name ( name ). "
        ]
      },
      {
        "title": "34232533",
        "h": "tf.name_scope",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.47",
        "t_prob": "0.83",
        "r_prob": "1.0",
        "prob": "0.39009999999999995",
        "sentences": [
          "when you create a variable with tf.get_variable instead of tf.variable , tensorflow will start checking the names of the vars created with the same method to see if they collide.",
          "if you created a var with tf.get_variable and you try to change the prefix of your variable names by using the tf.name_scope context manager , this won ' t prevent the tensorflow of raising an exception.",
          "in summary , tf.name_scope just add a prefix to all tensor created in that scope ( except the vars created with tf.get_variable ), and tf.variable_scope add a prefix to the variables created with tf.get_variable."
        ]
      },
      {
        "title": "42542428",
        "h": "tf.variable",
        "t": "tf.name_scope",
        "r": "S1",
        "h_prob": "0.61",
        "t_prob": "0.78",
        "r_prob": "0.72",
        "prob": "0.342576",
        "sentences": [
          "you don ' t need to define two function for generating models , you can use tf.name_scope , and pass a model name to the function to use it as a prefix for variable declaration.",
          "even if you feed both models with exactly same training batches , you can ' t expect to have exactly similar characteristics models since initial values for weights are different and this could cause falling into different local minimum of error surface."
        ]
      }
    ],
    "tf.operation": [
      {
        "title": "44903756",
        "h": "tf.variable",
        "t": "tf.operation",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.76",
        "r_prob": "1.0",
        "prob": "0.7296",
        "sentences": [
          "the tensorflow graph is an object which contains your various tf.tensor and tf.operation.",
          "when you create these tensors ( e.g.",
          "using tf.variable or tf.constant ) or operations ( e.g.",
          "work with the default graph ,. ",
          "create a fresh new graph ,. "
        ]
      },
      {
        "title": "54977745",
        "h": "tf.operation",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.81",
        "r_prob": "0.98",
        "prob": "0.579474",
        "sentences": [
          "tf.operation represents a graph node and performs computation on tensors.",
          "tf.constant returns a special kind of tf.operation which takes 0 tensors as input and produces 0 tensors as output since it performs no computation.",
          "while tf.variable is in fact a nested operation ( or subgraph ) consists of 3 nodes.",
          "maybe this node is just like a pointer pointing to the corresponding device memory with its value , waiting for other callable nodes to find it .. "
        ]
      },
      {
        "title": "42861919",
        "h": "tf.operation",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.83",
        "r_prob": "0.91",
        "prob": "0.506051",
        "sentences": [
          "the documentation for tf.variable.op is not particularly clear , but it does refer to the crucial tf.operation used in the implementation of a tf.variable : any op that depends on a tf.variable will be on a path from that operation."
        ]
      },
      {
        "title": "53906363",
        "h": "tf.variable",
        "t": "tf.operation",
        "r": "S1",
        "h_prob": "0.58",
        "t_prob": "0.68",
        "r_prob": "0.99",
        "prob": "0.390456",
        "sentences": [
          "a session may own resources , such as tf.variable and it is important to release these resources when they are no longer required.",
          "the fetches argument may be a single graph element , or an arbitrarily nested list , tuple , namedtuple , dict , or ordereddict containing graph elements at its leaves.",
          "a graph element can be one of the following types : ",
          "an tf.operation.",
          "the corresponding fetched value will be none .. "
        ]
      }
    ],
    "tf.function": [
      {
        "title": "61875571",
        "h": "tf.variable",
        "t": "tf.function",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.82",
        "r_prob": "0.82",
        "prob": "0.53792",
        "sentences": [
          "elaborating jdehesa ' s comment for the benefit of the community.",
          "in the official documentation for tf.function , it is mentioned as : ",
          "it is always recommended to pass tensors as arguments to the functions instead of python scalars so that all the operations will be captured in a single graph.",
          "tf.function only allows creating new tf.variable objects when it is called for the first time.",
          "in general , it is recommended to create stateful objects like tf.variable outside of tf.function and passing them as arguments."
        ]
      },
      {
        "title": "61147928",
        "h": "tf.function",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.79",
        "r_prob": "0.9",
        "prob": "0.50481",
        "sentences": [
          "i ' m assuming you want an instance of tf.variable only to use assign.",
          "however , when using tf.function , you should always provide variables from outside , and use built - in tensorflow data structures inside."
        ]
      },
      {
        "title": "59350211",
        "h": "tf.function",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.65",
        "t_prob": "0.93",
        "r_prob": "0.59",
        "prob": "0.356655",
        "sentences": [
          "the code in your first snippet ( the one without the @ tf.function ) takes advantage of tensorflow 2 ' s eager execution to manipulate a numpy array ( i.e ., your outer iteration object ) directly.",
          "with @ tf.function , this doesn ' t work because @ tf.function tries to compile your code into a tf.graph , which cannot operate on a numpy array directly ( it can only process tensorflow tensors ). ",
          "to get around this issue , use a tf.variable and keep assigning value into its slices.",
          "with @ tf.function , what you are trying to do is actually achievable with simpler code , by taking advantage of @ tf.function ' s automatic python - to - graph transformation feature ( known as autograph ). "
        ]
      }
    ]
  },
  "tf.estimator.train_and_evaluate": {},
  "tf.device": {
    "tf.train.replica_device_setter": [
      {
        "title": "38666008",
        "h": "tf.train.replica_device_setter",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "1.0",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.96",
        "sentences": [
          "higher - level constructs like tf.train.replica_device_setter () wrap tf.device () to specify common policies such as \" shard the variables across parameter servers , and otherwise put all ops on the worker device ,\" and we use this extensively in distributed training.",
          "in practice we have found that a small set of annotations usually yields a more efficient placement than the dynamic placer will determine , but improving the placement algorithm remains an area of active research."
        ]
      },
      {
        "title": "37733117",
        "h": "tf.device",
        "t": "tf.train.replica_device_setter",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9409",
        "sentences": [
          "if a model has long , independent computation paths , then you can split the model across multiple gpus and have each compute a part of it.",
          "start up multiple copies of the model , train them , and then synchronize their learning ( the gradients applied to their weights & biases ). ",
          "but to tl ; dr that source : in a multi - gpu setup , it ' s often best to synchronously update the model by storing the weights on the cpu ( well , in its attached dram ). ",
          "but in a multi - machine setup , we often use a separate \" parameter server \" that stores and propagates the weight updates."
        ]
      },
      {
        "title": "40979990",
        "h": "tf.train.replica_device_setter",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "0.96",
        "prob": "0.940896",
        "sentences": [
          "if you split your code across multiple devices , tensorflow will add the appropriate communication , and this extends to devices in different processes.",
          "the with tf.device ( tf.train.replica_device_setter (...)): block tells tensorflow to put each variable on a different ps task by setting its device to \"/ job : ps / task :{ i }\" ( for different values of { i }, chosen in a round - robin fashion ). "
        ]
      },
      {
        "title": "40712445",
        "h": "tf.train.replica_device_setter",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.9216899999999999",
        "sentences": [
          "the device placement for nodes happens only once.",
          "you can control the device placement with directive such as tf.device or tf.train.replica_device_setter.",
          "when you use tf.device , a device function would be pushed to a stack and the following nodes would call the device function in the stack to get a device assignment."
        ]
      },
      {
        "title": "41482827",
        "h": "tf.train.replica_device_setter",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.9025",
        "sentences": [
          "instead of achieving this by calling tf.device manually everywhere , a helper function named tf.train.replica_device_setter is used that sets a tf.variable ' s device to a parameter server , and the other operations to a worker."
        ]
      },
      {
        "title": "46064309",
        "h": "tf.train.replica_device_setter",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.97",
        "r_prob": "0.96",
        "prob": "0.88464",
        "sentences": [
          "there ' s magic happening in tf.train.replica_device_setter which takes place of the manual with tf.device annotations and has the effect of automatically assigning variables across devices."
        ]
      },
      {
        "title": "38794027",
        "h": "tf.train.replica_device_setter",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.97",
        "r_prob": "0.89",
        "prob": "0.854667",
        "sentences": [
          "this definition is outside the scope of the with tf.device ( tf.train.replica_device_setter (...)): block above , so no device is assigned to global_step.",
          "fortunately , the solution is simple.",
          "you can either define global_step inside the with tf.device ( tf.train.replica_device_setter (...)): block above , or add a small with tf.device (\"/ job : ps / task : 0 \"): block as follows : "
        ]
      }
    ],
    "tf.session": [
      {
        "title": "59532982",
        "h": "tf.device",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.94",
        "r_prob": "0.99",
        "prob": "0.921294",
        "sentences": [
          "with tf.device (...): applies to the graph nodes created within the scope , not session.run calls.",
          "mainly you can stop using tf.session and tf.graph.",
          "use @ tf.function instead , i believe this basic structure will work : "
        ]
      },
      {
        "title": "36320000",
        "h": "tf.device",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.86",
        "r_prob": "1.0",
        "prob": "0.8428",
        "sentences": [
          "wrap large contiguous regions of your code in a with tf.device (...): block , naming the different gpus : "
        ]
      },
      {
        "title": "41495299",
        "h": "tf.device",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.8036",
        "sentences": [
          "use the device_count of configproto like tf.session ( config = tf.configproto ( device_count ={\" gpu \": 0 , \" cpu \": 1 })). "
        ]
      },
      {
        "title": "33961914",
        "h": "tf.session",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.97",
        "r_prob": "0.95",
        "prob": "0.7832749999999999",
        "sentences": [
          "the tensorboard graph visualizer only sees the explicit device assignments that you have made in your program ( i.e.",
          "those made using with tf.device (\"...\"): blocks ). ",
          "a later placement stage runs inside the tensorflow backend , and assigns every node to a device.",
          "currently there is no support for this in tensorboard , but you can extract some information by creating the tf.session as follows : "
        ]
      }
    ],
    "tf.graph": [
      {
        "title": "51183870",
        "h": "tf.device",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.834372",
        "sentences": [
          "the fact that you specify a device ( with tf.device ('/ gpu : 0 ) for your loop is a hint that it is the case : you typically specify a device for new nodes as this does not affect nodes that are already defined.",
          "fortunately , tensorflow has a convenient tool to spot those errors : tf.graph.finalize."
        ]
      },
      {
        "title": "59532982",
        "h": "tf.device",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.95",
        "r_prob": "0.8",
        "prob": "0.7524000000000001",
        "sentences": [
          "with tf.device (...): applies to the graph nodes created within the scope , not session.run calls.",
          "mainly you can stop using tf.session and tf.graph.",
          "use @ tf.function instead , i believe this basic structure will work : "
        ]
      },
      {
        "title": "40873770",
        "h": "tf.device",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.96",
        "r_prob": "0.68",
        "prob": "0.55488",
        "sentences": [
          "yes.",
          "this process only needs to create part of the graph for the current node using with tf.device.",
          "if you use within - graph replication with single client , your client needs to create graph for all nodes using multiple with tf.graph sections .. "
        ]
      }
    ]
  },
  "tf.contrib.data.choose_from_datasets": {},
  "tf.contrib.data.sample_from_datasets": {},
  "tf.random_normal )": {},
  "tf.contrib.lookup.keyvaluetensorinitializer": {},
  "tf.unsorted_segment_sum": {},
  "ger -": {},
  "tf.compat.v1.enable_eager_execution": {},
  "neural networks": {
    "neural network": [
      {
        "title": "50505609",
        "h": "neural networks",
        "t": "neural network",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.92",
        "r_prob": "0.91",
        "prob": "0.7451080000000001",
        "sentences": [
          "in multiclass , your classifier chooses one class from n other classes.",
          "usually , the last layer in neural networks that do multiclass classification is a softmax layer.",
          "it makes sense to use binary cross - entropy for that , since the way most neural network framework work makes it behave like you calculate average binary cross - entropy over these binary tasks.",
          "in neural networks that are multilabel classifiers , sigmoid is used as the last layer ( kaggle kernel you linked uses sigmoid as activation in the last layer ). "
        ]
      },
      {
        "title": "39023007",
        "h": "neural network",
        "t": "neural networks",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.8",
        "r_prob": "0.99",
        "prob": "0.74448",
        "sentences": [
          "the sklearn gradientboostingclassifier is a different algorithm than a neural network.",
          "this is the trade - off when using neural networks ; if you want performance better than alternative algorithms like random forests and svm , you need to tune the hyper parameters."
        ]
      },
      {
        "title": "62668130",
        "h": "neural networks",
        "t": "neural network",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.78",
        "r_prob": "0.89",
        "prob": "0.5623020000000001",
        "sentences": [
          "neural networks are designed to copy - cat.",
          "this is the nature of a neural network.",
          "an isolated optimization problem asks a fundamentally different question.",
          "if you want to phrase an optimization problem in terms of a neural network solution it would look like this.",
          "given a collection of approximated functions ( millions of trained neural networks ) and known optimized solutions ( the expected solutions for each one ), train a new neural network that mimics this behavior.",
          "these are great examples of something a neural network could learn to do , on unseen functions from the dataset.",
          "the neural network , might even learn the underlying behavior so well that it works with coefficients outside the initial dataset too.",
          "of course , one could start building a massive collection of optimization neural networks that applies in millions of different cases for all kinds of different problems.",
          "such \" neural network zoo \" could solve all of optimization theory."
        ]
      },
      {
        "title": "59999062",
        "h": "neural network",
        "t": "neural networks",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.89",
        "r_prob": "0.63",
        "prob": "0.487809",
        "sentences": [
          "the second question is case - dependent ; when building neural networks from scratch , there is no guarantee that your problem will work better with approach a or approach b ; this is why we do hyperparameter search and optimization , in order to seek for the best overall parameters in order to minimize our loss on the validation set .. "
        ]
      },
      {
        "title": "47182326",
        "h": "neural networks",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.79",
        "t_prob": "0.87",
        "r_prob": "0.62",
        "prob": "0.426126",
        "sentences": [
          "convolutional networks are just another type of neural network.",
          "even in \" normal \" neural networks , one doesn ' t typically specify weights and biases manually."
        ]
      },
      {
        "title": "49952428",
        "h": "neural networks",
        "t": "neural network",
        "r": "S1",
        "h_prob": "0.62",
        "t_prob": "0.76",
        "r_prob": "0.79",
        "prob": "0.372248",
        "sentences": [
          "while neural networks are very complex and powerful , they aren ' t a magic box.",
          "this neural network has a clear sense of the frequency of the wave , but needs a bit more work on determining the general trend of the line."
        ]
      },
      {
        "title": "54173845",
        "h": "neural networks",
        "t": "neural network",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.74",
        "r_prob": "0.56",
        "prob": "0.348096",
        "sentences": [
          "still , i can already tell you that starting with neural networks is a poor course of action , as you immediately forsake understanding of the domain.",
          "troubleshooting of misbehaving neural network is far more cumbersome than majority of other learning systems."
        ]
      }
    ],
    "learning rate": [
      {
        "title": "50843787",
        "h": "learning rate",
        "t": "neural networks",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.83",
        "r_prob": "1.0",
        "prob": "0.6971999999999999",
        "sentences": [
          "reference : dropout : a simple way to prevent neural networks from overfitting "
        ]
      },
      {
        "title": "47916386",
        "h": "learning rate",
        "t": "neural networks",
        "r": "S2",
        "h_prob": "0.79",
        "t_prob": "0.75",
        "r_prob": "0.99",
        "prob": "0.5865750000000001",
        "sentences": [
          "dropout : a simple way to prevent neural networks from overfitting."
        ]
      },
      {
        "title": "51188348",
        "h": "learning rate",
        "t": "neural networks",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.92",
        "r_prob": "0.54",
        "prob": "0.4024080000000001",
        "sentences": [
          "though some times this large percentage of rate argument is feasible , some times it hinders the learning rate of neural networks."
        ]
      }
    ],
    "machine learning": [
      {
        "title": "62399725",
        "h": "machine learning",
        "t": "neural networks",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.77",
        "r_prob": "1.0",
        "prob": "0.5852",
        "sentences": [
          "to represent any point in this space , say d1 , we need a tuple of three real numbers ( x1 , y1 , z1 ). "
        ]
      },
      {
        "title": "51938945",
        "h": "machine learning",
        "t": "neural networks",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.79",
        "r_prob": "0.94",
        "prob": "0.564376",
        "sentences": [
          "this isn ' t how neural networks , or machine learning in general works."
        ]
      },
      {
        "title": "53239472",
        "h": "neural networks",
        "t": "machine learning",
        "r": "S1",
        "h_prob": "0.64",
        "t_prob": "0.62",
        "r_prob": "0.94",
        "prob": "0.372992",
        "sentences": [
          "high - level object oriented libraries that bring about abstraction when developing neural networks ( nn ) or other machine learning ( ml ) algorithms .. "
        ]
      }
    ]
  },
  "tf.u": {},
  "tf.estimator.linearclassifier": {},
  "tf.nn.deconv2d": {},
  "tf.nn.sampled_softmax_loss": {},
  "tf.nn.samp": {},
  "tf.keras.applications.mobilenet": {},
  "tf.linalg.tensordot": {},
  "tf.assign_add": {},
  "tf.keras.utils.progbar": {},
  "binary classification": {},
  "tf.clip_by_value": {},
  "tf.compute_gradients": {},
  "tf.nn.rnn": {
    "tf.nn.dynamic_rnn": [
      {
        "title": "38465663",
        "h": "tf.nn.rnn",
        "t": "tf.nn.dynamic_rnn",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "the tf.nn.dynamic_rnn () or tf.nn.rnn () operations allow to specify the initial state of the rnn using the initial_state parameter.",
          "in tensorflow , you can wrap tensors in tf.variable () to keep their values in the graph between multiple session runs."
        ]
      },
      {
        "title": "36267491",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.rnn",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "for rnns specifically , there are two options : tf.nn.rnn , and tf.nn.dynamic_rnn.",
          "the first function creates t subgraphs , where t is the length of the python list of inputs you provide ( that is , inputs is a len t python list of shape [ batch , depth ] tensors ). ",
          "in this case , there is exactly one subgraph for the \" time step \", and it gets run over and over until your input has been processed."
        ]
      },
      {
        "title": "36950950",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.rnn",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9503999999999999",
        "sentences": [
          "use tf.nn.rnn or tf.nn.dynamic_rnn which do this , and a lot of other nice things , for you."
        ]
      },
      {
        "title": "40986014",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.rnn",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9215",
        "sentences": [
          "that means , if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 rnn steps.",
          "second , you ’ re unable to pass in longer sequences (> 200 ) than you ’ ve originally specified.",
          "tf.nn.dynamic_rnn solves this.",
          "it uses a tf.while loop to dynamically construct the graph when it is executed."
        ]
      },
      {
        "title": "42497900",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.rnn",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.8929999999999999",
        "sentences": [
          "you should use tf.nn.dynamic_rnn.",
          "fyi : what is the upside of using tf.nn.rnn instead of tf.nn.dynamic_rnn in tensorflow ? "
        ]
      },
      {
        "title": "51426029",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.rnn",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.8929999999999999",
        "sentences": [
          "this is still a useful resource ( despite being written a couple years ago ): http :// www.wildml.com / 2016 / 08 / rnns - in - tensorflow - a - practical - guide - and - undocumented - features / ",
          "in it , denny britz has the following comment on the static / dynamic issue : ",
          "static ",
          "internally , tf.nn.rnn creates an unrolled graph for a fixed rnn length.",
          "that means , if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 rnn steps.",
          "second , you ’ re unable to pass in longer sequences (> 200 ) than you ’ ve originally specified.",
          "dynamic ",
          "tf.nn.dynamic_rnn solves this.",
          "it uses a tf.while loop to dynamically construct the graph when it is executed."
        ]
      },
      {
        "title": "42627927",
        "h": "tf.nn.dynamic_rnn",
        "t": "tf.nn.rnn",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.67",
        "r_prob": "1.0",
        "prob": "0.6499",
        "sentences": [
          "as of version 1.0 of the api tf.nn.rnn was removed.",
          "try using tf.nn.dynamic_rnn."
        ]
      }
    ]
  },
  "tf.global_variable_initializer": {},
  "tf.train.saver": {
    "tf.all_variables": [
      {
        "title": "39134460",
        "h": "tf.all_variables",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "instead it provides a way to associate a graphdef with the weights stored in one or more checkpoint files , written by a tf.train.saver.",
          "in you training program , write out a checkpoint using a tf.train.saver.",
          "if you want to get the value of each variable , you can ( for example ) find the variable in tf.all_variables () collection and pass it to sess.run () to get its value.",
          "you could also filter tf.all_variables () to find the particular weights and biases that you ' re trying to extract from the model."
        ]
      },
      {
        "title": "43098373",
        "h": "tf.all_variables",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "the default behavior of tf.train.saver is to save ( or restore ) every variable in tf.all_variables () ( in addition to any other \" saveable objects \") using their name property as a key.",
          "you can override the mapping between variables in the checkpoint and tf.variable objects by passing a var_list argument to the tf.train.saver constructor as follows : "
        ]
      },
      {
        "title": "37039218",
        "h": "tf.all_variables",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.9114",
        "sentences": [
          "saver by default uses the list in ops.graphkeys.variables collection ( accessible through tf.all_variables ()), and if you restored from graphdef rather than using python api to build your model , that collection is empty.",
          "you could specify the list of variables manually in tf.train.saver ( var_list =[' myvariable1 : 0 ', ' myvariable2 : 0 ',...]). "
        ]
      },
      {
        "title": "35675700",
        "h": "tf.all_variables",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "0.87",
        "prob": "0.852687",
        "sentences": [
          "when , in cifar10_eval.py a tf.train.saver is created , it uses tf.all_variables (), which includes the implicitly - created variable from the tf.nn.string_input_producer (). "
        ]
      },
      {
        "title": "34458812",
        "h": "tf.all_variables",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "1.0",
        "r_prob": "0.99",
        "prob": "0.8217",
        "sentences": [
          "when you construct your inference graph , you should be able to construct a tf.train.saver () with no arguments , and it will construct the appropriate save and restore ops for you.",
          "the result of tf.all_variables ()) must be a subset of the variables in the training graph , and ( ii ) the corresponding variables must have exactly the same names."
        ]
      }
    ],
    "tf.train.import_meta_graph": [
      {
        "title": "40981970",
        "h": "tf.train.import_meta_graph",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.97",
        "r_prob": "0.99",
        "prob": "0.9506969999999999",
        "sentences": [
          "extract the model building code into a python function , and call it before creating the tf.train.saver in each program .. ",
          "use saver.export_meta_graph () to write out the graph structure along with a checkpoint in your first program , and tf.train.import_meta_graph () to import the graph structure ( and create an appropriately configure tf.train.saver instance ) in your second program .. "
        ]
      },
      {
        "title": "42641032",
        "h": "tf.train.saver",
        "t": "tf.train.import_meta_graph",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "are you running on same machine or different machine   saver = tf.train.saver () ",
          "the following comment is in tensorflow docs   note : restarting training from saved meta_graph only works if the device assignments have not changed.",
          "  saver = tf.train.import_meta_graph ( metafile ) "
        ]
      },
      {
        "title": "53453868",
        "h": "tf.train.import_meta_graph",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.8341999999999999",
        "sentences": [
          "the tf.train.import_meta_graph restores the graph , meaning rebuilds the network architecture that was stored to the file.",
          "the call to tf.train.saver.restore on the other hand only restores the variable values from the file to the current graph in the session ( this naturally fails if the some values of in the file belong to variables that do not exist in the currently active graph ). ",
          "so if you already build the network layers in the code , you don ' t need to call tf.train.import_meta_graph.",
          "otherwise this might be causing you problems."
        ]
      }
    ],
    "tf.train.write_graph": [
      {
        "title": "38950073",
        "h": "tf.train.write_graph",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "0.98",
        "prob": "0.931588",
        "sentences": [
          "by calling saver.save () on a tf.train.saver object ) contain only the weights , and any other variables defined in the same program.",
          "note that calling saver.save () also produces a file containing a metagraphdef , which contains a graph and details of how to associate the weights from a checkpoint with that graph.",
          "see the tutorial for more details.",
          "tf.train.write_graph () only writes the graph structure ; not the weights.",
          "a frozen graph can be loaded using tf.import_graph_def (). ",
          "in this case , the weights are ( typically ) embedded in the graph , so you don ' t need to load a separate checkpoint."
        ]
      },
      {
        "title": "40396570",
        "h": "tf.train.write_graph",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.8543999999999999",
        "sentences": [
          "a.",
          "the graphdef (*. pb ) using tf.train.write_graph : https :// www.tensorflow.org / versions / r0.11 / api_docs / cc / index.html ",
          "b.",
          "the weights (*. ckpt ) using tf.train.saver : https :// www.tensorflow.org / versions / r0.11 / api_docs / python / state_ops.html   saver "
        ]
      },
      {
        "title": "34516450",
        "h": "tf.train.saver",
        "t": "tf.train.write_graph",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.87",
        "r_prob": "0.99",
        "prob": "0.809622",
        "sentences": [
          "then you will be able to run a step by feeding a value for ( i ), and fetching the value for ( ii ). ",
          "one final concern is how to represent the model parameters in your exported graph.",
          "there are several ways to do this , including shipping a tensorflow checkpoint ( written by a tf.train.saver ) as part of your app , and running the restore ops to reload it."
        ]
      },
      {
        "title": "57490232",
        "h": "tf.train.write_graph",
        "t": "tf.train.saver",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.7979999999999999",
        "sentences": [
          "or is this whole approach wrong ? ",
          "a : the main problem is that tf.train.write_graph saves the tensorflow graph , but not the weights of your model.",
          "q : do i need to do use tf.train.saver ? ",
          "in addition to saving the graph ( which is only necessary if your subsequent scripts do not explicitly recreate it ), you should use tf.train.saver to save the weights of your model : ",
          "calling saver.save also saves a metagraphdef which can then be used to restore the graph , so it is not necessary for you to use tf.train.write_graph.",
          "to restore the weights , simply use saver.restore : ",
          "if you want to generate a single.pb file , please use the former tf.train.saver approach.",
          "save and restore in tensorflow .. ",
          "stackoverflow answer to tensorflow saving into / loading a graph from a file .. "
        ]
      }
    ],
    "tf.session": [
      {
        "title": "36286136",
        "h": "tf.train.saver",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.87",
        "r_prob": "0.96",
        "prob": "0.79344",
        "sentences": [
          "however , you exit the with block before creating ( i ) the tf.session , and ( ii ) the tf.train.saver.",
          "however , this can lead to name collisions between different cells in your ipython notebook , which is awkward when using the tf.train.saver , since it uses the name property of a tf.variable as the key in the checkpoint file.",
          "alternatively , you can create the tf.session inside the with graph.as_default (): block , in which case it will use graph for all of its operations."
        ]
      },
      {
        "title": "38627631",
        "h": "tf.train.saver",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.8",
        "r_prob": "0.91",
        "prob": "0.70616",
        "sentences": [
          "the tf.train.saver must be created after the variables that you want to restore ( or save ). ",
          "in addition , you must pass the new tf.graph that you created to the tf.session constructor so you ' ll need to move the creation of sess inside that with block as well."
        ]
      },
      {
        "title": "42739036",
        "h": "tf.train.saver",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.42",
        "r_prob": "0.99",
        "prob": "0.39501",
        "sentences": [
          "the tf.train.saver is a \" passive \" utility for writing checkpoints , and it only writes a checkpoint when some other code calls its.save () method.",
          "if you are using the low - level tensorflow api ( tf.session ) and writing your own training loop , you can simply insert calls to saver.save () in your own code."
        ]
      }
    ]
  },
  "tf.io.parse_tensor": {},
  "tf.multiply": {
    "tf.add": [
      {
        "title": "49977599",
        "h": "tf.add",
        "t": "tf.multiply",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9506",
        "sentences": [
          "c.f.",
          "tf.add () doc , or tf.multiply () doc , etc."
        ]
      },
      {
        "title": "39112432",
        "h": "tf.add",
        "t": "tf.multiply",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9409",
        "sentences": [
          "this looks like an unfortunate implementation detail of tf.random_uniform (): it currently uses tf.add () and tf.multiply () to rescale the random value from [- 1 , + 1 ] to [ minval , maxval ], but if the shape of minval or maxval is unknown , tf.add () and tf.multiply () can ' t infer the proper shapes , because there might be broadcasting involved."
        ]
      },
      {
        "title": "56833985",
        "h": "tf.add",
        "t": "tf.multiply",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.97",
        "r_prob": "0.85",
        "prob": "0.6843349999999999",
        "sentences": [
          "update : so , you want to initialize prevop with tf.add ( inp1 , 10 ) and then update it with values of op , which is tf.add ( tf.multiply ( prevop , inp2 ), inp1 ). "
        ]
      },
      {
        "title": "57469599",
        "h": "tf.multiply",
        "t": "tf.add",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.65",
        "r_prob": "0.79",
        "prob": "0.42107000000000006",
        "sentences": [
          "then c = tf.add ( b , 2.0 ): "
        ]
      }
    ],
    "tf.matmul": [
      {
        "title": "40672159",
        "h": "tf.matmul",
        "t": "tf.multiply",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.9216899999999999",
        "sentences": [
          "in addition to tf.reduce_sum ( tf.multiply ( x , y )), you can also do tf.matmul ( x , tf.reshape ( y , [- 1 , 1 ])). "
        ]
      },
      {
        "title": "47647175",
        "h": "tf.multiply",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.88",
        "r_prob": "1.0",
        "prob": "0.8096",
        "sentences": [
          "you are trying to do matrix multiplication.",
          "so you should make use of tf.matmul.",
          "the operation of tf.multiply does elementwise multiplication for which both the tensor ' s shapes must be same."
        ]
      },
      {
        "title": "34908326",
        "h": "tf.matmul",
        "t": "tf.multiply",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.86",
        "r_prob": "1.0",
        "prob": "0.8083999999999999",
        "sentences": [
          "the tf.matmul () op requires that both of its inputs are matrices ( i.e.",
          "a 1 - d tensor ), which is the source of the error.",
          "by contrast , the * operator ( an alias for tf.multiply ()) is a broadcasting element - wise multiplication.",
          "* this was true when i posted the answer at first , but now tf.matmul () also supports batched matrix multiplications.",
          "see the documentation for more details."
        ]
      },
      {
        "title": "47583685",
        "h": "tf.matmul",
        "t": "tf.multiply",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.79002",
        "sentences": [
          "what tf.multiply ( x , x ) does is essentially multiplying each element of the matrix with itself , like ",
          "just put it down with variable names [[ a b ] [ c d ]] instead of actual numbers and look at what does tf.matmul ( x , x ) and tf.multiply ( x , x ) do."
        ]
      },
      {
        "title": "55401848",
        "h": "tf.multiply",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.83",
        "r_prob": "0.99",
        "prob": "0.65736",
        "sentences": [
          "the tf.matmul operator performs a matrix multiplication , which means that each element in the resulting matrix is a sum of products ( which corresponds exactly to what you describe ). ",
          "x = [ 2 , 3 , 1 ] y = [ 3 , 1 , 2 ] ",
          "then the result would be : ",
          "tf.matmul ( x , y ) = 2 * 3 + 3 * 1 + 1 * 2 = 11 ",
          "there you can see the weighted sum.",
          "p.s : tf.multiply performs element - wise multiplication , which is not what we want here."
        ]
      },
      {
        "title": "46599680",
        "h": "tf.multiply",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.66",
        "r_prob": "0.99",
        "prob": "0.6011280000000001",
        "sentences": [
          "please use tf.matmul instead of tf.multiply in your pred equation.",
          "tf.multiply does a element wise multiplication hence , it will generate a matrix of same dimension as train_x , whereas tf.matmul will do a matrix multiplication and will generate the resultant matrix based on the actual matrix multiplication rule."
        ]
      },
      {
        "title": "47556275",
        "h": "tf.multiply",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.55",
        "r_prob": "0.97",
        "prob": "0.5174949999999999",
        "sentences": [
          "but , hope the following helps : ",
          "the * or tf.multiply does element - wise multiplication.",
          "use tf.matmul "
        ]
      },
      {
        "title": "47510518",
        "h": "tf.multiply",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.62",
        "t_prob": "0.75",
        "r_prob": "1.0",
        "prob": "0.46499999999999997",
        "sentences": [
          "and you did not include the squared error in the second sum.",
          "using * and tf.multiply is the same.",
          "( for matrix multiplication , you ought to use tf.matmul ) "
        ]
      },
      {
        "title": "43633250",
        "h": "tf.multiply",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.63",
        "r_prob": "0.7",
        "prob": "0.33957",
        "sentences": [
          "did you mean to use tf.matmul instead of the combination of tf.reduce_sum and tf.multiply ? "
        ]
      }
    ]
  },
  "tf.train.monitoredtrainingsession": {},
  "tf.train.scaffold": {},
  "tf.compat.v1.reset_default_graph": {},
  "tf.compat.v2.name_scope": {},
  "tf.compat.v1.variable_scope": {},
  "tf.nn.nce_loss": {},
  "tf.train.ftrloptimizer": {},
  "objective function": {},
  "module": {
    "tf.data.dataset": [
      {
        "title": "53320326",
        "h": "module",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.0",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.0",
        "sentences": [
          "when you use the tf.data.dataset module , it actually defines an input graph which is independant from the model graph.",
          "what happens here is that you first created a small graph by calling tf.data.dataset.from_tensor_slices (), then the estimator api created a second graph by calling dataset.make_one_shot_iterator () automatically."
        ]
      },
      {
        "title": "57057141",
        "h": "tf.data.dataset",
        "t": "module",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.0",
        "r_prob": "0.99",
        "prob": "0.0",
        "sentences": [
          "you can change your batch_size or using smarter ways to input your training data ( such as tf.data.dataset and using cache ). "
        ]
      },
      {
        "title": "54897167",
        "h": "tf.data.dataset",
        "t": "module",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.0",
        "r_prob": "0.94",
        "prob": "0.0",
        "sentences": [
          "the tf.data nodule has specific tools which help in building a input pipeline for your ml model.",
          "a input pipeline takes in the raw data , processes it and then feeds it to the model .. ",
          "when should i use tf.data module ? ",
          "there are two options to use tf.placeholder () or tf.data.dataset.",
          "the tf.data.dataset is a much easier implementation.",
          "this process would consume more time feeding in the data .. "
        ]
      }
    ]
  },
  "discrete": {},
  "tf.contrib.rnn.multirnncell": {},
  "tf.contrib.rnn.lstmblockcell": {},
  "tf.nn.state_saving_rnn": {},
  "tf.global_variable_": {},
  "tf.keras.layers.grucell": {},
  "tf.nn.rnn_": {},
  "tf.contrib.ffmpeg.decode_audio": {},
  "tf.contrib.ffmpeg.encode_audio": {},
  "tf.range": {},
  "tf.fun": {},
  "pooling layer": {},
  "tf.truncated_normal": {},
  "return": {},
  "tf.contrib.lookup.index_table_from_tensor": {},
  "tf.lookup": {},
  "tf.image.pad_to_bounding_box": {},
  "type": {},
  "tf.tile": {
    "tf.expand_dims": [
      {
        "title": "55566010",
        "h": "tf.expand_dims",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9211999999999999",
        "sentences": [
          "you can use tf.tile and tf.expand_dims with tf.concat."
        ]
      },
      {
        "title": "47644484",
        "h": "tf.expand_dims",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.88",
        "r_prob": "1.0",
        "prob": "0.8536",
        "sentences": [
          "to port over to tensorflow , we have the counterparts there : ",
          "tf.expand_dims and tf.tile."
        ]
      },
      {
        "title": "51146877",
        "h": "tf.expand_dims",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.85",
        "r_prob": "1.0",
        "prob": "0.8075",
        "sentences": [
          "you have to do it manually.",
          "w_t = tf.tile ( tf.expand_dims ( w , 0 ),[ 3 , 1 , 1 ]) "
        ]
      },
      {
        "title": "49710967",
        "h": "tf.tile",
        "t": "tf.expand_dims",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.89",
        "r_prob": "0.99",
        "prob": "0.7401239999999999",
        "sentences": [
          "documentation : ",
          "tf.expand_dims.",
          "tf.tile."
        ]
      },
      {
        "title": "41503526",
        "h": "tf.expand_dims",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.83",
        "r_prob": "1.0",
        "prob": "0.7387",
        "sentences": [
          "so your problem seems to be somewhere else , maybe you have a multiplication by zero somewhere which kills the gradients.",
          "it is not listed in the documentation and i think you can only see it by checking the source code or using the operation and see if you get the mentioned error when you try to optimize the model.",
          "for tf.tile and tf.expand_dims there are gradients defined."
        ]
      },
      {
        "title": "47133461",
        "h": "tf.expand_dims",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.78",
        "r_prob": "1.0",
        "prob": "0.7176",
        "sentences": [
          "to get the cartesian product of the two , i would use a combination of tf.expand_dims and tf.tile : "
        ]
      },
      {
        "title": "48404009",
        "h": "tf.expand_dims",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.78",
        "r_prob": "1.0",
        "prob": "0.6942",
        "sentences": [
          "you can obtain the desired output with a combination of tf.concat , tf.tile and tf.expand_dims : ",
          "tf.expand_dims adds a dimension we can then concat on "
        ]
      },
      {
        "title": "50808290",
        "h": "tf.expand_dims",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.65",
        "r_prob": "1.0",
        "prob": "0.6045",
        "sentences": [
          "1.",
          "to answer your first question ",
          "you could use a combination of tf.expand_dims , tf.tile : "
        ]
      },
      {
        "title": "55947159",
        "h": "tf.expand_dims",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.43",
        "r_prob": "1.0",
        "prob": "0.40419999999999995",
        "sentences": [
          "instead of tiling using tf.tile which actually creates a new tensor by copying the original tensor that many times , you could use tf.expand_dims which would be more performant in terms of memory."
        ]
      }
    ],
    "tf.concat": [
      {
        "title": "43372777",
        "h": "tf.tile",
        "t": "tf.concat",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7968",
        "sentences": [
          "you can use tf.concat () to concatenates the list of tensors , tf.tile () to creates a new tensor by replicating input multiples times , tf.reshape () "
        ]
      },
      {
        "title": "54902443",
        "h": "tf.concat",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.92",
        "r_prob": "0.99",
        "prob": "0.792396",
        "sentences": [
          "you can achieve this with tf.tile or tf.concat : "
        ]
      },
      {
        "title": "48404009",
        "h": "tf.concat",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.59",
        "t_prob": "0.78",
        "r_prob": "0.99",
        "prob": "0.455598",
        "sentences": [
          "you can obtain the desired output with a combination of tf.concat , tf.tile and tf.expand_dims : ",
          "tf.expand_dims adds a dimension we can then concat on ",
          "finally , tf.concat stitches together the two tensors giving the desired result."
        ]
      }
    ],
    "tf.reshape": [
      {
        "title": "43372777",
        "h": "tf.tile",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.7885",
        "sentences": [
          "you can use tf.concat () to concatenates the list of tensors , tf.tile () to creates a new tensor by replicating input multiples times , tf.reshape () "
        ]
      },
      {
        "title": "35367161",
        "h": "tf.tile",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7776000000000001",
        "sentences": [
          "you can achieve the effect of np.repeat () using a combination of tf.tile () and tf.reshape (): ",
          "you can simply compute jdx using tf.tile (): "
        ]
      },
      {
        "title": "44173148",
        "h": "tf.tile",
        "t": "tf.reshape",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.76",
        "r_prob": "0.52",
        "prob": "0.35568000000000005",
        "sentences": [
          "you should use tf.tile , not concat.",
          "if you need your tensor to have a slightly different shape , read about the second parameter in tile function and also use tf.reshape "
        ]
      },
      {
        "title": "44490169",
        "h": "tf.reshape",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.44",
        "r_prob": "0.86",
        "prob": "0.32920799999999995",
        "sentences": [
          "sadly the same function is not still implemented in tf.",
          "you can implement it by combining tf.tile , tf.reshape , tf.squeeze."
        ]
      }
    ],
    "tf.shape": [
      {
        "title": "45018288",
        "h": "tf.shape",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.98",
        "r_prob": "0.86",
        "prob": "0.7753760000000001",
        "sentences": [
          "this should work , ",
          "tf.ones ([ tf.shape ( a )[ 0 ], 1 , 1 ]) * a ",
          "also using tf.tile , we can obtain the same : ",
          "tf.tile ( tf.expand_dims ( a , 0 ), [ tf.shape ( a )[ 0 ], 1 , 1 ]) "
        ]
      },
      {
        "title": "36042389",
        "h": "tf.shape",
        "t": "tf.tile",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.75",
        "r_prob": "1.0",
        "prob": "0.6900000000000001",
        "sentences": [
          "the most general solution is to use the tf.shape () op to get the run - time size of the placeholder , and the tf.tile () op to expand h to the appropriate size : "
        ]
      },
      {
        "title": "48190364",
        "h": "tf.tile",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.58",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.5627159999999999",
        "sentences": [
          "solution is to use a combination of tf.shape ( which returns the shape at runtime ) and tf.tile ( which accepts the dynamic shape ). "
        ]
      },
      {
        "title": "49550420",
        "h": "tf.tile",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.92",
        "r_prob": "0.56",
        "prob": "0.4430720000000001",
        "sentences": [
          "there is a function in tensorflow that returns dynamic shape : tf.shape.",
          "this function returns tensor that will evaluate to actual value of a shape.",
          "also there are two functions that you may find useful for your case : tf.pad and tf.tile."
        ]
      }
    ]
  },
  "tf.nn.atrous_conv2d": {},
  "tf.rege": {},
  "tf.sparse_matmul": {},
  "tf.contrib.seq2seq.attentionwrapper": {},
  "learning rates": {
    "learning rate": [
      {
        "title": "47242482",
        "h": "learning rates",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.74",
        "t_prob": "0.74",
        "r_prob": "0.93",
        "prob": "0.509268",
        "sentences": [
          "after playing around with different learning rates ( and compensating with more epochs ), i got the following : "
        ]
      },
      {
        "title": "37432424",
        "h": "learning rates",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.52",
        "t_prob": "0.87",
        "r_prob": "0.91",
        "prob": "0.41168400000000005",
        "sentences": [
          "vanilla sgd needs ( accepts ) individual adaption of the learning rate."
        ]
      },
      {
        "title": "59864516",
        "h": "learning rate",
        "t": "learning rates",
        "r": "S2",
        "h_prob": "0.67",
        "t_prob": "0.6",
        "r_prob": "0.99",
        "prob": "0.39798",
        "sentences": [
          "refer to this answer for more details on adaptive learning rates."
        ]
      },
      {
        "title": "58434402",
        "h": "learning rate",
        "t": "learning rates",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.59",
        "r_prob": "0.82",
        "prob": "0.30479399999999995",
        "sentences": [
          "1 ) you can use adaptive learning rate ( exponential decay or step dependent may work for you ) furthermore , you can try extreme high learning rates when your model goes into local minimum."
        ]
      },
      {
        "title": "54119442",
        "h": "learning rate",
        "t": "learning rates",
        "r": "S2",
        "h_prob": "0.62",
        "t_prob": "0.71",
        "r_prob": "0.64",
        "prob": "0.281728",
        "sentences": [
          "following the comment by @ gerges dib , i tried out different learning rates in increasing order.",
          "it looks like the plateau was caused by the optimizer ' s learning rate being too low."
        ]
      },
      {
        "title": "60787567",
        "h": "learning rates",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.51",
        "t_prob": "0.83",
        "r_prob": "0.52",
        "prob": "0.220116",
        "sentences": [
          "using big learning rates at the end of training can cause plateauing or convergence issues.",
          "in the image , you can see that for learning rate = 0.1 it reaches high accuracy very fast but then plateaus and drops in accuracy.",
          "for a learning rate = 0.001 , it reaches high accuracy slower but is continuously increasing."
        ]
      }
    ]
  },
  "tf.ensi": {},
  "tf.l2_loss": {},
  "tf.compat.v1.nn.rnn_cell.dropoutwrapper": {},
  "tf.keras.backend.get_value": {},
  "tf.keras.backend.mean": {},
  "tf.layers.conv2d_transpose": {},
  "tf.tensorarray": {
    "tf.while_loop": [
      {
        "title": "55680074",
        "h": "tf.while_loop",
        "t": "tf.tensorarray",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.931095",
        "sentences": [
          "take a look at this example that does what you want using tf.while_loop () with tf.tensorarray and tf.slice () function : "
        ]
      },
      {
        "title": "43592281",
        "h": "tf.while_loop",
        "t": "tf.tensorarray",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.7426",
        "sentences": [
          "while the tf.map_fn () does use tf.tensorarray objects internally , and a tf.tensorarray can hold objects of different size , this program won ' t work as - is because tf.map_fn () converts its tf.tensorarray result back to a tf.tensor by stacking the elements together , and it is this operation that fails.",
          "you can however implement the tf.tensorarray - based using the lower - lever tf.while_loop () op instead : "
        ]
      },
      {
        "title": "55886502",
        "h": "tf.while_loop",
        "t": "tf.tensorarray",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.78",
        "r_prob": "1.0",
        "prob": "0.7098000000000001",
        "sentences": [
          "you could do something more complicated with a tf.while_loop and tf.tensorarray , but i suspect there would be an overhead involved that would make things more expensive for small problems ( and the code complexity would be non - trivial ). "
        ]
      },
      {
        "title": "54906367",
        "h": "tf.tensorarray",
        "t": "tf.while_loop",
        "r": "S1",
        "h_prob": "0.76",
        "t_prob": "0.86",
        "r_prob": "0.72",
        "prob": "0.47059199999999995",
        "sentences": [
          "use tf.tensorarray : ",
          "or tf.scan ( which is implemented using tf.while_loop and tf.tensorarray ): "
        ]
      }
    ]
  },
  "observations": {},
  "tf.sigmoid": {
    "tf.nn.softmax": [
      {
        "title": "50090357",
        "h": "tf.nn.softmax",
        "t": "tf.sigmoid",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "tf.nn.softmax () computes probability distribution over classes ( output neurons ), if you have just 1 output neuron then probability distribution over 1 neuron will always be 1.0.",
          "i would suggest to use tf.sigmoid () in combination with tf.greater (), e.g : "
        ]
      },
      {
        "title": "36481418",
        "h": "tf.nn.softmax",
        "t": "tf.sigmoid",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.9114",
        "sentences": [
          "alternatively you could change your code is to use tf.nn.softmax instead of tf.sigmoid."
        ]
      },
      {
        "title": "47761718",
        "h": "tf.sigmoid",
        "t": "tf.nn.softmax",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.874",
        "sentences": [
          "instead of tf.nn.softmax , you could also use tf.sigmoid on a single logit , then set the other output to one minus that."
        ]
      }
    ]
  },
  "generator": {
    "tf.data.dataset": [
      {
        "title": "50181132",
        "h": "generator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.0",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.0",
        "sentences": [
          "better yet , if you ' re distributing your training you can reduce to the set of servers who need some shard of your final dataset.",
          "the tensorflow dataset will provide you with an iterator that ' s accessed directly by the graph so there ' s no need for tf.placeholders or marshaling data outside of the tf.data.dataset.from_generator () code you write."
        ]
      },
      {
        "title": "56602867",
        "h": "generator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.0",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.0",
        "sentences": [
          "instead of passing the pandas dataframes arguments for the generator function in the args parameter in the tf.data.dataset.from_generator method , i used lambda to pass them in the generator function itself : "
        ]
      },
      {
        "title": "50278981",
        "h": "generator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.0",
        "t_prob": "0.98",
        "r_prob": "0.98",
        "prob": "0.0",
        "sentences": [
          "it may be possible to use tf.data.dataset.from_generator () for this case.",
          "you could convert this to a tf.data.dataset with the following code : "
        ]
      },
      {
        "title": "52989446",
        "h": "generator",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.0",
        "t_prob": "0.97",
        "r_prob": "0.96",
        "prob": "0.0",
        "sentences": [
          "i found a way to do it using tf.data.from_generator the trick i found was to make two separate dataset ( one for mat file and one for the jpg file ) and then to combine them using tf.data.dataset.zip "
        ]
      },
      {
        "title": "49837908",
        "h": "tf.data.dataset",
        "t": "generator",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.0",
        "r_prob": "0.74",
        "prob": "0.0",
        "sentences": [
          "if you cannot , consider using tf.data.dataset.from_generator.",
          "to augment your dataset , use tf.data.dataset.map either before or after the batch operation , depending on whether or not you want to apply a batch - wise operation ( something working on a 4d image tensor ) or element - wise operation ( 3d image tensor ). ",
          "see the article for notes on efficiencies."
        ]
      }
    ]
  },
  "tf.conrib.layers": {},
  "dense layer": {
    "activation function": [
      {
        "title": "62196784",
        "h": "dense layer",
        "t": "activation function",
        "r": "S2",
        "h_prob": "0.8",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.656",
        "sentences": [
          "it is meant to be used as a layer inside your model , not as a parameter of your dense layer.",
          "to have a function that works as an activation to give as a parameter to dense , you should use tf.keras.activations.relu."
        ]
      },
      {
        "title": "47415774",
        "h": "dense layer",
        "t": "activation function",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.64",
        "r_prob": "1.0",
        "prob": "0.49920000000000003",
        "sentences": [
          "the activation function in a dense layer is applied after the multiplication with the weights and the addition of the bias."
        ]
      },
      {
        "title": "60376294",
        "h": "dense layer",
        "t": "activation function",
        "r": "S2",
        "h_prob": "0.66",
        "t_prob": "0.76",
        "r_prob": "0.97",
        "prob": "0.48655200000000004",
        "sentences": [
          "you cannot really just use the logarithm as an activation function , as it is not defined for values x <= 0.0 , so if at any point the dense layer produces a negative or zero value , the logarithm will produce nan , which then propagates to the loss."
        ]
      }
    ]
  },
  "one_hot": {},
  "tf.contrib.eager.checkpoint": {},
  "tf.compat.v2.data.dataset": {},
  "tf.data.options": {},
  "tf.data.experimental": {},
  "apply": {},
  "deep networks": {},
  "tf.decode_raw": {},
  "tf.train.supervisor": {},
  "step size": {},
  "hyperparameters": {},
  "tf.nn.embedd": {},
  "tf.readerbase": {},
  "tf.queuebase": {},
  "tf.python_io.tfrecordwriter": {},
  "training set": {
    "batch size": [
      {
        "title": "62962498",
        "h": "batch size",
        "t": "training set",
        "r": "S2",
        "h_prob": "0.94",
        "t_prob": "0.9",
        "r_prob": "0.9",
        "prob": "0.7614",
        "sentences": [
          "the default batch size in model.fit is 32 ( documentation ). "
        ]
      },
      {
        "title": "45149909",
        "h": "batch size",
        "t": "training set",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.78",
        "r_prob": "1.0",
        "prob": "0.6318",
        "sentences": [
          "your accuracy is probably affected more by the size of your training set."
        ]
      },
      {
        "title": "61032181",
        "h": "batch size",
        "t": "training set",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.85",
        "r_prob": "0.74",
        "prob": "0.5283599999999999",
        "sentences": [
          "second with a batch size of 50 , 000 all 50 , 000 samples reside in memory.if you are working with images for example that would take up an enormous amount of memory and probably lead to a resource exhaust error.",
          "divide your training set into groups of 1000 samples , so you will have 50 groups ( batches ). ",
          "so it is best to pick a moderate batch size."
        ]
      },
      {
        "title": "57255064",
        "h": "batch size",
        "t": "training set",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.88",
        "r_prob": "0.71",
        "prob": "0.5185839999999999",
        "sentences": [
          "for the training set , you use the number of samples of the training set and divide it for the training batch size , and for the validation set , you divide the number of samples in the validation set with the validation batch size."
        ]
      },
      {
        "title": "58356960",
        "h": "batch size",
        "t": "training set",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.78",
        "r_prob": "0.54",
        "prob": "0.35802000000000006",
        "sentences": [
          "you can edit the config file to change the values for batch size and number of steps.",
          "number of epochs trained = ( number of images in training set / batch size )* num_steps "
        ]
      }
    ]
  },
  "tf.exp": {},
  "training": {
    "batch size": [
      {
        "title": "51810785",
        "h": "batch size",
        "t": "training",
        "r": "S2",
        "h_prob": "0.88",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.8184",
        "sentences": [
          "you need to have a larger batch size.",
          "see tradeoff batch size vs.number of iterations to train a neural network to get a better idea on choosing a more accurate value."
        ]
      },
      {
        "title": "50554516",
        "h": "batch size",
        "t": "training",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.782",
        "sentences": [
          "you set your batch size as 1 in your tensorflow pipeline during training but feeding in 500 batch size in your testing data.",
          "you can either set your training batch size 500 or testing batch size as 1."
        ]
      }
    ]
  },
  "sequential": {},
  "tf.control_flow_": {},
  "tf.contrib.cudnn_rnn.cudnnlstm": {},
  "tf.reduce_sum": {
    "tf.nn.softmax": [
      {
        "title": "50932897",
        "h": "tf.nn.softmax",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9408",
        "sentences": [
          "your formula for cross entropy is wrong.",
          "you can create a distribution by using tf.nn.softmax which uses the exponential function to make all the values positive.",
          "logits = neural_network_model ( x ) ",
          "predicted_probabilities = tf.nn.softmax ( logits ) ",
          "cross_entropy = tf.reduce_mean (- tf.reduce_sum ( y_ * tf.log ( predicted_probabilities ), reduction_indices =[ 1 ])) ",
          "a much better way of doing it is to use the builtin softmax cross entropy functions in tensorflow."
        ]
      },
      {
        "title": "43633250",
        "h": "tf.nn.softmax",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.9",
        "r_prob": "0.99",
        "prob": "0.87318",
        "sentences": [
          "the problem arises because you call tf.reduce_sum on the argument of tf.nn.softmax.",
          "as a result , the softmax function fails because a scalar is not a valid input argument.",
          "did you mean to use tf.matmul instead of the combination of tf.reduce_sum and tf.multiply ? "
        ]
      },
      {
        "title": "42800217",
        "h": "tf.nn.softmax",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.96",
        "r_prob": "0.9",
        "prob": "0.8553599999999999",
        "sentences": [
          "cost_v2 = tf.reduce_mean (- tf.reduce_sum ( y * tf.log ( tf.nn.softmax ( pred )), 1 )) "
        ]
      }
    ],
    "tf.square": [
      {
        "title": "46831346",
        "h": "tf.reduce_sum",
        "t": "tf.square",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.96",
        "r_prob": "0.96",
        "prob": "0.903168",
        "sentences": [
          "it is because you sum before taking the mean , so you get the squared error and not its mean.",
          "change tf.reduce_mean ( tf.reduce_sum ( tf.square ( tf.subtract ( y , y_ )))) to tf.reduce_mean (( tf.square ( tf.subtract ( y , y_ ))) "
        ]
      },
      {
        "title": "44400232",
        "h": "tf.square",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.99",
        "r_prob": "0.95",
        "prob": "0.8464499999999999",
        "sentences": [
          "just a little bit of linear algebra and you have your solution : ",
          "which means that you need to do : tf.reduce_sum ( tf.square ( a ), axis = 0 ) "
        ]
      },
      {
        "title": "48805042",
        "h": "tf.reduce_sum",
        "t": "tf.square",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.85",
        "r_prob": "0.97",
        "prob": "0.8080099999999999",
        "sentences": [
          "however , you might want to consider changing your loss from tf.sqrt ( tf.reduce_sum ( tf.square ( imgs - decoder_op ))) to tf.reduce_sum ( tf.square ( imgs - decoder_op )). "
        ]
      },
      {
        "title": "58390952",
        "h": "tf.square",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.75",
        "t_prob": "0.94",
        "r_prob": "0.94",
        "prob": "0.6627",
        "sentences": [
          "model.add_loss ( lambda : penalty * tf.reduce_sum ( tf.square ( layer.trainable_variables [ 0 ]) "
        ]
      },
      {
        "title": "55572669",
        "h": "tf.square",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.84",
        "r_prob": "0.7",
        "prob": "0.5585999999999999",
        "sentences": [
          "we thus get , which is : tf.square ( tf.sub ( target_out [ layer ], cont_out )). ",
          "this is why we sum all the difference into a single scalar using tf.reduce_sum."
        ]
      }
    ],
    "tf.log": [
      {
        "title": "50932897",
        "h": "tf.log",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.8928",
        "sentences": [
          "your formula for cross entropy is wrong.",
          "you can create a distribution by using tf.nn.softmax which uses the exponential function to make all the values positive.",
          "predicted_probabilities = tf.nn.softmax ( logits ) ",
          "cross_entropy = tf.reduce_mean (- tf.reduce_sum ( y_ * tf.log ( predicted_probabilities ), reduction_indices =[ 1 ])) ",
          "a much better way of doing it is to use the builtin softmax cross entropy functions in tensorflow."
        ]
      },
      {
        "title": "59979540",
        "h": "tf.log",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.89",
        "r_prob": "0.98",
        "prob": "0.811146",
        "sentences": [
          "tensorflow 1.x : ",
          "cross_entropy = - tf.reduce_sum ( y_ * tf.log ( tf.clip_by_value ( y_conv , 1e - 10 , 1.0 ))) "
        ]
      },
      {
        "title": "34764585",
        "h": "tf.log",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.85",
        "r_prob": "0.99",
        "prob": "0.7910099999999999",
        "sentences": [
          "tf_cross_entropy = - tf.reduce_sum ( tf_softmax_correct * tf.log ( tf_softmax )) ",
          "if you replace this with : ",
          "tf_cross_entropy = - tf.reduce_sum ( tf_softmax_correct * tf.log ( tf_softmax + 1e - 50 )) it should avoid the problem.",
          "if you double the batch size and use reduce_sum it will double the cost ( and the magnitude of the gradient ). ",
          "specifically this is what i ' m using now when debugging : ",
          "cross_entropy = - tf.reduce_sum ( y * tf.log ( model + 1e - 50 ))    avoid nan due to 0 * log ( 0 ) cross_entropy = tf.print ( cross_entropy , [ cross_entropy ], \" cost \")   print to the console tensorflow was started from "
        ]
      },
      {
        "title": "33645235",
        "h": "tf.log",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.81",
        "r_prob": "0.99",
        "prob": "0.657558",
        "sentences": [
          "cross_entropy = - tf.reduce_sum ( y_ * tf.log ( y )) ",
          "cross_entropy = - tf.reduce_mean ( y_ * tf.log ( y )) ",
          "well , if we sum , then doubling the batch size doubles the cost , and also doubles the magnitude of the gradient."
        ]
      },
      {
        "title": "42800217",
        "h": "tf.reduce_sum",
        "t": "tf.log",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.95",
        "r_prob": "0.6",
        "prob": "0.5471999999999999",
        "sentences": [
          "cost_v2 = tf.reduce_mean (- tf.reduce_sum ( y * tf.log ( tf.nn.softmax ( pred )), 1 )) "
        ]
      }
    ],
    "tf.reduce_mean": [
      {
        "title": "43825740",
        "h": "tf.reduce_sum",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.93",
        "r_prob": "0.96",
        "prob": "0.80352",
        "sentences": [
          "also tf.reduce_sum ( cost ) will do what you want , i think it is better to use tf.reduce_mean (). ",
          "here are a few reasons why : ",
          "on average you will get reduce_sum 4 times bigger for a two times bigger matrix."
        ]
      },
      {
        "title": "52774155",
        "h": "tf.reduce_sum",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.91",
        "r_prob": "0.89",
        "prob": "0.793702",
        "sentences": [
          "so in your example , you would do [ 0 , 1 , 1 , 0 , 0 , 1 ] for one class and [ 1 , 0 , 0 , 1 , 1 , 0 ] for the other.",
          "use tf.reduce_mean () on the correct axis to get the average of each class."
        ]
      },
      {
        "title": "42611524",
        "h": "tf.reduce_sum",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.91",
        "r_prob": "0.89",
        "prob": "0.7613059999999999",
        "sentences": [
          "you can use tf.sqrt followed by tf.reduce_sum and tf.reduce_mean.",
          "both tf.reduce_sum and tf.reduce_mean have an axis argument that indicates which dimensions to reduce."
        ]
      },
      {
        "title": "46831346",
        "h": "tf.reduce_sum",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.97",
        "r_prob": "0.7",
        "prob": "0.66542",
        "sentences": [
          "it is because you sum before taking the mean , so you get the squared error and not its mean.",
          "change tf.reduce_mean ( tf.reduce_sum ( tf.square ( tf.subtract ( y , y_ )))) to tf.reduce_mean (( tf.square ( tf.subtract ( y , y_ ))) "
        ]
      },
      {
        "title": "45160944",
        "h": "tf.reduce_mean",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.93",
        "r_prob": "0.97",
        "prob": "0.658533",
        "sentences": [
          "for example if you choose to use tf.reduce_mean , then your loss is averaged on all the elements of your batch.",
          "and so is the gradient.",
          "if you use tf.reduce_sum , then your gradient will be the sum of all your gradients element - wise."
        ]
      },
      {
        "title": "43930885",
        "h": "tf.reduce_sum",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.96",
        "r_prob": "0.64",
        "prob": "0.602112",
        "sentences": [
          "if you use tf.reduce_sum () in the upper example , as you did in the lower one , you should be able to achieve similar results with both methods : cost = tf.reduce_mean ( tf.reduce_sum ( tf.nn.softmax_cross_entropy_with_logits ( logits = logits , labels = y ))). "
        ]
      },
      {
        "title": "43932289",
        "h": "tf.reduce_sum",
        "t": "tf.reduce_mean",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.96",
        "r_prob": "0.6",
        "prob": "0.55296",
        "sentences": [
          "from tensorflow api here the second way , cost = tf.reduce_mean ( tf.reduce_sum (- y * tf.log ( hypothesis ))) is numerically unstable , and because of that you can ' t get same results , "
        ]
      }
    ],
    "tf.matmul": [
      {
        "title": "48172033",
        "h": "tf.reduce_sum",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.7110000000000001",
        "sentences": [
          "using tf.matmul ( x , x , transpose_b = true ) means that you are calculating x.",
          "is the matrix multiplication.",
          "tf.reduce_sum ( _ , axis = 1 ) takes the sum along 1st axis ( starting counting with 0 ) which means you are suming the rows : "
        ]
      },
      {
        "title": "41233901",
        "h": "tf.reduce_sum",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.76",
        "t_prob": "0.83",
        "r_prob": "0.98",
        "prob": "0.6181840000000001",
        "sentences": [
          "if you have an operation that can be parallelized internally , such as matrix multiplication ( tf.matmul ()) or a reduction ( e.g.",
          "tf.reduce_sum ()), tensorflow will execute it by scheduling tasks in a thread pool with intra_op_parallelism_threads threads.",
          "this configuration option therefore controls the maximum parallel speedup for a single operation."
        ]
      },
      {
        "title": "52943237",
        "h": "tf.reduce_sum",
        "t": "tf.matmul",
        "r": "S1",
        "h_prob": "0.72",
        "t_prob": "0.66",
        "r_prob": "0.92",
        "prob": "0.437184",
        "sentences": [
          "here ' s one way to do it using implicit broadcasting and tf.reduce_sum : ",
          "and an alternative way using tf.matmul , tf.reshape and tf.transpose : "
        ]
      }
    ]
  },
  "tf.keras.layers": {
    "tf.layers": [
      {
        "title": "56147622",
        "h": "tf.layers",
        "t": "tf.keras.layers",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.9211999999999999",
        "sentences": [
          "this is the low level api."
        ]
      },
      {
        "title": "54718798",
        "h": "tf.keras.layers",
        "t": "tf.layers",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.8464",
        "sentences": [
          "since tensorflow 1.12 , tf.layers are merely wrappers around tf.keras.layers.",
          "a few examples : ",
          "convolutional tf.layers just inherit from the convolutional tf.keras.layers , see source code here : ",
          "the same is true for all core tf.layers , e.g.",
          "tf.keras is becoming the de - facto high - level api for tensorflow , therefore tf.layers are now just wrappers around tf.keras.layers."
        ]
      },
      {
        "title": "58538279",
        "h": "tf.layers",
        "t": "tf.keras.layers",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.97",
        "r_prob": "0.85",
        "prob": "0.750295",
        "sentences": [
          "so i faced the same error but discovered that my version of tensorflow ( which is 2.0 ) moved layers from the tf package ( tf.layers ) to tf.keras.",
          "an easy fix would be to replace tf.layers with tf.keras.layers "
        ]
      },
      {
        "title": "55413120",
        "h": "tf.keras.layers",
        "t": "tf.layers",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.6596000000000001",
        "sentences": [
          "as per official docs , tf.layers are wrappers around tf.keras.layers.",
          "convolutional layers in layers api inherit from tf.keras.layers."
        ]
      },
      {
        "title": "53534547",
        "h": "tf.keras.layers",
        "t": "tf.layers",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.9",
        "r_prob": "0.6",
        "prob": "0.5075999999999999",
        "sentences": [
          "the core functionality corresponding to tf.contrib.layers is in tf.layers.",
          "if your goal is to prepare your code for tf 2.0 , consider that tf.contrib will be removed entirely ( either split from tf or integrated into it ) and that tf.layers too will be removed and the high - level api will reside under tf.keras.",
          "so to best prepare for tf 2.0 you should start using tf.keras.layers instead."
        ]
      }
    ]
  },
  "tf.train.range_input_producer": {},
  "tf.session": {
    "tf.global_variables_initializer": [
      {
        "title": "45139678",
        "h": "tf.global_variables_initializer",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9702",
        "sentences": [
          "try adding init = tf.global_variables_initializer () before with tf.session () as sess :, this should fix the \" uninitialized value \" error.",
          "tf.global_variables_initializer () has to be called after your graph has been defined.",
          "always define init as your last operation before with tf.session () ... to make sure you don ' t miss anything in the initialization."
        ]
      },
      {
        "title": "44435910",
        "h": "tf.global_variables_initializer",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9602999999999999",
        "sentences": [
          "you have to initialize the variables.",
          "try moving tf.global_variables_initializer () inside tf.session () as sess : block and run it as tf.global_variables_initializer (). run () "
        ]
      },
      {
        "title": "53886128",
        "h": "tf.session",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "0.94",
        "prob": "0.921294",
        "sentences": [
          "tf.session () initiates a tensorflow graph object in which tensors are processed through operations ( or ops ). ",
          "these have to be initiated once the session is created.",
          "hence we call tf.global_variables_initializer (). run () ",
          "a graph contains tensors and operations.",
          "in other words , graph provides a schema whereas a session processes a graph to compute values ( tensors ). "
        ]
      },
      {
        "title": "51537951",
        "h": "tf.global_variables_initializer",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "1.0",
        "r_prob": "1.0",
        "prob": "0.91",
        "sentences": [
          "and with this follwing code works as expected : with tf.session () as sess : sess.run ( tf.global_variables_initializer ()) image , label = sess.run ( make_batch ( 1 )) "
        ]
      },
      {
        "title": "54997427",
        "h": "tf.session",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.86",
        "r_prob": "0.99",
        "prob": "0.8003159999999999",
        "sentences": [
          "option 2 : use tf.session () instead of the ' with ' block but add the line sess.run ( tf.global_variables_initializer ()): "
        ]
      },
      {
        "title": "61988339",
        "h": "tf.global_variables_initializer",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.81",
        "r_prob": "0.96",
        "prob": "0.723168",
        "sentences": [
          "tried using tensorflow 2 and everything fails.",
          "no more tf.session (), no more tf.global_variables_initializer (), etc.",
          "so no working tensorflow in ubuntu , you have to use docker."
        ]
      },
      {
        "title": "50047139",
        "h": "tf.session",
        "t": "tf.global_variables_initializer",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.88",
        "r_prob": "0.99",
        "prob": "0.7143839999999999",
        "sentences": [
          "executing functions which define the graph again will just build another subgraph ( this probably also goes for your function which defines the placeholder and variables ). ",
          "the tf.global_variables_initializer operation should also only be executed once.",
          "so in the notebook after initializing the graph exactly once you can only call functions which wrap tensorflow graph evaluation code , not graph building code dynamically without resetting the kernel.",
          "examples for such methods which only evaluate an existing graph are session.run , other tf.session methods or similar evaluation methods like tensor.eval."
        ]
      }
    ],
    "tf.interactivesession": [
      {
        "title": "41608016",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "1.0",
        "r_prob": "0.97",
        "prob": "0.9602999999999999",
        "sentences": [
          "some questions ",
          "first why you use sess = tf.interactivesession () and with tf.session () as sess : at same time , just curious "
        ]
      },
      {
        "title": "43871573",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.8918",
        "sentences": [
          "change line 61 from sess = tf.interactivesession () to sess = tf.session () and rerun it on the command line."
        ]
      },
      {
        "title": "49943920",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "0.82",
        "prob": "0.7955639999999999",
        "sentences": [
          "replace tf.interactivesession () with with tf.session (): statement."
        ]
      },
      {
        "title": "46894945",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.98",
        "r_prob": "0.97",
        "prob": "0.7889979999999999",
        "sentences": [
          "inside a with graph.as_default (): block .. ",
          "inside a with tf.session (): block .. ",
          "between creating a tf.interactivesession and calling sess.close ().. ",
          "each of these scenarios involves registering a default ( and potentially \" nested \") tf.graph object , which will be unregistered when you exit the block ( or close the tf.interactivesession ). ",
          "resetting the default graph in those scenarios would leave the system in an inconsistent state , so you should ensure to exit the block ( or close the tf.interactivesession ) before calling tf.reset_default_graph (). "
        ]
      },
      {
        "title": "53919276",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.79",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7584",
        "sentences": [
          "this function applies only to the current thread.",
          "calling this function while a tf.session or tf.interactivesession is active will result in undefined behavior."
        ]
      },
      {
        "title": "44897259",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7488",
        "sentences": [
          "you get an error because you use it in a session.",
          "calling this function while a tf.session or tf.interactivesession is active will result in undefined behavior."
        ]
      },
      {
        "title": "44360401",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.71",
        "r_prob": "0.99",
        "prob": "0.688842",
        "sentences": [
          "because when you use tf.interactivesession () to create a session , this interactivesession installs itself as the default session on construction.",
          "but if you are using tf.session (), you need to explicitly point out which session to use when run a operation.",
          "so if you use tf.session (), the code in your question is broken as the global variables initializer is not bound with your session."
        ]
      },
      {
        "title": "43118291",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.91",
        "r_prob": "0.84",
        "prob": "0.542724",
        "sentences": [
          "i want to give my resolution , it work when i replace the line [ session = tf.session ()] with [ sess = tf.interactivesession ()]. "
        ]
      },
      {
        "title": "34013098",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.87",
        "r_prob": "0.57",
        "prob": "0.44631",
        "sentences": [
          "the failedpreconditionerror arises because the program is attempting to read a variable ( named \" variable_1 \") before it has been initialized.",
          "in tensorflow , all variables must be explicitly initialized , by running their \" initializer \" operations.",
          "note that this answer assumes that , as in the question , you are using tf.interactivesession , which allows you to run operations without specifying a session.",
          "for non - interactive uses , it is more common to use tf.session , and initialize as follows : "
        ]
      }
    ],
    "tf.reset_default_graph": [
      {
        "title": "49943920",
        "h": "tf.reset_default_graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9408",
        "sentences": [
          "replace tf.interactivesession () with with tf.session (): statement.",
          "tf.reset_default_graph () won ' t free those resources while the session is active."
        ]
      },
      {
        "title": "53919276",
        "h": "tf.reset_default_graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9311999999999999",
        "sentences": [
          "tf.reset_default_graph will clear the default graph stack and resets the global default graph.",
          "this function applies only to the current thread.",
          "calling this function while a tf.session or tf.interactivesession is active will result in undefined behavior."
        ]
      },
      {
        "title": "44897259",
        "h": "tf.reset_default_graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.9119999999999999",
        "sentences": [
          "you get an error because you use it in a session.",
          "from the tf.reset_default_graph () documentation : ",
          "calling this function while a tf.session or tf.interactivesession is active will result in undefined behavior.",
          "tf.reset_default_graph () can be helpful ( at least for me ) during the testing phase while i experiment in jupyter notebook."
        ]
      },
      {
        "title": "46894945",
        "h": "tf.reset_default_graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9114",
        "sentences": [
          "this error message is displayed when you call tf.reset_default_graph () in one of the following scenarios : ",
          "inside a with graph.as_default (): block .. ",
          "inside a with tf.session (): block .. ",
          "resetting the default graph in those scenarios would leave the system in an inconsistent state , so you should ensure to exit the block ( or close the tf.interactivesession ) before calling tf.reset_default_graph (). "
        ]
      }
    ],
    "tf.device": [
      {
        "title": "59532982",
        "h": "tf.device",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.94",
        "r_prob": "0.99",
        "prob": "0.921294",
        "sentences": [
          "with tf.device (...): applies to the graph nodes created within the scope , not session.run calls.",
          "mainly you can stop using tf.session and tf.graph.",
          "use @ tf.function instead , i believe this basic structure will work : "
        ]
      },
      {
        "title": "36320000",
        "h": "tf.device",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.86",
        "r_prob": "1.0",
        "prob": "0.8428",
        "sentences": [
          "wrap large contiguous regions of your code in a with tf.device (...): block , naming the different gpus : "
        ]
      },
      {
        "title": "41495299",
        "h": "tf.device",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.8036",
        "sentences": [
          "use the device_count of configproto like tf.session ( config = tf.configproto ( device_count ={\" gpu \": 0 , \" cpu \": 1 })). "
        ]
      },
      {
        "title": "33961914",
        "h": "tf.session",
        "t": "tf.device",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.97",
        "r_prob": "0.95",
        "prob": "0.7832749999999999",
        "sentences": [
          "the tensorboard graph visualizer only sees the explicit device assignments that you have made in your program ( i.e.",
          "those made using with tf.device (\"...\"): blocks ). ",
          "a later placement stage runs inside the tensorflow backend , and assigns every node to a device.",
          "currently there is no support for this in tensorboard , but you can extract some information by creating the tf.session as follows : "
        ]
      }
    ],
    "tf.compat.v1.session": [
      {
        "title": "55143329",
        "h": "tf.session",
        "t": "tf.compat.v1.session",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.89",
        "r_prob": "0.99",
        "prob": "0.872289",
        "sentences": [
          "according to tf 1 : 1 symbols map , in tf 2.0 you should use tf.compat.v1.session () instead of tf.session () ",
          "to get tf 1.x like behaviour in tf 2.0 one can run "
        ]
      },
      {
        "title": "58621799",
        "h": "tf.session",
        "t": "tf.compat.v1.session",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.89",
        "r_prob": "0.99",
        "prob": "0.854667",
        "sentences": [
          "answer : in tf 2.0 you should use tf.compat.v1.session () instead of tf.session () use the following code to get rid of the error in tensorflow2.0 : "
        ]
      },
      {
        "title": "59390988",
        "h": "tf.session",
        "t": "tf.compat.v1.session",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.84",
        "r_prob": "0.97",
        "prob": "0.7414679999999999",
        "sentences": [
          "attributeerror : module ' tensorflow ' has no attribute ' session ' ",
          "you need to use tf.compat.v1.session () as tf.session is deprecated."
        ]
      },
      {
        "title": "54646355",
        "h": "tf.compat.v1.session",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.6298",
        "sentences": [
          "if you use the tf 2.0 upgrade script on your original tensorflow model , tf.session ( config =...) should change to tf.compat.v1.session ( config =...) and work as expected."
        ]
      },
      {
        "title": "61896836",
        "h": "tf.session",
        "t": "tf.compat.v1.session",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.67",
        "r_prob": "0.99",
        "prob": "0.610236",
        "sentences": [
          "if eager execution is disabled , you can build a graph and then run it through tf.compat.v1.session in tf 2.x and tf.session in tf 1.x "
        ]
      },
      {
        "title": "59432802",
        "h": "tf.session",
        "t": "tf.compat.v1.session",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.54",
        "r_prob": "0.92",
        "prob": "0.48189600000000005",
        "sentences": [
          "david valenzuela urrutia ' s answer was for python 3.6.3 , tensorflow 1.4.0 so i thought of updating the answer ( code samples ) to tensorflow 2.x because some funtionalities like tf.session is not supported in tensorflow version 2 so you need to replace it with tf.compat.v1.session for it to work."
        ]
      },
      {
        "title": "57562774",
        "h": "tf.session",
        "t": "tf.compat.v1.session",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.5",
        "r_prob": "0.85",
        "prob": "0.38675",
        "sentences": [
          "therefore , if you want you code to be compatible with tensorflow 2.0 , you should use tf.compat.v1.session instead.",
          "so , just change this line : "
        ]
      }
    ],
    "tf.graph": [
      {
        "title": "59532982",
        "h": "tf.session",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.95",
        "r_prob": "0.96",
        "prob": "0.8572799999999999",
        "sentences": [
          "mainly you can stop using tf.session and tf.graph.",
          "use @ tf.function instead , i believe this basic structure will work : "
        ]
      },
      {
        "title": "49318734",
        "h": "tf.graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.95",
        "r_prob": "0.87",
        "prob": "0.80997",
        "sentences": [
          "passing loaded_graph to the tf.session () means you can only run ops created in that graph.",
          "if you wish to have the for loop then you may need to on each loop create a new graph using ",
          "g_1 = tf.graph () with g_1.as_default (): ... "
        ]
      },
      {
        "title": "36286136",
        "h": "tf.graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.87",
        "r_prob": "0.97",
        "prob": "0.742632",
        "sentences": [
          "in in [ 8 ] you create a tf.graph called graph and set it as default for the with graph.as_default (): block.",
          "however , you exit the with block before creating ( i ) the tf.session , and ( ii ) the tf.train.saver.",
          "alternatively , you can create the tf.session inside the with graph.as_default (): block , in which case it will use graph for all of its operations."
        ]
      },
      {
        "title": "54783729",
        "h": "tf.session",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.87",
        "r_prob": "0.88",
        "prob": "0.72732",
        "sentences": [
          "estimators create and manage tf.graph and tf.session objects for you."
        ]
      },
      {
        "title": "47795685",
        "h": "tf.session",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.92",
        "r_prob": "0.77",
        "prob": "0.6588120000000001",
        "sentences": [
          "the estimator api manages the tf.graph and tf.session by itself , it uses an input_fn to feed all related ops with values."
        ]
      },
      {
        "title": "49279970",
        "h": "tf.session",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.69",
        "t_prob": "0.98",
        "r_prob": "0.96",
        "prob": "0.6491519999999998",
        "sentences": [
          "when you use dataset.map ( map_func ), tensorflow defines a subgraph for all the ops created in the function map_func , and arranges to execute it efficiently in the same session as the rest of your graph.",
          "there is almost never any need to create a tf.graph or tf.session inside map_func : if your parsing function is made up of tensorflow ops , these ops can be embedded directly in the graph that defines the input pipeline.",
          "if your map_func contains non - tensorflow operations that you want to apply to each element , you should wrap them in a tf.py_func () ( or dataset.from_generator (), if the data generation process is defined in python logic ). "
        ]
      },
      {
        "title": "47793706",
        "h": "tf.graph",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.66",
        "t_prob": "0.82",
        "r_prob": "0.77",
        "prob": "0.41672400000000004",
        "sentences": [
          "the kmeansclustering estimator api builds its own tf.graph and manage tf.session by itself , so you don ' t need to run a tf.session to feed values ( that is done by input_fn ), that ' s why the valueerror arise.",
          "here x is not a tf.placeholder that needs to be feed at a tf.session run."
        ]
      }
    ],
    "tf.configproto": [
      {
        "title": "48438660",
        "h": "tf.configproto",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.816",
        "sentences": [
          "edit : you can pass a tf.configproto () to your tf.session ( config =...) through which you can specify gpu usage."
        ]
      },
      {
        "title": "34390517",
        "h": "tf.session",
        "t": "tf.configproto",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.66",
        "r_prob": "1.0",
        "prob": "0.5346000000000001",
        "sentences": [
          "to configure this value , you can pass a tf.configproto argument when constructing the tf.session : "
        ]
      },
      {
        "title": "45662992",
        "h": "tf.configproto",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.64",
        "t_prob": "0.98",
        "r_prob": "0.77",
        "prob": "0.482944",
        "sentences": [
          "verify that tensorflow is running with gpu check if gpu is working.",
          "sess = tf.session ( config = tf.configproto ( log_device_placement = true )) "
        ]
      },
      {
        "title": "44971079",
        "h": "tf.configproto",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.7",
        "t_prob": "0.98",
        "r_prob": "0.57",
        "prob": "0.3910199999999999",
        "sentences": [
          "try using sess = tf.session ( config = tf.configproto ( allow_soft_placement = true , log_device_placement = true )). "
        ]
      },
      {
        "title": "41495299",
        "h": "tf.configproto",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.36",
        "t_prob": "0.98",
        "r_prob": "0.99",
        "prob": "0.349272",
        "sentences": [
          "use the device_count of configproto like tf.session ( config = tf.configproto ( device_count ={\" gpu \": 0 , \" cpu \": 1 })). "
        ]
      },
      {
        "title": "48502090",
        "h": "tf.configproto",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.49",
        "t_prob": "0.4",
        "r_prob": "0.95",
        "prob": "0.1862",
        "sentences": [
          "also check it tf.configproto.allow_soft_placement is set to true or false in your tf.session."
        ]
      }
    ],
    "tf.train.saver": [
      {
        "title": "36286136",
        "h": "tf.train.saver",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.87",
        "r_prob": "0.96",
        "prob": "0.79344",
        "sentences": [
          "however , you exit the with block before creating ( i ) the tf.session , and ( ii ) the tf.train.saver.",
          "however , this can lead to name collisions between different cells in your ipython notebook , which is awkward when using the tf.train.saver , since it uses the name property of a tf.variable as the key in the checkpoint file.",
          "alternatively , you can create the tf.session inside the with graph.as_default (): block , in which case it will use graph for all of its operations."
        ]
      },
      {
        "title": "38627631",
        "h": "tf.train.saver",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.8",
        "r_prob": "0.91",
        "prob": "0.70616",
        "sentences": [
          "the tf.train.saver must be created after the variables that you want to restore ( or save ). ",
          "in addition , you must pass the new tf.graph that you created to the tf.session constructor so you ' ll need to move the creation of sess inside that with block as well."
        ]
      },
      {
        "title": "42739036",
        "h": "tf.train.saver",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.42",
        "r_prob": "0.99",
        "prob": "0.39501",
        "sentences": [
          "the tf.train.saver is a \" passive \" utility for writing checkpoints , and it only writes a checkpoint when some other code calls its.save () method.",
          "if you are using the low - level tensorflow api ( tf.session ) and writing your own training loop , you can simply insert calls to saver.save () in your own code."
        ]
      }
    ],
    "tf.placeholder": [
      {
        "title": "37635555",
        "h": "tf.session",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.83",
        "r_prob": "0.98",
        "prob": "0.7320599999999999",
        "sentences": [
          "if you want to add some python it should be separated from the graph and added via placeholders ( tf.placeholder ). ",
          "on the other side , in the tf.session () part , you will call the graph results through sess.run (), providing the feed_dict values ( python objects , mostly arrays ). ",
          "in the first part , you define the tensorflow graph ( compiled by tensorflow ). ",
          "this is where all the heavy calculus should be .. ",
          "in the second part , the code you run is in python but calls the tensorflow graph through sess.run (), which is run by tensorflow ( and should contain most of the calculus ). "
        ]
      },
      {
        "title": "60509694",
        "h": "tf.placeholder",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.84",
        "t_prob": "0.94",
        "r_prob": "0.73",
        "prob": "0.5764079999999999",
        "sentences": [
          "your creating a new graph for each iteration (= calling xcross ). ",
          "you should redefine xcross so that it takes a tf.placeholder as input and define it outside the loop , even outside the with tf.session as sess :. "
        ]
      },
      {
        "title": "51929310",
        "h": "tf.session",
        "t": "tf.placeholder",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.97",
        "r_prob": "0.56",
        "prob": "0.5106080000000001",
        "sentences": [
          "they should only ever be called once and then ran repeatedly using a tf.session.",
          "you can use a tf.placeholder for the input ( file name ). "
        ]
      }
    ],
    "script": [
      {
        "title": "38853802",
        "h": "script",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.92",
        "r_prob": "0.82",
        "prob": "0.648784",
        "sentences": [
          "assuming sess is your tf.session () and \" output \" is the name of your prediction node , the following code will serialize your minimal graph both into textual and binary protobuf."
        ]
      },
      {
        "title": "58603627",
        "h": "tf.session",
        "t": "script",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.62",
        "r_prob": "0.75",
        "prob": "0.4557",
        "sentences": [
          "using tf.enable_eager_execution () or evaluating a tensor within a tf.session () context ",
          "the following script doesn ' t throw an error when using either pickle or dill : "
        ]
      },
      {
        "title": "51653363",
        "h": "tf.session",
        "t": "script",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.79",
        "r_prob": "0.51",
        "prob": "0.346494",
        "sentences": [
          "device memory is reserved when you instantiate a tf.session.",
          "although it is common in \" raw \" tensorflow to create the graph first then instantiate a session , it is nonetheless allowed to proceed differently , and it is actually common in the keras world where one would often start a script with "
        ]
      }
    ]
  },
  "tf.one_hot": {
    "tf.nn.softmax_cross_entropy_with_logits": [
      {
        "title": "56795292",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.one_hot",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.84",
        "r_prob": "1.0",
        "prob": "0.8231999999999999",
        "sentences": [
          "you can see internally that tf.nn.softmax_cross_entropy_with_logits is called , which computes the softmax probabilities and the cross - entropy function at the same time.",
          "there is a function to get this encoding in tensoflow called tf.one_hot.",
          "for example tf.one_hot ([ 3 ], 5 ) would result in the vector [ 0 , 0 , 1 , 0 , 0 ]. "
        ]
      },
      {
        "title": "38486938",
        "h": "tf.one_hot",
        "t": "tf.nn.softmax_cross_entropy_with_logits",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.98",
        "r_prob": "0.68",
        "prob": "0.6197520000000001",
        "sentences": [
          "the typical loss function used for comparing two probability distributions is called cross entropy.",
          "tensorflow has the tf.nn.softmax_cross_entropy_with_logits function which implements that loss."
        ]
      },
      {
        "title": "39113607",
        "h": "tf.nn.softmax_cross_entropy_with_logits",
        "t": "tf.one_hot",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.7",
        "r_prob": "0.9",
        "prob": "0.5984999999999999",
        "sentences": [
          "just came up with a work - around --- i created a one - hot tensor on the label indices using tf.one_hot ( with the depth set at the   of labels ). ",
          "tf.one_hot automatically zeros out all indices with - 1 in the resulting one_hot tensor ( of shape [ batch ,   of labels ]) ",
          "this enables softmax loss ( i.e.",
          "tf.nn.softmax_cross_entropy_with_logits ) to \" ignore \" all - 1 labels."
        ]
      }
    ]
  },
  "tf.gather": {
    "tf.slice": [
      {
        "title": "35172568",
        "h": "tf.slice",
        "t": "tf.gather",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.91",
        "r_prob": "0.97",
        "prob": "0.8297379999999999",
        "sentences": [
          "so even if you would use tf.gather / tf.slice , you still would have to get values of these operations via eval / run."
        ]
      },
      {
        "title": "36133338",
        "h": "tf.slice",
        "t": "tf.gather",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.99",
        "r_prob": "0.98",
        "prob": "0.82467",
        "sentences": [
          "you can take advantage of the fact that - 1 is a special argument to the tf.slice () size argument , meaning \" all remaining elements in that dimension \". ",
          "alternatively , you can use tf.gather () to select one or more slices from a tensor on the zeroth dimension."
        ]
      },
      {
        "title": "35158370",
        "h": "tf.slice",
        "t": "tf.gather",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.91",
        "r_prob": "0.99",
        "prob": "0.657657",
        "sentences": [
          "use the indexing operator ( based on tf.slice ()) to extract a contiguous slice from the tensor.",
          "use the tf.gather () op to select a non - contiguous slice from the tensor.",
          "note that tf.gather () only allows you to select whole slices in the 0th dimension ( whole rows in the example of a matrix ), so you may need to tf.reshape () or tf.transpose () your input to obtain the appropriate elements."
        ]
      }
    ],
    "indices": [
      {
        "title": "56576878",
        "h": "tf.gather",
        "t": "indices",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.47",
        "r_prob": "0.98",
        "prob": "0.43296399999999996",
        "sentences": [
          "you could just shuffle the indices and then use tf.gather () to extract values corresponding to those shuffled indices : "
        ]
      },
      {
        "title": "47395282",
        "h": "indices",
        "t": "tf.gather",
        "r": "S1",
        "h_prob": "0.52",
        "t_prob": "0.88",
        "r_prob": "0.86",
        "prob": "0.393536",
        "sentences": [
          "tf.gather takes a indices parameter , that is meant to be a 1 - dimensional array of integers."
        ]
      },
      {
        "title": "64162020",
        "h": "indices",
        "t": "tf.gather",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.74",
        "r_prob": "0.53",
        "prob": "0.286306",
        "sentences": [
          "in most cases , the tf.gather method needs 1d indices , and that is right in your case , instead of indices with 3d ( 1 , 1 , 120 ), a 1d is sufficient ( 120 ,). "
        ]
      }
    ]
  },
  "tf.contrib.predictor": {},
  "tf.keras libr": {},
  "tf.contrib.framework.is_tensor": {},
  "tf.is_tensor": {},
  "tf.strings": {},
  "tf.varlenfeature": {},
  "tf.layers.dropout": {
    "tf.nn.dropout": [
      {
        "title": "47916567",
        "h": "tf.layers.dropout",
        "t": "tf.nn.dropout",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.8366",
        "sentences": [
          "well , i cannot make a comment ... ",
          "the only thing came to my mind is that tf.nn.dropout uses keep_prob while tf.layers.dropout uses dropout rate.",
          "please check if this rate is too high."
        ]
      },
      {
        "title": "49177382",
        "h": "tf.layers.dropout",
        "t": "tf.nn.dropout",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.7546",
        "sentences": [
          "there are two primary ways to perform dropout in tensorflow : ",
          "tf.nn.dropout ( low - level ). ",
          "tf.layers.dropout ( high - level , uses tf.nn.dropout under the hood ). "
        ]
      },
      {
        "title": "48085077",
        "h": "tf.layers.dropout",
        "t": "tf.nn.dropout",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7392",
        "sentences": [
          "however , for evaluation ( test ) phase they are completely different.",
          "tf.nn.dropout will still do random dropping while tf.layers.dropout won ' t drop anything ( transparent layer ). ",
          "in most cases it make sense to use tf.layers.dropout."
        ]
      },
      {
        "title": "44404530",
        "h": "tf.layers.dropout",
        "t": "tf.nn.dropout",
        "r": "S1",
        "h_prob": "0.76",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.722",
        "sentences": [
          "a quick glance through tensorflow / python / layers / core.py and tensorflow / python / ops / nn_ops.py reveals that tf.layers.dropout is a wrapper for tf.nn.dropout.",
          "the only differences in the two functions are : ",
          "the tf.nn.dropout has parameter keep_prob : \" probability that each element is kept \" tf.layers.dropout has parameter rate : \" the dropout rate \" thus , keep_prob = 1 - rate as defined here.",
          "the tf.layers.dropout has training parameter : \" whether to return the output in training mode ( apply dropout ) or in inference mode ( return the input untouched ). "
        ]
      },
      {
        "title": "48549654",
        "h": "tf.nn.dropout",
        "t": "tf.layers.dropout",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.7209000000000001",
        "sentences": [
          "apart from the answers from @ nikpod and @ salvador dali ",
          "the tf.nn.dropout scaled the weights by 1 ./ keep prob during training phase , while tf.layers.dropout scaled the weights by 1 ./( 1 - rate ). "
        ]
      },
      {
        "title": "49606249",
        "h": "tf.nn.dropout",
        "t": "tf.layers.dropout",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.73",
        "r_prob": "1.0",
        "prob": "0.7081",
        "sentences": [
          "tf.layers.dropout uses tf.nn.dropout function internally.",
          "tf.nn.dropout might be useful if you just want to use a higher level abstraction and do not want to control many facets of the dropout.",
          "2 ) https :// www.tensorflow.org / api_docs / python / tf / nn / dropout ",
          "tf.layers.dropout is a wrapper around tf.nn.dropout and there ' s a slight difference in terms that tf.layers uses \" rate of dropout \" while tf.nn \" uses the probability to keep the inputs \". ",
          "though a direct relation can be established between them.",
          "also there ' s an extra argument \" training \" in tf.layers.dropout which is used to control whether to return the output in training mode ( apply dropout ) or in inference mode ( return the input untouched ). "
        ]
      }
    ]
  },
  "tf.contrib.cluster_resolver": {},
  "tf.complex": {},
  "cross -": {},
  "cross entropy": {},
  "minimum": {
    "maximum": [
      {
        "title": "61035376",
        "h": "maximum",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.88",
        "t_prob": "0.78",
        "r_prob": "0.97",
        "prob": "0.665808",
        "sentences": [
          "given a vector x , the q - th percentile of x is the value q / 100 of the way from the minimum to the maximum in a sorted copy of x."
        ]
      },
      {
        "title": "59115958",
        "h": "maximum",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.67",
        "t_prob": "0.69",
        "r_prob": "0.98",
        "prob": "0.45305399999999996",
        "sentences": [
          "in that case i would simply use 0.5 for both mean and std , such that the minimum value 0 will be converted to ( 0 - 0.5 ) / 0.5 = - 1 and the maximum value of 1 to ( 1 - 0.5 ) / 0.5 = 1."
        ]
      },
      {
        "title": "57450309",
        "h": "minimum",
        "t": "maximum",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.74",
        "r_prob": "0.76",
        "prob": "0.433048",
        "sentences": [
          "on the other hand , your labels indicate the magnitude of a single event where 5 is the maximum and 0 is the minimum.",
          "therefore , i would normalize your labels as : "
        ]
      }
    ],
    "learning rate": [
      {
        "title": "53536568",
        "h": "learning rate",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.68",
        "r_prob": "0.99",
        "prob": "0.5452920000000001",
        "sentences": [
          "likely your learning rate is too high.",
          "when the learning rate is too high , the network takes large leaps when changing the weights , and this can cause it to overshoot the local minimum it ' s approaching."
        ]
      },
      {
        "title": "33899251",
        "h": "minimum",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.86",
        "r_prob": "0.97",
        "prob": "0.525546",
        "sentences": [
          "increase it to something like 2000 at a minimum ( running 20 times through your dataset ). ",
          "that ' s the issue , your learning rate was too high for the loss function you were using."
        ]
      },
      {
        "title": "39234296",
        "h": "learning rate",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.8",
        "t_prob": "0.71",
        "r_prob": "0.78",
        "prob": "0.44304",
        "sentences": [
          "then the learning rate is decayed.",
          "to me it sounds like the learning rate was too high initially , and it got stuck in a local minimum afterwards.",
          "decaying the learning rate at that point , once it ' s already stuck in a local minimum , is not going to help it escape that minimum."
        ]
      },
      {
        "title": "51119754",
        "h": "learning rate",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.72",
        "t_prob": "0.57",
        "r_prob": "1.0",
        "prob": "0.41039999999999993",
        "sentences": [
          "and you set the learning rate is too large ( you can set lr = 0.01 or lr = 0.001 ), will be near the minimum point of shock.this is my code : cifar - 10 "
        ]
      },
      {
        "title": "64023626",
        "h": "minimum",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.69",
        "r_prob": "0.85",
        "prob": "0.36949499999999996",
        "sentences": [
          "the network is able to jump out of the previous local minimum.",
          "this indicates that it was previously stuck here , i.e.",
          "the learning rate was either too small ( if the error curve is flat or decreasing very slowly ) or too large ( if it oscillates around this point ) "
        ]
      },
      {
        "title": "50591136",
        "h": "learning rate",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.63",
        "r_prob": "0.7",
        "prob": "0.36603",
        "sentences": [
          "note that improvement from there is not guaranteed , because the model may have reached the local minimum , which may be global.",
          "there is no point to resume a model in order to search for another local minimum , unless you intent to increase the learning rate in a controlled fashion and nudge the model into a possibly better minimum not far away."
        ]
      },
      {
        "title": "35122970",
        "h": "learning rate",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.58",
        "r_prob": "0.7",
        "prob": "0.30855999999999995",
        "sentences": [
          "in practice this means your optimization will move towards a local minimum , but after getting close enough , will jump around the solution with steps proportional to the learning rate.",
          "use learning rate schedule to gradually decrease learning rate."
        ]
      }
    ]
  },
  "tf.maximum": {},
  "tf.minimum": {},
  "maximum": {
    "batch size": [
      {
        "title": "39734668",
        "h": "batch size",
        "t": "maximum",
        "r": "S2",
        "h_prob": "0.93",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.7440000000000001",
        "sentences": [
          "instead , inputs may be a single tensor where the maximum time is either the first or second dimension ( see the parameter time_major ). "
        ]
      },
      {
        "title": "62372248",
        "h": "batch size",
        "t": "maximum",
        "r": "S2",
        "h_prob": "0.96",
        "t_prob": "0.79",
        "r_prob": "0.97",
        "prob": "0.735648",
        "sentences": [
          "to create an instance of tf.dataset you should first organize your dataset in a numpy array with shape ( maximum sequence length , batch size , size of each record ). "
        ]
      },
      {
        "title": "38487518",
        "h": "batch size",
        "t": "maximum",
        "r": "S2",
        "h_prob": "0.88",
        "t_prob": "0.56",
        "r_prob": "0.99",
        "prob": "0.4878720000000001",
        "sentences": [
          "from there you can determine your maximum batch size.",
          "so you would probably want to be conservative with the maximum batch size."
        ]
      },
      {
        "title": "51769566",
        "h": "batch size",
        "t": "maximum",
        "r": "S2",
        "h_prob": "0.86",
        "t_prob": "0.61",
        "r_prob": "0.82",
        "prob": "0.43017199999999994",
        "sentences": [
          "for a standard machine learning / deep learning algorithm , choosing a batch size will have an impact on several aspects : ",
          "the bigger the batch size , the faster you will loop over your dataset n times to perform training .. "
        ]
      },
      {
        "title": "63030766",
        "h": "batch size",
        "t": "maximum",
        "r": "S2",
        "h_prob": "0.86",
        "t_prob": "0.49",
        "r_prob": "0.98",
        "prob": "0.412972",
        "sentences": [
          "reduce your batch size."
        ]
      }
    ],
    "minimum": [
      {
        "title": "61035376",
        "h": "maximum",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.88",
        "t_prob": "0.78",
        "r_prob": "0.97",
        "prob": "0.665808",
        "sentences": [
          "given a vector x , the q - th percentile of x is the value q / 100 of the way from the minimum to the maximum in a sorted copy of x."
        ]
      },
      {
        "title": "59115958",
        "h": "maximum",
        "t": "minimum",
        "r": "S2",
        "h_prob": "0.67",
        "t_prob": "0.69",
        "r_prob": "0.98",
        "prob": "0.45305399999999996",
        "sentences": [
          "in that case i would simply use 0.5 for both mean and std , such that the minimum value 0 will be converted to ( 0 - 0.5 ) / 0.5 = - 1 and the maximum value of 1 to ( 1 - 0.5 ) / 0.5 = 1."
        ]
      },
      {
        "title": "57450309",
        "h": "minimum",
        "t": "maximum",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.74",
        "r_prob": "0.76",
        "prob": "0.433048",
        "sentences": [
          "on the other hand , your labels indicate the magnitude of a single event where 5 is the maximum and 0 is the minimum.",
          "therefore , i would normalize your labels as : "
        ]
      }
    ]
  },
  "tf.tran.string_input_produce": {},
  "tf.float32 )": {},
  "tf.layers.flatten": {},
  "gradient norm": {},
  "tf.contrib.distributions.normal": {},
  "tf.distributions.normal": {},
  "l2 regularization": {
    "data augmentation": [
      {
        "title": "59171581",
        "h": "data augmentation",
        "t": "l2 regularization",
        "r": "S2",
        "h_prob": "0.71",
        "t_prob": "0.46",
        "r_prob": "0.99",
        "prob": "0.323334",
        "sentences": [
          "code for regularization is shown below ( you can try l1 regularization or l1_l2 regularization as well ): ",
          "perform image data augmentation using imagedatagenerator."
        ]
      },
      {
        "title": "59172613",
        "h": "data augmentation",
        "t": "l2 regularization",
        "r": "S2",
        "h_prob": "0.7",
        "t_prob": "0.46",
        "r_prob": "0.99",
        "prob": "0.31878",
        "sentences": [
          "code for regularization is shown below ( you can try l1 regularization or l1_l2 regularization as well ): ",
          "perform image data augmentation using imagedatagenerator."
        ]
      },
      {
        "title": "62391176",
        "h": "l2 regularization",
        "t": "data augmentation",
        "r": "S2",
        "h_prob": "0.67",
        "t_prob": "0.47",
        "r_prob": "1.0",
        "prob": "0.3149",
        "sentences": [
          "add more data use data augmentation use architectures that generalize well add regularization ( mostly dropout , l1 / l2 regularization are also possible ) reduce architecture complexity.",
          ". ",
          "use data augmentation.",
          "reduce architecture complexity .. "
        ]
      }
    ],
    "tf.keras.callbacks.earlystopping": [
      {
        "title": "59246895",
        "h": "tf.keras.callbacks.earlystopping",
        "t": "l2 regularization",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.44",
        "r_prob": "0.96",
        "prob": "0.287232",
        "sentences": [
          "use early stopping.",
          "code is shown below ",
          "callback = tf.keras.callbacks.earlystopping ( monitor =' val_loss ', patience = 15 ) ",
          "code for regularization is shown below ( you can try l1 regularization or l1_l2 regularization as well ). "
        ]
      },
      {
        "title": "59172613",
        "h": "tf.keras.callbacks.earlystopping",
        "t": "l2 regularization",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.46",
        "r_prob": "0.72",
        "prob": "0.225216",
        "sentences": [
          "use early stopping.",
          "code is shown below ",
          "callback = tf.keras.callbacks.earlystopping ( monitor =' val_loss ', patience = 15 ) ",
          "code for regularization is shown below ( you can try l1 regularization or l1_l2 regularization as well ): "
        ]
      },
      {
        "title": "59171581",
        "h": "tf.keras.callbacks.earlystopping",
        "t": "l2 regularization",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.46",
        "r_prob": "0.72",
        "prob": "0.225216",
        "sentences": [
          "use early stopping.",
          "code is shown below ",
          "callback = tf.keras.callbacks.earlystopping ( monitor =' val_loss ', patience = 15 ) ",
          "code for regularization is shown below ( you can try l1 regularization or l1_l2 regularization as well ): "
        ]
      }
    ]
  },
  "tf.log": {
    "tf.nn.softmax": [
      {
        "title": "42800217",
        "h": "tf.nn.softmax",
        "t": "tf.log",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.931095",
        "sentences": [
          "cost_v2 = tf.reduce_mean (- tf.reduce_sum ( y * tf.log ( tf.nn.softmax ( pred )), 1 )) "
        ]
      },
      {
        "title": "41366886",
        "h": "tf.nn.softmax",
        "t": "tf.log",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "0.95",
        "prob": "0.9216899999999999",
        "sentences": [
          "sparse_softmax_cross_entropy_with_logits is equivalent to a numerically stable version of the following : ",
          "- 1.",
          "* tf.gather ( tf.log ( tf.nn.softmax ( logits )), target ) "
        ]
      },
      {
        "title": "50932897",
        "h": "tf.nn.softmax",
        "t": "tf.log",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.93",
        "r_prob": "0.8",
        "prob": "0.72912",
        "sentences": [
          "your formula for cross entropy is wrong.",
          "you can create a distribution by using tf.nn.softmax which uses the exponential function to make all the values positive.",
          "logits = neural_network_model ( x ) ",
          "predicted_probabilities = tf.nn.softmax ( logits ) ",
          "cross_entropy = tf.reduce_mean (- tf.reduce_sum ( y_ * tf.log ( predicted_probabilities ), reduction_indices =[ 1 ])) ",
          "a much better way of doing it is to use the builtin softmax cross entropy functions in tensorflow."
        ]
      }
    ],
    "tf.reduce_sum": [
      {
        "title": "50932897",
        "h": "tf.log",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.8928",
        "sentences": [
          "your formula for cross entropy is wrong.",
          "you can create a distribution by using tf.nn.softmax which uses the exponential function to make all the values positive.",
          "predicted_probabilities = tf.nn.softmax ( logits ) ",
          "cross_entropy = tf.reduce_mean (- tf.reduce_sum ( y_ * tf.log ( predicted_probabilities ), reduction_indices =[ 1 ])) ",
          "a much better way of doing it is to use the builtin softmax cross entropy functions in tensorflow."
        ]
      },
      {
        "title": "59979540",
        "h": "tf.log",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.89",
        "r_prob": "0.98",
        "prob": "0.811146",
        "sentences": [
          "tensorflow 1.x : ",
          "cross_entropy = - tf.reduce_sum ( y_ * tf.log ( tf.clip_by_value ( y_conv , 1e - 10 , 1.0 ))) "
        ]
      },
      {
        "title": "34764585",
        "h": "tf.log",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.85",
        "r_prob": "0.99",
        "prob": "0.7910099999999999",
        "sentences": [
          "tf_cross_entropy = - tf.reduce_sum ( tf_softmax_correct * tf.log ( tf_softmax )) ",
          "if you replace this with : ",
          "tf_cross_entropy = - tf.reduce_sum ( tf_softmax_correct * tf.log ( tf_softmax + 1e - 50 )) it should avoid the problem.",
          "if you double the batch size and use reduce_sum it will double the cost ( and the magnitude of the gradient ). ",
          "specifically this is what i ' m using now when debugging : ",
          "cross_entropy = - tf.reduce_sum ( y * tf.log ( model + 1e - 50 ))    avoid nan due to 0 * log ( 0 ) cross_entropy = tf.print ( cross_entropy , [ cross_entropy ], \" cost \")   print to the console tensorflow was started from "
        ]
      },
      {
        "title": "33645235",
        "h": "tf.log",
        "t": "tf.reduce_sum",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.81",
        "r_prob": "0.99",
        "prob": "0.657558",
        "sentences": [
          "cross_entropy = - tf.reduce_sum ( y_ * tf.log ( y )) ",
          "cross_entropy = - tf.reduce_mean ( y_ * tf.log ( y )) ",
          "well , if we sum , then doubling the batch size doubles the cost , and also doubles the magnitude of the gradient."
        ]
      },
      {
        "title": "42800217",
        "h": "tf.reduce_sum",
        "t": "tf.log",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.95",
        "r_prob": "0.6",
        "prob": "0.5471999999999999",
        "sentences": [
          "cost_v2 = tf.reduce_mean (- tf.reduce_sum ( y * tf.log ( tf.nn.softmax ( pred )), 1 )) "
        ]
      }
    ]
  },
  "tf.fill": {
    "tf.shape": [
      {
        "title": "35855219",
        "h": "tf.fill",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.99",
        "r_prob": "1.0",
        "prob": "0.9801",
        "sentences": [
          "if you are trying to create a tensor with a dynamic size and the same ( constant ) value for every element , you can use tf.fill () and tf.shape () to create an appropriately - shaped tensor."
        ]
      },
      {
        "title": "49054121",
        "h": "tf.fill",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.85",
        "t_prob": "0.85",
        "r_prob": "0.99",
        "prob": "0.7152749999999999",
        "sentences": [
          "edit : ",
          "actually , replacing my_get_shape with tf.shape in the previous snippet works exacly the same.",
          "it seems that tf.shape should be the default ( being careful not to cram the graph with it ) unless you explicitly want to keep dimensions undefined.",
          "this class can , among other things , check whether values in a tensor are already known , which is true for example for tf.shape when the given tensor is static or for tf.fill ( and related like tf.ones ) with known values.",
          "it seems that , for example , for tf.shape , it is able to specifically pick known values and leave the rest as undefined."
        ]
      },
      {
        "title": "38832651",
        "h": "tf.fill",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.5821999999999999",
        "sentences": [
          "there is an alternative solution that uses tf.fill () like your initial version."
        ]
      },
      {
        "title": "48623392",
        "h": "tf.shape",
        "t": "tf.fill",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.4",
        "r_prob": "0.99",
        "prob": "0.34056000000000003",
        "sentences": [
          "you can use tf.shape in order to get the tensor shape as a tensor type.",
          "num2 = tf.fill ( num1.shape , tensor_val1 ) "
        ]
      }
    ],
    "tf.constant": [
      {
        "title": "35855219",
        "h": "tf.constant",
        "t": "tf.fill",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.99",
        "r_prob": "0.83",
        "prob": "0.7970489999999999",
        "sentences": [
          "a tf.constant () has fixed size and value at graph construction time , so it probably isn ' t the right op for your application.",
          "if you are trying to create a tensor with a dynamic size and the same ( constant ) value for every element , you can use tf.fill () and tf.shape () to create an appropriately - shaped tensor."
        ]
      },
      {
        "title": "37380546",
        "h": "tf.constant",
        "t": "tf.fill",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.53",
        "r_prob": "1.0",
        "prob": "0.4982",
        "sentences": [
          "note that tf.constant () does not accept a dynamic shape as an argument — for then it would not be constant !— but the similar tf.fill () op does."
        ]
      },
      {
        "title": "43007712",
        "h": "tf.constant",
        "t": "tf.fill",
        "r": "S1",
        "h_prob": "0.79",
        "t_prob": "0.6",
        "r_prob": "0.75",
        "prob": "0.3555",
        "sentences": [
          "the shape argument of the tf.constant () op expects a static shape , so you can ' t use a tf.tensor as part of the argument.",
          "fortunately there is another op that will suffice : tf.fill (), which allows the shape ( its dims argument ) to be a tf.tensor."
        ]
      }
    ]
  },
  "activation function": {
    "learning rate": [
      {
        "title": "62817087",
        "h": "learning rate",
        "t": "activation function",
        "r": "S2",
        "h_prob": "0.75",
        "t_prob": "0.78",
        "r_prob": "1.0",
        "prob": "0.585",
        "sentences": [
          "i would use relu instead of sigmoid as the activation function.",
          "try a smaller learning rate.",
          "actually i find i get the best results using a variable learning rate."
        ]
      },
      {
        "title": "63242757",
        "h": "learning rate",
        "t": "activation function",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.57",
        "r_prob": "1.0",
        "prob": "0.48449999999999993",
        "sentences": [
          "you have the wrong activation function.",
          "for multi - class problems , use ' softmax '.. ",
          "your optimizer ' s learning rate is a little too high , so the step is too high and jumping all over the cost function."
        ]
      },
      {
        "title": "54031459",
        "h": "learning rate",
        "t": "activation function",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.72",
        "r_prob": "0.76",
        "prob": "0.415872",
        "sentences": [
          "reduce learning rate if your learning rate is too high you might converge in non - sufficient optima , which also tend to optimize for gray , black or white predictions only.",
          "the activation function causes the values to vanish ! "
        ]
      }
    ],
    "loss function": [
      {
        "title": "60575553",
        "h": "activation function",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.95",
        "r_prob": "0.98",
        "prob": "0.75411",
        "sentences": [
          "the second of all , you are using the activation ' sigmoid ', but your loss function + metric is mse.",
          "if it is indeed a regression , then the activation function should be ' linear ' at the last step."
        ]
      },
      {
        "title": "49580867",
        "h": "activation function",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.88",
        "r_prob": "0.87",
        "prob": "0.6354479999999999",
        "sentences": [
          "you can set the classifier ' s optimizer and the activation function in the hidden layers , but i don ' t think you can define a custom loss function."
        ]
      },
      {
        "title": "53847486",
        "h": "activation function",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.88",
        "t_prob": "0.89",
        "r_prob": "0.57",
        "prob": "0.446424",
        "sentences": [
          "if it is a classification task you must use ' categorical_crossentropy ' as the loss function ( instead of ' sparse_categorical_crossentropy ' which you are currently using ) and use ' softmax ' as the activation function of last layer."
        ]
      },
      {
        "title": "55634744",
        "h": "activation function",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.9",
        "r_prob": "0.51",
        "prob": "0.39015",
        "sentences": [
          "use ' sigmoid ' as the activation function of last layer .. ",
          "use ' binary_crossentropy ' as the loss function .. "
        ]
      },
      {
        "title": "45546361",
        "h": "activation function",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.67",
        "t_prob": "0.9",
        "r_prob": "0.6",
        "prob": "0.36180000000000007",
        "sentences": [
          "many autoencoder use mse as their loss function.",
          "i would suggest using tanh as your intermediate activation function."
        ]
      }
    ],
    "dense layer": [
      {
        "title": "62196784",
        "h": "dense layer",
        "t": "activation function",
        "r": "S2",
        "h_prob": "0.8",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.656",
        "sentences": [
          "it is meant to be used as a layer inside your model , not as a parameter of your dense layer.",
          "to have a function that works as an activation to give as a parameter to dense , you should use tf.keras.activations.relu."
        ]
      },
      {
        "title": "47415774",
        "h": "dense layer",
        "t": "activation function",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.64",
        "r_prob": "1.0",
        "prob": "0.49920000000000003",
        "sentences": [
          "the activation function in a dense layer is applied after the multiplication with the weights and the addition of the bias."
        ]
      },
      {
        "title": "60376294",
        "h": "dense layer",
        "t": "activation function",
        "r": "S2",
        "h_prob": "0.66",
        "t_prob": "0.76",
        "r_prob": "0.97",
        "prob": "0.48655200000000004",
        "sentences": [
          "you cannot really just use the logarithm as an activation function , as it is not defined for values x <= 0.0 , so if at any point the dense layer produces a negative or zero value , the logarithm will produce nan , which then propagates to the loss."
        ]
      }
    ],
    "neural network": [
      {
        "title": "51831196",
        "h": "activation function",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.83",
        "r_prob": "0.82",
        "prob": "0.5512859999999999",
        "sentences": [
          "i agree with @ cyniikal , your network seems too complex for this dataset.",
          "if you would like to add layers to your neural network ( the network will converge with more difficulties ), i highly recommend reading this article on neural nets.",
          "specifically , since you added sigmoid as your last activation function , i believe you are suffering from a vanishing gradient problem."
        ]
      },
      {
        "title": "52336461",
        "h": "activation function",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.67",
        "t_prob": "0.81",
        "r_prob": "0.97",
        "prob": "0.5264190000000001",
        "sentences": [
          "the reason for your problem is because the output of your neural network is being squashed by an activation function.",
          "this way , initially , your neural network will output close to 0 values , and then the weights will be updated and the learning will be more stable."
        ]
      },
      {
        "title": "44498122",
        "h": "activation function",
        "t": "neural network",
        "r": "S2",
        "h_prob": "0.74",
        "t_prob": "0.91",
        "r_prob": "0.51",
        "prob": "0.343434",
        "sentences": [
          "these weights ( and sometimes biases ) are what we learn in a neural network."
        ]
      }
    ]
  },
  "tf.metrics.recall": {},
  "tf.layers.conv1d": {},
  "indices": {
    "tf.gather": [
      {
        "title": "56576878",
        "h": "tf.gather",
        "t": "indices",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.47",
        "r_prob": "0.98",
        "prob": "0.43296399999999996",
        "sentences": [
          "you could just shuffle the indices and then use tf.gather () to extract values corresponding to those shuffled indices : "
        ]
      },
      {
        "title": "47395282",
        "h": "indices",
        "t": "tf.gather",
        "r": "S1",
        "h_prob": "0.52",
        "t_prob": "0.88",
        "r_prob": "0.86",
        "prob": "0.393536",
        "sentences": [
          "tf.gather takes a indices parameter , that is meant to be a 1 - dimensional array of integers."
        ]
      },
      {
        "title": "64162020",
        "h": "indices",
        "t": "tf.gather",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.74",
        "r_prob": "0.53",
        "prob": "0.286306",
        "sentences": [
          "in most cases , the tf.gather method needs 1d indices , and that is right in your case , instead of indices with 3d ( 1 , 1 , 120 ), a 1d is sufficient ( 120 ,). "
        ]
      }
    ],
    "loss function": [
      {
        "title": "64126418",
        "h": "indices",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.6",
        "t_prob": "0.83",
        "r_prob": "0.86",
        "prob": "0.42827999999999994",
        "sentences": [
          "according to the model.fit () reference , this parameter should be a dict : optional dictionary mapping class indices ( integers ) to a weight ( float ) value , used for weighting the loss function ( during training only ). "
        ]
      },
      {
        "title": "59493502",
        "h": "indices",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.59",
        "t_prob": "0.83",
        "r_prob": "0.87",
        "prob": "0.42603899999999995",
        "sentences": [
          "class_weight : optional dictionary mapping class indices ( integers ) to a weight ( float ) value , used for weighting the loss function ( during training only ). "
        ]
      },
      {
        "title": "57936450",
        "h": "indices",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.59",
        "t_prob": "0.83",
        "r_prob": "0.87",
        "prob": "0.42603899999999995",
        "sentences": [
          "class_weight : optional dictionary mapping class indices ( integers ) to a weight ( float ) value , used for weighting the loss function ( during training only ). ",
          "one way could be to increase the loss value for classes with low samples."
        ]
      },
      {
        "title": "52972521",
        "h": "indices",
        "t": "loss function",
        "r": "S2",
        "h_prob": "0.58",
        "t_prob": "0.83",
        "r_prob": "0.78",
        "prob": "0.375492",
        "sentences": [
          "class_weight : optional dictionary mapping class indices ( integers ) to a weight ( float ) value , used for weighting the loss function ( during training only ). ",
          "sample_weight : optional numpy array of weights for the training samples , used for weighting the loss function ( during training only ). ",
          "if you have a weight for each sample , you can pass the numpy array as sample_weight to achieve the same effect without writing your own loss function."
        ]
      }
    ]
  },
  "outputs": {},
  "tf.ones": {
    "tf.shape": [
      {
        "title": "33711582",
        "h": "tf.shape",
        "t": "tf.ones",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.9021",
        "sentences": [
          "the way to solve your problem is to use tf.pack operation : ",
          "o = tf.ones ( shape = tf.pack ([ tf.shape ( x )[ 0 ], 1 ])) "
        ]
      },
      {
        "title": "45018288",
        "h": "tf.shape",
        "t": "tf.ones",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.91",
        "r_prob": "0.75",
        "prob": "0.6279",
        "sentences": [
          "this should work , ",
          "tf.ones ([ tf.shape ( a )[ 0 ], 1 , 1 ]) * a ",
          "also using tf.tile , we can obtain the same : ",
          "tf.tile ( tf.expand_dims ( a , 0 ), [ tf.shape ( a )[ 0 ], 1 , 1 ]) "
        ]
      },
      {
        "title": "49054121",
        "h": "tf.ones",
        "t": "tf.shape",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.85",
        "r_prob": "0.87",
        "prob": "0.525045",
        "sentences": [
          "edit : ",
          "actually , replacing my_get_shape with tf.shape in the previous snippet works exacly the same.",
          "it seems that tf.shape should be the default ( being careful not to cram the graph with it ) unless you explicitly want to keep dimensions undefined.",
          "this class can , among other things , check whether values in a tensor are already known , which is true for example for tf.shape when the given tensor is static or for tf.fill ( and related like tf.ones ) with known values.",
          "it seems that , for example , for tf.shape , it is able to specifically pick known values and leave the rest as undefined."
        ]
      }
    ]
  },
  "convolutional layers": {},
  "convolutional layer": {
    "width": [
      {
        "title": "40706157",
        "h": "width",
        "t": "convolutional layer",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.89",
        "r_prob": "0.71",
        "prob": "0.511839",
        "sentences": [
          "in general the input for a convolutional layer is ( numsamples , numchannels , width , height ). "
        ]
      },
      {
        "title": "57401878",
        "h": "width",
        "t": "convolutional layer",
        "r": "S1",
        "h_prob": "0.72",
        "t_prob": "0.77",
        "r_prob": "0.86",
        "prob": "0.476784",
        "sentences": [
          "therefore , the input has the shape [ time , feature_size ] or if you are processing a batch [ batch_size , time , feature_size ]. ",
          "in your case , the input has a shape [ batch_size , number_of_frames , height , width , num_channels ]. ",
          "then , you use a convolutional layer , to learn spatial dependencies between the pixels in each video frame.",
          "therefore , for each video frame , the convolutional layer is going to provide you a tensor with shape [ activation_map_width , activation_map_height , number_of_filters ]. "
        ]
      },
      {
        "title": "44865605",
        "h": "convolutional layer",
        "t": "width",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.58",
        "r_prob": "0.95",
        "prob": "0.42977999999999994",
        "sentences": [
          "according to my experiments in pytorch , if convolutional layer before the bn outputs more than one value ( i.e.",
          "1 x feat_nb x height x width , where height > 1 or width > 1 ), then the bn still works fine even when the batch size is equal to one."
        ]
      }
    ]
  },
  "batch norm": {
    "batch normalization": [
      {
        "title": "51157850",
        "h": "batch normalization",
        "t": "batch norm",
        "r": "S2",
        "h_prob": "0.72",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.5832",
        "sentences": [
          "you can use this to extract fairly easily the variables from layers that used batch norm.",
          "now that you know which layers used batch norm , for every such layer , you can extract its weights w , bias b , batch norm variance v , mean m , gamma and beta parameters."
        ]
      },
      {
        "title": "46620505",
        "h": "batch normalization",
        "t": "batch norm",
        "r": "S2",
        "h_prob": "0.72",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.576",
        "sentences": [
          "fused batch norm combines the multiple operations needed to do batch normalization into a single kernel.",
          "batch norm is an expensive process that for some models makes up a large percentage of the operation time.",
          "using fused batch norm can result in a 12 %- 30 % speedup."
        ]
      },
      {
        "title": "60861291",
        "h": "batch normalization",
        "t": "batch norm",
        "r": "S2",
        "h_prob": "0.74",
        "t_prob": "0.7",
        "r_prob": "0.99",
        "prob": "0.51282",
        "sentences": [
          "certain ops , such as batch normalization , are disabled on prediction - this can make a big difference with certain architectures , although it generally isn ' t supposed to if you ' re using batch norm correctly.",
          ". "
        ]
      },
      {
        "title": "44002219",
        "h": "batch normalization",
        "t": "batch norm",
        "r": "S2",
        "h_prob": "0.73",
        "t_prob": "0.83",
        "r_prob": "0.75",
        "prob": "0.45442499999999997",
        "sentences": [
          "the non - fused batch norm does computations using several individual ops.",
          "fused batch norm combines the individual operations into a single kernel , which runs faster.",
          "fused batch norm combines the multiple operations needed to do batch normalization into a single kernel.",
          "batch norm is an expensive process that for some models makes up a large percentage of the operation time.",
          "using fused batch norm can result in a 12 %- 30 % speedup."
        ]
      }
    ]
  },
  "tf.keras.layers.globalaveragepooling1d": {},
  "tf.keras.layers.averagepooling1d": {},
  "tf.config.run_functions_eagerly": {},
  "tf.image.decode_png": {},
  "tf.batch_matmul": {},
  "tf.compat.v2.variable": {},
  "tf.constant_initializer": {
    "tf.get_variable": [
      {
        "title": "37976597",
        "h": "tf.constant_initializer",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.93",
        "r_prob": "1.0",
        "prob": "0.8649000000000001",
        "sentences": [
          "i ' m not familiar with the tutorial but it looks like you provided tf.constant_initializer ( 0.0 ) as your data type which returns an initializer to generate constants.",
          "the third parameter of tf.get_variable () should be the data type of your variable which for a biases variable is usually something like tf.float32 or tf.float64."
        ]
      },
      {
        "title": "38820162",
        "h": "tf.constant_initializer",
        "t": "tf.get_variable",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "0.51",
        "prob": "0.48980399999999996",
        "sentences": [
          "the tf.constant_initializer () function might not accept a tf.tensor as an argument , but tf.get_variable () does accept a tf.tensor as its initializer argument.",
          "... which requires even fewer characters ! ",
          "the reason tf.constant_initializer () doesn ' t take an arbitrary tensor is that it is designed to initialize variables of many different shapes with the same constant value for each element.",
          "... wouldn ' t make much sense.",
          "arguably we could make tf.constant_initializer () accept a scalar tf.tensor , and then it would have semantics similar to tf.fill (), but we haven ' t had any demand for that yet."
        ]
      },
      {
        "title": "57440644",
        "h": "tf.get_variable",
        "t": "tf.constant_initializer",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.96",
        "r_prob": "0.6",
        "prob": "0.47807999999999995",
        "sentences": [
          "and below three things are from tensorflow site and also useful.",
          "tf.get_variable : gets an existing variable with these parameters or create a new one.",
          "tf.constant_initializer : initializer that generates tensors with constant values.",
          "to put it simply , the role of tf.constant_initializer is to generate constant values.",
          "in tensorflow , you should initialize variables before creating graphs and sessions.",
          "therefore you have to use tf.global_variables_initializer when using variables.",
          "actually , in your code , you don ' t have to use tf.constant_initializer because tf.get_variable uses default initializer as glorot_uniform_initializer.",
          "example code 2 : you don ' t have to use tf.constant_initializer.",
          "finally , even if you use tf.constant_initializer in tf.get_variable you should use tf.global_variables_initializer.",
          "the reason is that tf.get_variable is a variable.",
          "so you initialize variables before tf.session by using tf.global_variables_initializer.",
          "maybe you can think of initializer in tf.get_variable ( e.g tf.constant_initializer , glorot_uniform_initializer ) as initializing value by some kind of distributions when initializing variables by using tf.global_variables_initializer."
        ]
      }
    ]
  },
  "tf.traing.replica_device_setter": {},
  "tf.train.syn": {},
  "tf.dynamic_partition": {},
  "tf.nn.sigmoid": {},
  "tf.contrib.seq2seq.sequence_loss": {},
  "tf.add_to_collection": {
    "tf.graphkeys": [
      {
        "title": "38878750",
        "h": "tf.add_to_collection",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "0.95",
        "prob": "0.9216899999999999",
        "sentences": [
          "if you are using the \" losses collection \" ( e.g.",
          "via tf.contrib.losses.add_loss () or tf.add_to_collection ( tf.graphkeys.losses , ...), you can use tf.contrib.losses.get_total_loss () to get a single loss value that can be passed to a standard tensorflow tf.train.optimizer subclass."
        ]
      },
      {
        "title": "38920063",
        "h": "tf.add_to_collection",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "0.95",
        "prob": "0.9216899999999999",
        "sentences": [
          "instead , if you add the scalar loss from each sub - network to the \" losses collection \" ( e.g.",
          "via tf.contrib.losses.add_loss () or tf.add_to_collection ( tf.graphkeys.losses , ...), you can use tf.contrib.losses.get_total_loss () to get a single loss value that can be passed to a single standard tensorflow tf.train.optimizer subclass."
        ]
      },
      {
        "title": "43328813",
        "h": "tf.add_to_collection",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.86",
        "r_prob": "0.95",
        "prob": "0.8006599999999999",
        "sentences": [
          "equivalently , variables can be created via the regular weights = tf.variable (...) constructor , followed by tf.add_to_collection ( tf.graphkeys.regularization_losses , weights ). "
        ]
      },
      {
        "title": "43588490",
        "h": "tf.add_to_collection",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.95",
        "r_prob": "0.77",
        "prob": "0.67298",
        "sentences": [
          "that is not a big issue though , since tensorflow provides graph collections for exactly this use case , making for a much cleaner design ( since it makes you group your things logically ). ",
          "basically you want to add the penalties to the collection using tf.add_to_collection : "
        ]
      }
    ],
    "tf.get_collection": [
      {
        "title": "50068241",
        "h": "tf.add_to_collection",
        "t": "tf.get_collection",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.92",
        "r_prob": "0.93",
        "prob": "0.81282",
        "sentences": [
          "you can add the state tensor to a graph collection , which is basically a key value store to track tensors , using tf.add_to_collection and retrieve it later using tf.get_collection."
        ]
      },
      {
        "title": "43059794",
        "h": "tf.add_to_collection",
        "t": "tf.get_collection",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.78",
        "r_prob": "1.0",
        "prob": "0.7254",
        "sentences": [
          "while variables must add to collections before retrieving the collection.",
          "tf.add_to_collection (\" vars \", w1 ) tf.add_to_collection (\" vars \", b1 ) ... then all_vars = tf.get_collection (' vars ') "
        ]
      },
      {
        "title": "44655199",
        "h": "tf.add_to_collection",
        "t": "tf.get_collection",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.56",
        "r_prob": "0.95",
        "prob": "0.49476000000000003",
        "sentences": [
          "another option is to display the graph using tensorboard or jupyter notebook and the show_graph command.",
          "if you want to set it up in the future so that you can retrieve it by name , you need to add it to a collection using tf.add_to_collection : ",
          "you might also be able to get by just naming it and then specifying its scope in tf.get_collection instead , but i am not sure."
        ]
      },
      {
        "title": "43909969",
        "h": "tf.add_to_collection",
        "t": "tf.get_collection",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.48",
        "r_prob": "0.99",
        "prob": "0.42292799999999997",
        "sentences": [
          "to get back a handle on your weights , ",
          "you can use their names in the tf graph : w1 = get_variable ( name =' w1 '). ",
          "the problem is that you ' ll have to pay close attention to your name scopes , and make sure that you don ' t have multiple variables of the same name ( in which case tf adds ' _1 ' to one of their names , so you might get the wrong one ). ",
          "when building the graph , before saving it , do for instance : tf.add_to_collection (' weights ', w1 ) and tf.add_to_collection (' weights ', w2 ), and in your restoring code : [ w1 , w2 ] = tf.get_collection (' weights1 '). "
        ]
      }
    ]
  },
  "string_input_produ": {},
  "tf.nn.separable_conv2d": {},
  "tf.keras.metrics.recall": {},
  "tf.keras.metric.recall": {},
  "tf.contrib.learn": {},
  "tf.contrib ap": {},
  "tf.group": {},
  "tf - sli": {},
  "tf.contrib.slim": {},
  "tf.gfile.open": {},
  "tf.contrib.learn.dnnclassifier": {},
  "tf.contrib.learn.skcompat": {},
  "tf.assign": {
    "tf.variable": [
      {
        "title": "44300480",
        "h": "tf.variable",
        "t": "tf.assign",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.95",
        "r_prob": "0.9",
        "prob": "0.82935",
        "sentences": [
          "from the docs ( emphasis mine ): ",
          "calling tf.variable () adds several ops to the graph : ",
          "a variable op that holds the variable value .. ",
          "an initializer op that sets the variable to its initial value.",
          "this is actually a tf.assign op .. ",
          "the ops for the initial value , such as the zeros op for the biases variable in the example are also added to the graph .. "
        ]
      },
      {
        "title": "44929824",
        "h": "tf.variable",
        "t": "tf.assign",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.8",
        "r_prob": "0.96",
        "prob": "0.7296",
        "sentences": [
          "tf.variable objects cannot be used as loop variables in a while loop , as loop variables are implemented differently.",
          "so either create your variable outside the loop and update it yourself with tf.assign in each iteration or manually keep track of the updates as you do with loop variables ( by returning their updated values from the loop lambdas , and in your case using the value from the inner loop as the new value for the outer loop ). "
        ]
      },
      {
        "title": "51122680",
        "h": "tf.variable",
        "t": "tf.assign",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.98",
        "r_prob": "0.92",
        "prob": "0.72128",
        "sentences": [
          "tf.variable can be used to create the same type of code.",
          "the main properties of tf.variable i understand is this.",
          "reference is this ",
          "tf.variable has to be initialized before it is used by using tf.assign or executing an initializer or by loading its saved state from a file."
        ]
      },
      {
        "title": "44220922",
        "h": "tf.assign",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.95",
        "r_prob": "0.74",
        "prob": "0.66082",
        "sentences": [
          "in the line tf.assign ( a , b ) you are instructing tensorflow to assign the value of b to the variable a.",
          "if you replace your first line with a = tf.variable ( 1 ) then you have a program that will execute successfully."
        ]
      },
      {
        "title": "58235644",
        "h": "tf.variable",
        "t": "tf.assign",
        "r": "S1",
        "h_prob": "0.63",
        "t_prob": "0.86",
        "r_prob": "0.98",
        "prob": "0.530964",
        "sentences": [
          "tf.assign * functions are available as methods on tf.variable in tf 2.0.",
          "note that unlike tf.assign in tf 1.x , tf.variable.assing_sub will execute the assignment eagerly."
        ]
      },
      {
        "title": "42659635",
        "h": "tf.variable",
        "t": "tf.assign",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.73",
        "r_prob": "0.71",
        "prob": "0.46128699999999995",
        "sentences": [
          "having tf.variable objects as loop variables for while loops is not supported , and will behave in weird nondeterministic ways.",
          "always use tf.assign and friends to update the value of a tf.variable."
        ]
      }
    ]
  },
  "tf.image.encode_png": {},
  "deep neural network": {},
  "svm": {},
  "tf.audio_": {},
  "tf.estimator.experimental.stop_if_no_decrease_hook": {},
  "tf.contrib.estimator": {},
  "tf.py_function": {},
  "tf.io.parse_single_example": {},
  "epochs x": {},
  "tf.round": {},
  "tf.keras.activations.elu": {},
  "tf.pa": {},
  "tf.logical_and": {},
  "tf.randomshufflequeue": {
    "tf.train.shuffle_batch": [
      {
        "title": "35729020",
        "h": "tf.train.shuffle_batch",
        "t": "tf.randomshufflequeue",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.47",
        "r_prob": "0.98",
        "prob": "0.45138799999999996",
        "sentences": [
          "if you move this line after images = get_batch (), your program should work.",
          "what is the problem here ? ",
          "the tf.train.shuffle_batch () function internally uses a tf.randomshufflequeue to produce a randomized batch."
        ]
      },
      {
        "title": "34594851",
        "h": "tf.train.shuffle_batch",
        "t": "tf.randomshufflequeue",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.37",
        "r_prob": "0.99",
        "prob": "0.35164799999999996",
        "sentences": [
          "the tf.train.shuffle_batch () function can be used to produce ( one or more ) tensors containing a batch of inputs.",
          "internally , tf.train.shuffle_batch () creates a tf.randomshufflequeue , on which it calls q.enqueue () with the image and label tensors to enqueue a single element ( image - label pair ). ",
          "note that , although it looks from the code like read_input and filename_queue have a functional relationship , there is an additional wrinkle.",
          "simply evaluating the result of tf.train.shuffle_batch () will block forever , because no elements have been added to the internal queue.",
          "to simplify this , when you call tf.train.shuffle_batch (), tensorflow will add a queuerunner to an internal collection in the graph."
        ]
      },
      {
        "title": "41971507",
        "h": "tf.train.shuffle_batch",
        "t": "tf.randomshufflequeue",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.35",
        "r_prob": "0.99",
        "prob": "0.33263999999999994",
        "sentences": [
          "the tf.train.shuffle_batch () function uses a tf.randomshufflequeue internally to accumulate batches of batch_size elements , which are sampled uniformly at random from the elements currently in the queue.",
          "if the dataset has only 200 images , it would be easily possible to load the entire dataset in memory.",
          "tf.train.shuffle_batch () would be quite inefficient , because it enqueue each image and label multiple times in the tf.randomshufflequeue."
        ]
      },
      {
        "title": "39493501",
        "h": "tf.train.shuffle_batch",
        "t": "tf.randomshufflequeue",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.24",
        "r_prob": "0.99",
        "prob": "0.22571999999999998",
        "sentences": [
          "the tensors returned from tf.train.shuffle_batch (), image_batch and label_batch correspond to 32 probably *- different images packed together , and the 32 related labels.",
          "tensorflow uses a tf.randomshufflequeue internally to shuffle the data , and creates additional threads to evaluate single_image and single_label so that they can be added to this queue.",
          "the tf.train.shuffle_batch () function has different behaviors depending on the arguments you pass."
        ]
      }
    ]
  },
  "tf.contrib.framework.filter_variables": {},
  "tf.graphkeys": {
    "tf.add_to_collection": [
      {
        "title": "38878750",
        "h": "tf.add_to_collection",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "0.95",
        "prob": "0.9216899999999999",
        "sentences": [
          "if you are using the \" losses collection \" ( e.g.",
          "via tf.contrib.losses.add_loss () or tf.add_to_collection ( tf.graphkeys.losses , ...), you can use tf.contrib.losses.get_total_loss () to get a single loss value that can be passed to a standard tensorflow tf.train.optimizer subclass."
        ]
      },
      {
        "title": "38920063",
        "h": "tf.add_to_collection",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "0.95",
        "prob": "0.9216899999999999",
        "sentences": [
          "instead , if you add the scalar loss from each sub - network to the \" losses collection \" ( e.g.",
          "via tf.contrib.losses.add_loss () or tf.add_to_collection ( tf.graphkeys.losses , ...), you can use tf.contrib.losses.get_total_loss () to get a single loss value that can be passed to a single standard tensorflow tf.train.optimizer subclass."
        ]
      },
      {
        "title": "43328813",
        "h": "tf.add_to_collection",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.86",
        "r_prob": "0.95",
        "prob": "0.8006599999999999",
        "sentences": [
          "equivalently , variables can be created via the regular weights = tf.variable (...) constructor , followed by tf.add_to_collection ( tf.graphkeys.regularization_losses , weights ). "
        ]
      },
      {
        "title": "43588490",
        "h": "tf.add_to_collection",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.95",
        "r_prob": "0.77",
        "prob": "0.67298",
        "sentences": [
          "that is not a big issue though , since tensorflow provides graph collections for exactly this use case , making for a much cleaner design ( since it makes you group your things logically ). ",
          "basically you want to add the penalties to the collection using tf.add_to_collection : "
        ]
      }
    ],
    "tf.contrib.framework.local_variable": [
      {
        "title": "49327230",
        "h": "tf.contrib.framework.local_variable",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.855",
        "sentences": [
          "it appears that the moving mean and moving variance are stored within tf.graphkeys.global_variables and it looks like the reason nothing showed up in model_variables is because you need to use tf.contrib.framework.local_variable "
        ]
      },
      {
        "title": "43602899",
        "h": "tf.contrib.framework.local_variable",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.97",
        "r_prob": "0.89",
        "prob": "0.828768",
        "sentences": [
          "usually used for temporarily variables , like counters.",
          "note : use tf.contrib.framework.local_variable to add to this collection.",
          "so to create a local variable you need to do something like this."
        ]
      },
      {
        "title": "43602962",
        "h": "tf.contrib.framework.local_variable",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.97",
        "r_prob": "0.81",
        "prob": "0.7542719999999999",
        "sentences": [
          "a local variable in tf is any variable which was created with collections =[ tf.graphkeys.local_variables ]. ",
          "usually used for temporarily variables , like counters.",
          "note : use tf.contrib.framework.local_variable to add to this collection.",
          "a global variable is mostly every other variable initialized by you."
        ]
      }
    ]
  },
  "tf.train.get_checkpoint_state": {},
  "keras api": {},
  "tf.assign_sub": {},
  "tf.float64": {},
  "tf.compat.v2.random.set_seed": {},
  "tf.indexedslices": {},
  "tf.roll": {},
  "tf.sequence_mask": {},
  "convolutional -": {},
  "tf.contrib.layers.real_valued_column": {},
  "discriminator": {
    "gan": [
      {
        "title": "53593130",
        "h": "discriminator",
        "t": "gan",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.76",
        "r_prob": "0.92",
        "prob": "0.6432640000000001",
        "sentences": [
          "the same can be done for discriminator and gan."
        ]
      },
      {
        "title": "60300581",
        "h": "discriminator",
        "t": "gan",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.72",
        "r_prob": "0.99",
        "prob": "0.6272639999999999",
        "sentences": [
          "in a gan nash equilibrium is reached when you converge i.e.",
          "when the loss of the discriminator does not get reduced at the expense of the generator and v.v.",
          "another way would be to implement some sort of early stopping ( or if you use tensorboard track the losses of the two networks to visualize convergence ). ",
          "this article seems to have an extensive explanation of the nash equilibrium for gans."
        ]
      },
      {
        "title": "42470757",
        "h": "discriminator",
        "t": "gan",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.72",
        "r_prob": "1.0",
        "prob": "0.5903999999999999",
        "sentences": [
          "first , i would advise you to switch to the functional api models.",
          "the way i have found around that problem , is to define the layers and use them for both the discriminator and the gan models."
        ]
      },
      {
        "title": "60590207",
        "h": "discriminator",
        "t": "gan",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.6",
        "r_prob": "0.8",
        "prob": "0.4272",
        "sentences": [
          "discriminator is compiled first and then comes the gan part ( self.combined ). "
        ]
      },
      {
        "title": "51070467",
        "h": "discriminator",
        "t": "gan",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.42",
        "r_prob": "0.68",
        "prob": "0.22848000000000004",
        "sentences": [
          "when you do gan there are few things that change from normal neural network training.",
          "the artificial images from the generator network change at each update of the weights in the network .. ",
          "it is pointless to train the discriminator on a lot of data , if you then update the generator."
        ]
      }
    ]
  },
  "network": {},
  "tf.nn.conv2d and sor": {},
  "tf.wholefilereader": {
    "tf.read_file": [
      {
        "title": "53020118",
        "h": "tf.read_file",
        "t": "tf.wholefilereader",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.54",
        "r_prob": "0.99",
        "prob": "0.49183200000000005",
        "sentences": [
          "i found the solution by replacing the tf.wholefilereader () with tf.read_file (): "
        ]
      },
      {
        "title": "34345827",
        "h": "tf.wholefilereader",
        "t": "tf.read_file",
        "r": "S1",
        "h_prob": "0.37",
        "t_prob": "0.89",
        "r_prob": "1.0",
        "prob": "0.3293",
        "sentences": [
          "use tf.read_file ( filename ) rather than tf.wholefilereader () to read your image files.",
          "tf.read_file () is a stateless op that consumes a single filename and produces a single string containing the contents of the file."
        ]
      },
      {
        "title": "42987992",
        "h": "tf.wholefilereader",
        "t": "tf.read_file",
        "r": "S1",
        "h_prob": "0.39",
        "t_prob": "0.84",
        "r_prob": "1.0",
        "prob": "0.3276",
        "sentences": [
          "since you are using tf.wholefilereader , you may be able to avoid the problem of synchronizing multiple queues by replacing it with the much simpler tf.read_file () op , as follows : "
        ]
      }
    ]
  },
  "tf.train.example": {},
  "matmul": {},
  "tf.train.get_or_create_global_step": {},
  "tf.meshgrid": {},
  "tf.zeros": {},
  "tf.tfrecordreader": {},
  "autoencoder": {},
  "tf.keras.backend.get_session": {},
  "tf.interactivesession": {
    "tf.session": [
      {
        "title": "41608016",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "1.0",
        "r_prob": "0.97",
        "prob": "0.9602999999999999",
        "sentences": [
          "some questions ",
          "first why you use sess = tf.interactivesession () and with tf.session () as sess : at same time , just curious "
        ]
      },
      {
        "title": "43871573",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.8918",
        "sentences": [
          "change line 61 from sess = tf.interactivesession () to sess = tf.session () and rerun it on the command line."
        ]
      },
      {
        "title": "49943920",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "0.82",
        "prob": "0.7955639999999999",
        "sentences": [
          "replace tf.interactivesession () with with tf.session (): statement."
        ]
      },
      {
        "title": "46894945",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.83",
        "t_prob": "0.98",
        "r_prob": "0.97",
        "prob": "0.7889979999999999",
        "sentences": [
          "inside a with graph.as_default (): block .. ",
          "inside a with tf.session (): block .. ",
          "between creating a tf.interactivesession and calling sess.close ().. ",
          "each of these scenarios involves registering a default ( and potentially \" nested \") tf.graph object , which will be unregistered when you exit the block ( or close the tf.interactivesession ). ",
          "resetting the default graph in those scenarios would leave the system in an inconsistent state , so you should ensure to exit the block ( or close the tf.interactivesession ) before calling tf.reset_default_graph (). "
        ]
      },
      {
        "title": "53919276",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.79",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7584",
        "sentences": [
          "this function applies only to the current thread.",
          "calling this function while a tf.session or tf.interactivesession is active will result in undefined behavior."
        ]
      },
      {
        "title": "44897259",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.78",
        "t_prob": "0.96",
        "r_prob": "1.0",
        "prob": "0.7488",
        "sentences": [
          "you get an error because you use it in a session.",
          "calling this function while a tf.session or tf.interactivesession is active will result in undefined behavior."
        ]
      },
      {
        "title": "44360401",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.71",
        "r_prob": "0.99",
        "prob": "0.688842",
        "sentences": [
          "because when you use tf.interactivesession () to create a session , this interactivesession installs itself as the default session on construction.",
          "but if you are using tf.session (), you need to explicitly point out which session to use when run a operation.",
          "so if you use tf.session (), the code in your question is broken as the global variables initializer is not bound with your session."
        ]
      },
      {
        "title": "43118291",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.91",
        "r_prob": "0.84",
        "prob": "0.542724",
        "sentences": [
          "i want to give my resolution , it work when i replace the line [ session = tf.session ()] with [ sess = tf.interactivesession ()]. "
        ]
      },
      {
        "title": "34013098",
        "h": "tf.interactivesession",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.87",
        "r_prob": "0.57",
        "prob": "0.44631",
        "sentences": [
          "the failedpreconditionerror arises because the program is attempting to read a variable ( named \" variable_1 \") before it has been initialized.",
          "in tensorflow , all variables must be explicitly initialized , by running their \" initializer \" operations.",
          "note that this answer assumes that , as in the question , you are using tf.interactivesession , which allows you to run operations without specifying a session.",
          "for non - interactive uses , it is more common to use tf.session , and initialize as follows : "
        ]
      }
    ],
    "tf.reset_default_graph": [
      {
        "title": "53919276",
        "h": "tf.reset_default_graph",
        "t": "tf.interactivesession",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.7663",
        "sentences": [
          "tf.reset_default_graph will clear the default graph stack and resets the global default graph.",
          "this function applies only to the current thread.",
          "calling this function while a tf.session or tf.interactivesession is active will result in undefined behavior."
        ]
      },
      {
        "title": "46894945",
        "h": "tf.reset_default_graph",
        "t": "tf.interactivesession",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.83",
        "r_prob": "0.99",
        "prob": "0.764181",
        "sentences": [
          "this error message is displayed when you call tf.reset_default_graph () in one of the following scenarios : ",
          "inside a with tf.session (): block .. ",
          "between creating a tf.interactivesession and calling sess.close ().. ",
          "each of these scenarios involves registering a default ( and potentially \" nested \") tf.graph object , which will be unregistered when you exit the block ( or close the tf.interactivesession ). ",
          "resetting the default graph in those scenarios would leave the system in an inconsistent state , so you should ensure to exit the block ( or close the tf.interactivesession ) before calling tf.reset_default_graph (). "
        ]
      },
      {
        "title": "44897259",
        "h": "tf.reset_default_graph",
        "t": "tf.interactivesession",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.78",
        "r_prob": "1.0",
        "prob": "0.741",
        "sentences": [
          "you get an error because you use it in a session.",
          "from the tf.reset_default_graph () documentation : ",
          "calling this function while a tf.session or tf.interactivesession is active will result in undefined behavior.",
          "tf.reset_default_graph () can be helpful ( at least for me ) during the testing phase while i experiment in jupyter notebook."
        ]
      }
    ]
  },
  "to": {
    "tf.gradients": [
      {
        "title": "39974148",
        "h": "tf.gradients",
        "t": "to",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.0",
        "r_prob": "0.63",
        "prob": "0.0",
        "sentences": [
          "tf.gradients provides this functionality via its grad_ys argument , see here.",
          "in your case , tf.gradients ([ final_layer ], list_of_variables , grad_ys =[ _deriv ]) would compute the gradients you want.",
          "unfortunately , it looks like the build - in optimizers don ' t pass a grad_ys argument to tf.gradients."
        ]
      },
      {
        "title": "50080336",
        "h": "tf.gradients",
        "t": "to",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.0",
        "r_prob": "0.57",
        "prob": "0.0",
        "sentences": [
          "tf.gradients will sum over the gradients of the input tensor."
        ]
      },
      {
        "title": "46532900",
        "h": "tf.gradients",
        "t": "to",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.0",
        "r_prob": "0.57",
        "prob": "0.0",
        "sentences": [
          "by default tf.gradients takes the gradient of the scalar you get by summing all elements of all tensors passed to tf.gradients as outputs."
        ]
      }
    ]
  },
  "tf.nn.convolution": {},
  "tf.saved_model.main_op.main_op": {},
  "tf.tables_initializer": {},
  "tf.function": {
    "tf.print": [
      {
        "title": "57704523",
        "h": "tf.print",
        "t": "tf.function",
        "r": "S1",
        "h_prob": "0.81",
        "t_prob": "0.77",
        "r_prob": "1.0",
        "prob": "0.6237",
        "sentences": [
          "sometimes the way tf.function executes can cause us a little confusion - particularly when we mix in vanilla python operations such as print (). ",
          "we should remember that when we decorate a function with tf.function it ' s no longer just a python function.",
          "it behaves a little differently in order to enable fast and efficient use in tf.",
          "the first thing to note is that if we use tf.print () in place of print () then we get the expected output : ",
          "this means that the python only operations ( such as print () could get executed more than once ) but tf operations such as tf.print () will behave as you would normally expect.",
          "and i also want to know how tf.function woks when two functions call each other.",
          "in general , we need only decorate the \" outer \" function with tf.function (. fun () in your example ) but if you could call the inner function directly too then you are free to decorate that too."
        ]
      },
      {
        "title": "55303437",
        "h": "tf.print",
        "t": "tf.function",
        "r": "S1",
        "h_prob": "0.46",
        "t_prob": "0.84",
        "r_prob": "0.93",
        "prob": "0.35935200000000006",
        "sentences": [
          "inside the graph indicated by the decorator @ tf.function , you can use tf.print to print the values of your tensor."
        ]
      },
      {
        "title": "56307018",
        "h": "tf.print",
        "t": "tf.function",
        "r": "S1",
        "h_prob": "0.6",
        "t_prob": "0.7",
        "r_prob": "0.81",
        "prob": "0.3402",
        "sentences": [
          "what ' s the difference between tf.print and python print ?. ",
          "tf.print is a tensorflow construct , that prints on standard error by default and , more importantly , it produces an operation when evaluated.",
          "tf.function is able to capture the generated operation of tf.print and convert it to a graph node.",
          "therefore , tf.function is not able to convert it in its graph equivalent and executes it only during the function tracing.",
          "the above only apply when there ' s tf.function decorator ? ",
          "outside that , tf.print runs before python print ?. ",
          "yes.",
          "tf.print does not run before or after print.",
          "at any rate , i suggest you read the three articles linked since they cover in detail this and other peculiarities of tf.function."
        ]
      },
      {
        "title": "56900992",
        "h": "tf.function",
        "t": "tf.print",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.29",
        "r_prob": "1.0",
        "prob": "0.23199999999999998",
        "sentences": [
          "if you want to print in - graph , use tf.print."
        ]
      },
      {
        "title": "56306679",
        "h": "tf.print",
        "t": "tf.function",
        "r": "S1",
        "h_prob": "0.4",
        "t_prob": "0.59",
        "r_prob": "0.58",
        "prob": "0.13687999999999997",
        "sentences": [
          "that is why outside of your @ tf.function function , the output of the python print is a number ( tensorflow executes the graph directly and gives out the number to the normal print function ) and that is also why tf.print prints immediately.",
          "on the other hand , inside the @ tf.function function tensorflow will not execute the graph immediately."
        ]
      }
    ],
    "tf.variable": [
      {
        "title": "61875571",
        "h": "tf.variable",
        "t": "tf.function",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.82",
        "r_prob": "0.82",
        "prob": "0.53792",
        "sentences": [
          "elaborating jdehesa ' s comment for the benefit of the community.",
          "in the official documentation for tf.function , it is mentioned as : ",
          "it is always recommended to pass tensors as arguments to the functions instead of python scalars so that all the operations will be captured in a single graph.",
          "tf.function only allows creating new tf.variable objects when it is called for the first time.",
          "in general , it is recommended to create stateful objects like tf.variable outside of tf.function and passing them as arguments."
        ]
      },
      {
        "title": "61147928",
        "h": "tf.function",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.71",
        "t_prob": "0.79",
        "r_prob": "0.9",
        "prob": "0.50481",
        "sentences": [
          "i ' m assuming you want an instance of tf.variable only to use assign.",
          "however , when using tf.function , you should always provide variables from outside , and use built - in tensorflow data structures inside."
        ]
      },
      {
        "title": "59350211",
        "h": "tf.function",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.65",
        "t_prob": "0.93",
        "r_prob": "0.59",
        "prob": "0.356655",
        "sentences": [
          "the code in your first snippet ( the one without the @ tf.function ) takes advantage of tensorflow 2 ' s eager execution to manipulate a numpy array ( i.e ., your outer iteration object ) directly.",
          "with @ tf.function , this doesn ' t work because @ tf.function tries to compile your code into a tf.graph , which cannot operate on a numpy array directly ( it can only process tensorflow tensors ). ",
          "to get around this issue , use a tf.variable and keep assigning value into its slices.",
          "with @ tf.function , what you are trying to do is actually achievable with simpler code , by taking advantage of @ tf.function ' s automatic python - to - graph transformation feature ( known as autograph ). "
        ]
      }
    ]
  },
  "tf.global_norm": {},
  "tf.estimator.estimatorspec": {},
  "tf.saved_model.simple_save": {},
  "tf.saved_model.builder": {},
  "tf.sessions.variables": {},
  "tf.math.add": {},
  "tf.initializers.glorotnormal": {},
  "gpu set": {},
  "tf.mul": {},
  "tf.nn.local_response_normalization": {},
  "tf.nn.lrn": {},
  "tf.estimator.modekeys": {},
  "tf.nn.crelu": {},
  "tf.": {},
  "tf.contrib.framework.init_from_checkpoint": {},
  "tf.decode_json_example": {},
  "tf.stri": {},
  "tf.keras.layers.input": {},
  "batch -": {},
  "tf.audio.decode_wav": {},
  "tf.io.read_file": {},
  "tf.random": {},
  "rmsprop": {
    "learning rate": [
      {
        "title": "54579638",
        "h": "learning rate",
        "t": "rmsprop",
        "r": "S2",
        "h_prob": "0.73",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.6716",
        "sentences": [
          "you can do this by creating your own optimizer with a different learning rate.",
          "the rmsprop optimizer defaults to a learning rate of 0.001.",
          "if your images are in [ 0 , 1 ] then i suggest trying a higher learning rate , maybe 0.1."
        ]
      },
      {
        "title": "52356306",
        "h": "learning rate",
        "t": "rmsprop",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.84",
        "r_prob": "1.0",
        "prob": "0.6552",
        "sentences": [
          "you could also set a large initial learning rate and anneal it over the course of several training epochs by employing keras ' s learningratescheduler callback and defining a custom learning rate schedule ( for sgd ). "
        ]
      },
      {
        "title": "41488871",
        "h": "rmsprop",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.74",
        "r_prob": "0.99",
        "prob": "0.6153839999999999",
        "sentences": [
          "learning rate and batch size.",
          "learning rate and number of neurons.",
          "for the learning rate with adam and rmsprop , i found values around 0.001 to be optimal for most problems."
        ]
      }
    ]
  },
  "performance_": {},
  "tf.datasync": {},
  "estimator.train": {},
  "argmax": {
    "index": [
      {
        "title": "57516809",
        "h": "argmax",
        "t": "index",
        "r": "S2",
        "h_prob": "0.52",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.47840000000000005",
        "sentences": [
          "use np.argmax method to find the index with highest probability : "
        ]
      },
      {
        "title": "64204315",
        "h": "argmax",
        "t": "index",
        "r": "S2",
        "h_prob": "0.42",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.336",
        "sentences": [
          "use argmax () method to get the index with the largest value across axes of a tensor."
        ]
      },
      {
        "title": "48266090",
        "h": "argmax",
        "t": "index",
        "r": "S2",
        "h_prob": "0.37",
        "t_prob": "0.75",
        "r_prob": "0.68",
        "prob": "0.18869999999999998",
        "sentences": [
          "setting y_true [ i ][ 1 ] = 1 makes sure that j <= 1 ( because np.argmax picks the smallest max index ), which allows to bypass keras guards."
        ]
      }
    ]
  },
  "index": {
    "argmax": [
      {
        "title": "57516809",
        "h": "argmax",
        "t": "index",
        "r": "S2",
        "h_prob": "0.52",
        "t_prob": "0.92",
        "r_prob": "1.0",
        "prob": "0.47840000000000005",
        "sentences": [
          "use np.argmax method to find the index with highest probability : "
        ]
      },
      {
        "title": "64204315",
        "h": "argmax",
        "t": "index",
        "r": "S2",
        "h_prob": "0.42",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.336",
        "sentences": [
          "use argmax () method to get the index with the largest value across axes of a tensor."
        ]
      },
      {
        "title": "48266090",
        "h": "argmax",
        "t": "index",
        "r": "S2",
        "h_prob": "0.37",
        "t_prob": "0.75",
        "r_prob": "0.68",
        "prob": "0.18869999999999998",
        "sentences": [
          "setting y_true [ i ][ 1 ] = 1 makes sure that j <= 1 ( because np.argmax picks the smallest max index ), which allows to bypass keras guards."
        ]
      }
    ]
  },
  "tf.feature_column": {},
  "tf.estimator.dnnclassifier": {},
  "tf.edit_distance": {},
  "logit_": {},
  "tf.spectral": {},
  "tf.image_summary": {},
  "softmax cross entropy": {},
  "tf.keras.layers.conv1d": {},
  "tf.keras.preprocessing.sequence.pad_sequences": {},
  "gradient descent": {
    "learning rate": [
      {
        "title": "56360009",
        "h": "learning rate",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.7",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.63",
        "sentences": [
          "the learning rate is analogous with the size of the step."
        ]
      },
      {
        "title": "57284025",
        "h": "learning rate",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.7",
        "t_prob": "0.9",
        "r_prob": "1.0",
        "prob": "0.63",
        "sentences": [
          "the learning rate is analogous with the size of the step."
        ]
      },
      {
        "title": "53549555",
        "h": "learning rate",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.74",
        "t_prob": "0.82",
        "r_prob": "0.98",
        "prob": "0.594664",
        "sentences": [
          "this is because gradient descent is an iterative technique."
        ]
      },
      {
        "title": "50717404",
        "h": "gradient descent",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.71",
        "r_prob": "0.9",
        "prob": "0.48563999999999996",
        "sentences": [
          "your initial learning rate for the gradient descent is just too big for it to converge toward a minima ( see for instance this other thread about gradient descent and learning rate values : \" gradient descent explodes if learning rate is too large \"). "
        ]
      },
      {
        "title": "35050158",
        "h": "learning rate",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.72",
        "r_prob": "0.99",
        "prob": "0.449064",
        "sentences": [
          "if you use online gradient descent , it can be as simple as using a larger / smaller learning rate when seeing imbalanced examples."
        ]
      },
      {
        "title": "39315068",
        "h": "gradient descent",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.79",
        "t_prob": "0.74",
        "r_prob": "0.58",
        "prob": "0.339068",
        "sentences": [
          "you are overflowing float32 because the learning rate is too high for your problem , and instead of converging the weight variable ( w ) is oscillating towards larger and larger magnitudes on each step of gradient descent."
        ]
      },
      {
        "title": "33719722",
        "h": "learning rate",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.6",
        "t_prob": "0.74",
        "r_prob": "0.71",
        "prob": "0.31523999999999996",
        "sentences": [
          "he already suggested to decrease the learning rate.",
          "gradient descent is the most basic algorithm."
        ]
      },
      {
        "title": "41442953",
        "h": "learning rate",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.9",
        "r_prob": "0.52",
        "prob": "0.29484000000000005",
        "sentences": [
          "so yes , it does not make much sense to use exponential decay on adamoptimizer but on gradient descent or momentum optimizer."
        ]
      },
      {
        "title": "46655987",
        "h": "gradient descent",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.63",
        "t_prob": "0.66",
        "r_prob": "0.64",
        "prob": "0.266112",
        "sentences": [
          "for what concerns adagrad , let me remember what a standard gradient descent update step looks like : ",
          "often , people use a particular scheduling for the learning rate eta because they need a larger eta in the initial phase of learning , while a smaller eta in the final phase ( when you are very close to the minimum and you want to avoid oscillating around it ). "
        ]
      }
    ],
    "batch size": [
      {
        "title": "38840883",
        "h": "batch size",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.82",
        "t_prob": "0.68",
        "r_prob": "1.0",
        "prob": "0.5576",
        "sentences": [
          "there is a batch size , because it uses mini - batch gradient descent."
        ]
      },
      {
        "title": "38047478",
        "h": "batch size",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.72",
        "r_prob": "0.91",
        "prob": "0.550368",
        "sentences": [
          "switching from gradient descent to stochastic gradient descent you need to keep some things in mind.",
          "it may be worth looking further into differences between gradient descent and stochastic gradient descent."
        ]
      },
      {
        "title": "44471919",
        "h": "gradient descent",
        "t": "batch size",
        "r": "S2",
        "h_prob": "0.59",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.48379999999999995",
        "sentences": [
          "it is just a way to train models faster ( mini - batch gradient descent ) "
        ]
      },
      {
        "title": "44543550",
        "h": "batch size",
        "t": "gradient descent",
        "r": "S2",
        "h_prob": "0.84",
        "t_prob": "0.74",
        "r_prob": "0.6",
        "prob": "0.37295999999999996",
        "sentences": [
          "usually , this loop is based on the gradient descent algorithm.",
          "there are many variations on gradient descent ( batch , stochastic , mini - batch ) as well as other algorithms for optimizing the learning parameters ( e.g ., l - bfgs ). "
        ]
      }
    ]
  },
  "tf.keras.callbacks.learningratescheduler": {},
  "tf.data.tfrecorddataset": {
    "tf.data.dataset": [
      {
        "title": "55219739",
        "h": "tf.data.tfrecorddataset",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.97",
        "r_prob": "1.0",
        "prob": "0.8827",
        "sentences": [
          "in the real use case you would replace data with the list of file names and tf.data.dataset.from_tensor_slices ( string_data ( d )) with tf.data.tfrecorddataset ( d ), but otherwise it should work similarly."
        ]
      },
      {
        "title": "54399204",
        "h": "tf.data.tfrecorddataset",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "1.0",
        "r_prob": "0.99",
        "prob": "0.8712",
        "sentences": [
          "you can use tf.data.dataset.batch ( batch_size = train_batch_size ) for batching the input data but for that frist you have to create a dataset from your input data by using the relevant method for your data for example dataset = tf.data.tfrecorddataset ( filename ). "
        ]
      },
      {
        "title": "62083141",
        "h": "tf.data.tfrecorddataset",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.7789999999999999",
        "sentences": [
          "for example , you could pass the queue as a generator into the dataset api ( see tf.data.dataset.from_generator ). ",
          "for a tensorflow implementation , you could use tf.data.dataset.shard with tf.data.tfrecorddataset."
        ]
      }
    ]
  },
  "tf.distributions.multinomial": {},
  "object detection": {},
  "tf.contrib.eager.iterator": {},
  "tf.nn.sparse_softmax_corss_": {},
  "tf.metrics .": {},
  "tf.metrics.precision_at_thresholds": {},
  "tf.compat.v1.configproto": {},
  "tf.string.split": {},
  "tf.train.match_filenames_once": {},
  "tf.size": {},
  "tf.qr": {},
  "tf.keras.backend.function": {},
  "square": {},
  "tf.metrics.specificity_at_sensitivity": {},
  "tf.metrics": {},
  "tf.autograph.to_code": {},
  "tf.test.gpu_device_name": {},
  "tf.compat.v1.session": {
    "tf.session": [
      {
        "title": "55143329",
        "h": "tf.session",
        "t": "tf.compat.v1.session",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.89",
        "r_prob": "0.99",
        "prob": "0.872289",
        "sentences": [
          "according to tf 1 : 1 symbols map , in tf 2.0 you should use tf.compat.v1.session () instead of tf.session () ",
          "to get tf 1.x like behaviour in tf 2.0 one can run "
        ]
      },
      {
        "title": "58621799",
        "h": "tf.session",
        "t": "tf.compat.v1.session",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.89",
        "r_prob": "0.99",
        "prob": "0.854667",
        "sentences": [
          "answer : in tf 2.0 you should use tf.compat.v1.session () instead of tf.session () use the following code to get rid of the error in tensorflow2.0 : "
        ]
      },
      {
        "title": "59390988",
        "h": "tf.session",
        "t": "tf.compat.v1.session",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.84",
        "r_prob": "0.97",
        "prob": "0.7414679999999999",
        "sentences": [
          "attributeerror : module ' tensorflow ' has no attribute ' session ' ",
          "you need to use tf.compat.v1.session () as tf.session is deprecated."
        ]
      },
      {
        "title": "54646355",
        "h": "tf.compat.v1.session",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.6298",
        "sentences": [
          "if you use the tf 2.0 upgrade script on your original tensorflow model , tf.session ( config =...) should change to tf.compat.v1.session ( config =...) and work as expected."
        ]
      },
      {
        "title": "61896836",
        "h": "tf.session",
        "t": "tf.compat.v1.session",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.67",
        "r_prob": "0.99",
        "prob": "0.610236",
        "sentences": [
          "if eager execution is disabled , you can build a graph and then run it through tf.compat.v1.session in tf 2.x and tf.session in tf 1.x "
        ]
      },
      {
        "title": "59432802",
        "h": "tf.session",
        "t": "tf.compat.v1.session",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.54",
        "r_prob": "0.92",
        "prob": "0.48189600000000005",
        "sentences": [
          "david valenzuela urrutia ' s answer was for python 3.6.3 , tensorflow 1.4.0 so i thought of updating the answer ( code samples ) to tensorflow 2.x because some funtionalities like tf.session is not supported in tensorflow version 2 so you need to replace it with tf.compat.v1.session for it to work."
        ]
      },
      {
        "title": "57562774",
        "h": "tf.session",
        "t": "tf.compat.v1.session",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.5",
        "r_prob": "0.85",
        "prob": "0.38675",
        "sentences": [
          "therefore , if you want you code to be compatible with tensorflow 2.0 , you should use tf.compat.v1.session instead.",
          "so , just change this line : "
        ]
      }
    ]
  },
  "tf.keras.layers.embedding": {},
  "tf.contrib.resampler": {},
  "tf.errors.outofrangeerror": {},
  "tf.split_v": {},
  "divide": {},
  "tf.strided_slice": {},
  "tf.keras.layers.flatten": {},
  "tf.variable_scope": {
    "tf.name_scope": [
      {
        "title": "37670385",
        "h": "tf.name_scope",
        "t": "tf.variable_scope",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.79",
        "r_prob": "1.0",
        "prob": "0.7189000000000001",
        "sentences": [
          "tf.name_scope () is used to visualize variables.",
          "tf.name_scope ( name ) ",
          "wrapper for graph.name_scope () using the default graph .. ",
          "what i think you are looking for is tf.variable_scope (): ",
          "tf.get_variable (, , ): creates or returns a variable with a given name.",
          "tf.variable_scope (): manages namespaces for names passed to tf.get_variable (). "
        ]
      },
      {
        "title": "47153523",
        "h": "tf.name_scope",
        "t": "tf.variable_scope",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.87",
        "r_prob": "0.89",
        "prob": "0.704613",
        "sentences": [
          "change tf.name_scope ( timeseriesname ) to tf.variable_scope ( timeseriesname ). ",
          "the difference between tf.name_scope and tf.variable_scope is discussed in this quesion."
        ]
      },
      {
        "title": "48194899",
        "h": "tf.name_scope",
        "t": "tf.variable_scope",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.77",
        "r_prob": "1.0",
        "prob": "0.5929",
        "sentences": [
          "ok i ' ve found the issue apparently tf.name_scope is for operation only and tf.variable_scope works for both operations and variables ( as per this tf issue ). "
        ]
      },
      {
        "title": "52277944",
        "h": "tf.variable_scope",
        "t": "tf.name_scope",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.68",
        "r_prob": "0.88",
        "prob": "0.5804480000000001",
        "sentences": [
          "tf.variable_scope is an evolution of tf.name_scope to handle variable reuse.",
          "as you noticed , it does more than tf.name_scope , so there is no real reason to use tf.name_scope : not surprisingly , a tf developper advises to just use tf.variable_scope.",
          "my understanding for having tf.name_scope still lying around is that there are subtle incompatibilities in the behavior of those two , which invalidates tf.variable_scope as a drop - in replacement for tf.name_scope."
        ]
      },
      {
        "title": "43343662",
        "h": "tf.name_scope",
        "t": "tf.variable_scope",
        "r": "S1",
        "h_prob": "0.61",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.49410000000000004",
        "sentences": [
          "tf.name_scope defines a prefix for the operations defined within the scope.",
          "tf.variable_scope defines a prefix for the operations and variables defined within the scope.",
          "you have to use tf.variable_scope if you want to create a variable with the same name of another variable but in a different scope.",
          "tf.name_scope is used to define custom operations in order to well define the context.",
          "personally , i use tf.variable_scope almost always.",
          "moreover , yes , tf.variable_scope creates good looking graph in tensorboard exactly as tf.named_scope "
        ]
      },
      {
        "title": "34606240",
        "h": "tf.name_scope",
        "t": "tf.variable_scope",
        "r": "S1",
        "h_prob": "0.79",
        "t_prob": "0.81",
        "r_prob": "0.62",
        "prob": "0.39673800000000004",
        "sentences": [
          "every time you create a new name scope ( with tf.name_scope ()) or variable scope ( with tf.variable_scope ()) a unique suffix is added to the current name scope , based on the given string , possibly with an additional suffix to make it unique.",
          "when you enter a new name scope ( by entering a with tf.name_scope (\"...\"): or with tf.variable_scope (\"...\"): block ), tensorflow creates a new , unique name for the scope."
        ]
      }
    ]
  },
  "tf.contrib.rnn.static_bidirectional_rnn": {},
  "tf.initialize_local_variables": {},
  "tf.floor_div": {},
  "tf.floordiv": {},
  "tf.keras.models.model": {},
  "cost function": {},
  "tf.reduce_max": {},
  "tf.metrics.average_precision_at_k": {},
  "tf.metrics.sparse_average_precision_at_k": {},
  "strides": {},
  "np.float": {},
  "np.prod": {},
  "instances": {},
  "feedforward neural network": {},
  "tf.summary.tensor_summary": {},
  "model": {},
  "tf.math.xlogy": {},
  "logic": {},
  "tf.bincount": {},
  "tf.ne": {},
  "tf.variable_op_scope": {},
  "tf.image.decode_image": {},
  "tf.feature_column.input_layer": {},
  "tf.feat": {},
  "tf.contrib.metrics.accuracy": {},
  "tf.contrib.autograph": {},
  "tf.con": {},
  "tf.truediv": {},
  "environment": {},
  "vm": {},
  "tf.nn.batch_normalization": {
    "tf.layers.batch_normalization": [
      {
        "title": "48006315",
        "h": "tf.nn.batch_normalization",
        "t": "tf.layers.batch_normalization",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.91",
        "r_prob": "0.99",
        "prob": "0.8828820000000001",
        "sentences": [
          "just to add to the list , there ' re several more ways to do batch - norm in tensorflow : ",
          "tf.nn.batch_normalization is a low - level op.",
          "the difference is that it ' s optimized for 4d input tensors , which is the usual case in convolutional neural networks.",
          "tf.nn.batch_normalization accepts tensors of any rank greater than 1 .. ",
          "tf.layers.batch_normalization is a high - level wrapper over the previous ops.",
          "tf.contrib.layers.batch_norm is the early implementation of batch norm , before it ' s graduated to the core api ( i.e ., tf.layers ). ",
          "finally , there ' s also keras layer keras.layers.batchnormalization , which in case of tensorflow backend invokes tf.nn.batch_normalization .. "
        ]
      },
      {
        "title": "49531019",
        "h": "tf.nn.batch_normalization",
        "t": "tf.layers.batch_normalization",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.88",
        "r_prob": "1.0",
        "prob": "0.8712",
        "sentences": [
          "there is a big difference between tf.nn.batch_normalization and tf.layers.batch_normalization.",
          "you might not even use batches as input , so there would be no minibatch statistics ). ",
          "by default , tensorflow only executes what it needs to.",
          "the tf.control_dependencies context manager forces tensorflow to do the updates every time it computes whatever is in the code block ( in this case the cost ). "
        ]
      },
      {
        "title": "47984274",
        "h": "tf.nn.batch_normalization",
        "t": "tf.layers.batch_normalization",
        "r": "S1",
        "h_prob": "0.99",
        "t_prob": "0.98",
        "r_prob": "0.82",
        "prob": "0.7955639999999999",
        "sentences": [
          "simply use tf.layers.batch_normalization.",
          "it also creates variables via tf.get_variable (), hence they can be shared as well.",
          "in addition , it works seamlessly with tf.layers.conv * functions.",
          "update : tf.nn.batch_normalization is fine too.",
          "it ' s a more low - level function that requires you manage mean and variance tensors yourself.",
          "in fact , tf.layers.batch_normalization is a wrapper over tf.nn."
        ]
      }
    ]
  },
  "tf.contrib.tpu": {},
  "tf.contrib.lite": {},
  "record": {},
  "tf.recor": {},
  "tf.mer": {},
  "tf.estimator.export.predictoutput": {},
  "tensors": {},
  "l2 norm": {},
  "tf.auto": {},
  "tf.image.convert_image_dtype": {},
  "tf.keras.input": {},
  "tf.nn.rnn_cell.multirnncell": {},
  "uniform": {},
  "tf.compat.v2": {},
  "tf.train.optimizer": {},
  "tf.uniqu": {},
  "tf.contrib.data": {
    "tf.data.dataset": [
      {
        "title": "48031461",
        "h": "tf.data.dataset",
        "t": "tf.contrib.data",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.98",
        "r_prob": "1.0",
        "prob": "0.9603999999999999",
        "sentences": [
          "tf.contrib.data has been deprecated and been removed ( check here ). ",
          "you need to upgrade the tensorflow version using : sudo pip3 install -- upgrade tensorflow.",
          "check the installation guide here.",
          "open the python terminal and type ",
          "import tensorflow as tf ",
          "dataset = tf.data.dataset ",
          "hope it will help."
        ]
      },
      {
        "title": "46763985",
        "h": "tf.contrib.data",
        "t": "tf.data.dataset",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.99",
        "r_prob": "0.99",
        "prob": "0.940896",
        "sentences": [
          "alternatively , i ' d encourage you to check out the tf.data.dataset interface ( possible tf.contrib.data.dataset in tensorflow 1.3 or prior ). "
        ]
      },
      {
        "title": "51179552",
        "h": "tf.data.dataset",
        "t": "tf.contrib.data",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.8",
        "r_prob": "1.0",
        "prob": "0.776",
        "sentences": [
          "as far as i know a recent update completely removed most \" standard \" ops from tf.contrib.data.",
          "it now only contains \" experimental \"/ volatile code.",
          "simply use tf.data.dataset instead."
        ]
      }
    ]
  },
  "tf.keras.": {},
  "scatter": {},
  "tf.batch_gather": {},
  "tf.contrib.distributions": {},
  "tf.distribution": {},
  "tf.int64": {},
  "tf.image.extract_patches": {},
  "tf.container": {},
  "tf.l": {},
  "machine learning": {
    "learning rate": [
      {
        "title": "46878756",
        "h": "learning rate",
        "t": "machine learning",
        "r": "S2",
        "h_prob": "0.89",
        "t_prob": "0.93",
        "r_prob": "0.77",
        "prob": "0.6373290000000001",
        "sentences": [
          "in machine learning , placeholders are usually used for nodes that hold data , because we may want to run the same graph again and again , in a loop , with different parts of our dataset.",
          "people also use placeholders for parameters that change during training , like the learning rate."
        ]
      },
      {
        "title": "39315068",
        "h": "machine learning",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.74",
        "r_prob": "0.87",
        "prob": "0.54723",
        "sentences": [
          "picking a good learning rate is often the first challenge when implementing or using a machine learning algorithm.",
          "getting increased loss values instead of converging to a minimum is usually a sign that learning rate is too high."
        ]
      },
      {
        "title": "46844258",
        "h": "machine learning",
        "t": "learning rate",
        "r": "S2",
        "h_prob": "0.78",
        "t_prob": "0.64",
        "r_prob": "0.53",
        "prob": "0.26457600000000003",
        "sentences": [
          "just a note : since in every machine learning framework dropout is implemented in its \" inverted \" version , you should have to lower your learning rate in order to overcome the \" boost \" that the dropout probability gives to the learning rate."
        ]
      }
    ],
    "neural networks": [
      {
        "title": "62399725",
        "h": "machine learning",
        "t": "neural networks",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.77",
        "r_prob": "1.0",
        "prob": "0.5852",
        "sentences": [
          "to represent any point in this space , say d1 , we need a tuple of three real numbers ( x1 , y1 , z1 ). "
        ]
      },
      {
        "title": "51938945",
        "h": "machine learning",
        "t": "neural networks",
        "r": "S2",
        "h_prob": "0.76",
        "t_prob": "0.79",
        "r_prob": "0.94",
        "prob": "0.564376",
        "sentences": [
          "this isn ' t how neural networks , or machine learning in general works."
        ]
      },
      {
        "title": "53239472",
        "h": "neural networks",
        "t": "machine learning",
        "r": "S1",
        "h_prob": "0.64",
        "t_prob": "0.62",
        "r_prob": "0.94",
        "prob": "0.372992",
        "sentences": [
          "high - level object oriented libraries that bring about abstraction when developing neural networks ( nn ) or other machine learning ( ml ) algorithms .. "
        ]
      }
    ],
    "deep learning": [
      {
        "title": "45979913",
        "h": "machine learning",
        "t": "deep learning",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.82",
        "r_prob": "0.82",
        "prob": "0.53792",
        "sentences": [
          "image encoding using a cnn model ( openface for instance ).",
          "image search using a voisinage algorithm ( knn , lshforest , etc ).",
          "here ' s a blog article that draws a nice walk through the steps required to do face - based user identification : ",
          "machine learning is fun part 4 modern face recognition with deep learning."
        ]
      },
      {
        "title": "56208907",
        "h": "machine learning",
        "t": "deep learning",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.9",
        "r_prob": "0.61",
        "prob": "0.5050800000000001",
        "sentences": [
          "gal , yarin , and zoubin ghahramani.",
          "\" dropout as a bayesian approximation : representing model uncertainty in deep learning .\" ",
          "international conference on machine learning.",
          "2016."
        ]
      },
      {
        "title": "61549702",
        "h": "machine learning",
        "t": "deep learning",
        "r": "S1",
        "h_prob": "0.66",
        "t_prob": "0.61",
        "r_prob": "0.95",
        "prob": "0.38247",
        "sentences": [
          "machine learning - based approach : https :// www.researchgate.net / publication / 266672947_estimating_smile_intensity_a_better_way ",
          "deep learning ( cnn ): https :// arxiv.org / pdf / 1602.00172.pdf ",
          "if you have the label for smiling intensity , you can just solve it as a regression problem , the cnn will have one output , will try to regress the smile intensity ( the normalized smile intensity with sigmoid in this case ). "
        ]
      },
      {
        "title": "38875433",
        "h": "machine learning",
        "t": "deep learning",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.7",
        "r_prob": "0.55",
        "prob": "0.35805000000000003",
        "sentences": [
          "it ' s originally called skflow ( scikit flow ). ",
          "it is for both deep learning as well as general machine learning."
        ]
      },
      {
        "title": "59779253",
        "h": "machine learning",
        "t": "deep learning",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.89",
        "r_prob": "0.55",
        "prob": "0.357335",
        "sentences": [
          ". ",
          "your dataset size is obviously too small for deep learning.",
          "adding more training data or try traditional machine learning like svm , lr .. "
        ]
      }
    ]
  },
  "tf.python.tools.optimiz": {},
  "tf.python.tools.free": {},
  "tf.extract_image_patches": {},
  "tf.data.experimental.sample_from_datasets": {},
  "tf.sli": {},
  "standard deviation": {},
  "logarith": {},
  "log_prob": {},
  "jacobian": {},
  "context": {},
  "tf.fixedlenfeature": {},
  "clip": {},
  "batch": {},
  "tf.config.experimental.set_memory_growth": {},
  "tf.squared_difference": {},
  "tf.reduce_prod": {},
  "tf.contrib.rnn.basiclstmcell": {},
  "tf.contrib.rnn.lstmcell": {},
  "tf.train.profilerhook": {},
  "tf.initializers.random_uniform": {},
  "tf.train.server": {},
  "target": {},
  "optimization": {},
  "tf.unique": {},
  "examples": {},
  "tf.foldl": {},
  "tf.py_": {},
  "tf.data.experimental.parse_example_dataset": {},
  "tf.io.parse_example": {},
  "estimator.train ()": {},
  "tf.losses.get_regularization_loss": {},
  "tf.compat.v1.layers": {},
  "tf.contrib.layers.sparse_column_with_hash_bucket": {},
  "tf.numpy_function": {},
  "tf.train.newcheckpointreader": {},
  "tf.train.batch_join": {},
  "tf.get_default_session": {},
  "tf.contrib.lookup": {},
  "tf.equal": {
    "tf.argmax": [
      {
        "title": "47053144",
        "h": "tf.equal",
        "t": "tf.argmax",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.97",
        "r_prob": "0.98",
        "prob": "0.874552",
        "sentences": [
          "try checking the axis along which you need to find the maximum.",
          "probably it should be : ",
          "correct_pred = tf.equal ( tf.argmax ( prediction , axis = 1 ), tf.argmax ( y , axis = 1 )) "
        ]
      },
      {
        "title": "43428933",
        "h": "tf.equal",
        "t": "tf.argmax",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.97",
        "r_prob": "0.9",
        "prob": "0.81189",
        "sentences": [
          "for computation accuracy , i change ",
          "correct_pred = tf.equal ( tf.argmax ( self.score_ , 1 ), tf.argmax ( y , 1 )) ",
          "to ",
          "correct_pred = tf.equal ( tf.argmax ( self.score_ , 1 ), y )) "
        ]
      },
      {
        "title": "41863099",
        "h": "tf.argmax",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.82",
        "r_prob": "0.99",
        "prob": "0.77121",
        "sentences": [
          "evaluating tf.argmax ( pred , 1 ) gives a tensor whose evaluation will give array ([ 5 , 5 , 2 , 1 , 3 , 0 ]) ",
          "evaluating tf.argmax ( y , 1 ) gives a tensor whose evaluation will give array ([ 5 , 5 , 2 , 1 , 3 , 0 ]) ",
          "following our example , tf.equal ( tf.argmax ( pred , 1 ), tf.argmax ( y , 1 )) returns a tensor whose evaluation will givearray ( 1 , 1 , 1 , 1 , 1 , 1 ). ",
          "y_test_prediction can be obtained by executing pred = tf.argmax ( logits , 1 ) ",
          "the documentation for tf.argmax and tf.equal can be accessed by following the links below.",
          "tf.argmax () https :// www.tensorflow.org / api_docs / python / math_ops / sequence_comparison_and_indexing   argmax ",
          "tf.equal () https :// www.tensorflow.org / versions / master / api_docs / python / control_flow_ops / comparison_operators   equal "
        ]
      },
      {
        "title": "49582606",
        "h": "tf.argmax",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.91",
        "r_prob": "0.8",
        "prob": "0.6988800000000001",
        "sentences": [
          "if you are using sigmoids , your outputs will be each ( independently ) be between 0 and 1.",
          "in that case , you would remove the tf.argmax call and instead check if the preds and label are exactly the same vectors , which would look like tf.reduce_all ( tf.equal ( preds , label ), axis = 0 ). ",
          "for the latter , the code would look like tf.reduce_sum ( tf.equal ( preds , label ), axis = 0 ). "
        ]
      },
      {
        "title": "34080116",
        "h": "tf.argmax",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.84",
        "r_prob": "0.79",
        "prob": "0.650328",
        "sentences": [
          "in other words , the tensor defined with correct_prediction = tf.equal ( tf.argmax ( y , 1 ), tf.argmax ( y_ , 1 )) doesn ' t contain the list of booleans , it contains the instructions for computing it in a tensorflow graph.",
          "in order to get the actual values , you need to tell tensorflow to compute it in a graph.",
          "first , you need a tf.session variable.",
          "a simple way to get it for testing in a interactive shell is sess = tf.interactivesession (), followed by variable initialization : sess.run ( tf.initialize_all_variables ()). "
        ]
      },
      {
        "title": "51343481",
        "h": "tf.argmax",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.83",
        "r_prob": "0.85",
        "prob": "0.64906",
        "sentences": [
          "tf.equal ( tf.argmax ( prediction , 1 , name =\" argmax_pred \"), tf.argmax ( y , 1 , name =\" y_pred \"), name =\" correct_pred \"). ",
          "this means that when you try to run correct_pred in your android code , it will attempt to compute tf.equal ( tf.argmax ( prediction , 1 , name =\" argmax_pred \"), tf.argmax ( y , 1 , name =\" y_pred \"). "
        ]
      },
      {
        "title": "49614687",
        "h": "tf.argmax",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.97",
        "t_prob": "0.8",
        "r_prob": "0.82",
        "prob": "0.63632",
        "sentences": [
          "replace your _ , c = sess.run ([ optimizer , cost ], ...) with _ , c , p = sess.run ([ optimizer , cost , predictions ], ...). ",
          "if you want to do it in tensorflow , move the correct prediction and accuracy computations up to where you compute cost , and change your sess.run line to : _ , c , a = sess.run ([ optimizer , cost , accuracy ], ...) "
        ]
      },
      {
        "title": "42414255",
        "h": "tf.argmax",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.71",
        "r_prob": "0.92",
        "prob": "0.62054",
        "sentences": [
          "mnist usually has one - hot coded output.",
          "in that case correct_prediction = tf.equal ( tf.argmax ( prediction , 1 ), tf.argmax ( y , 1 )) makes sense as tf.argmax converts the one - hot code to the actual class which is then compared by tf.equal."
        ]
      },
      {
        "title": "44571038",
        "h": "tf.equal",
        "t": "tf.argmax",
        "r": "S1",
        "h_prob": "0.62",
        "t_prob": "0.97",
        "r_prob": "0.78",
        "prob": "0.46909199999999995",
        "sentences": [
          "the best way i see is using a mask and tf.boolean_mask k times , with the i - th mask being given by tf.equal ( i , tf.argmax ( z , axis =- 1 )) "
        ]
      },
      {
        "title": "43025066",
        "h": "tf.equal",
        "t": "tf.argmax",
        "r": "S1",
        "h_prob": "0.44",
        "t_prob": "0.95",
        "r_prob": "0.99",
        "prob": "0.41381999999999997",
        "sentences": [
          "so besically , correct_pred is just 1 number ( 0 or 1 ) because it is based on one picture ( so if tf.argmax ( model_op , 1 )= tf.argmax ( y , 1 ) then correct_pred = 1.",
          "otherwise , it equals 0 ). ",
          "the array correct_pred is just a 0 - 1 vector ( because it is a result of tf.equal ). "
        ]
      }
    ],
    "tf.reduce_all": [
      {
        "title": "42320470",
        "h": "tf.equal",
        "t": "tf.reduce_all",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.82",
        "r_prob": "1.0",
        "prob": "0.7871999999999999",
        "sentences": [
          "the problem arises because tf.equal () is an elementwise operation and it returns a tensor with the same shape as its arguments.",
          "the easiest way to fix your expression is to use tf.reduce_all () to aggregate the results of tf.equal () down to a scalar before computing the tf.logical_and (), as follows : "
        ]
      },
      {
        "title": "49582606",
        "h": "tf.reduce_all",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.91",
        "r_prob": "0.92",
        "prob": "0.7785960000000001",
        "sentences": [
          "if you are using sigmoids , your outputs will be each ( independently ) be between 0 and 1.",
          "in that case , you would remove the tf.argmax call and instead check if the preds and label are exactly the same vectors , which would look like tf.reduce_all ( tf.equal ( preds , label ), axis = 0 ). ",
          "for the latter , the code would look like tf.reduce_sum ( tf.equal ( preds , label ), axis = 0 ). "
        ]
      },
      {
        "title": "41877446",
        "h": "tf.reduce_all",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.81",
        "r_prob": "1.0",
        "prob": "0.7209000000000001",
        "sentences": [
          "the tf.equal () operator is an elementwise operator.",
          "therefore , sess.run ( tf.equal ( y , y )) in your program will return the array [ true , true , true ]. ",
          "* note however that tf.equal ( x , y ) will broadcast its arguments if they have different shapes , so you might get the unexpected result that two tensors with different shapes are \" equal \" using this program."
        ]
      }
    ],
    "tf.where": [
      {
        "title": "58407536",
        "h": "tf.equal",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.77",
        "t_prob": "0.94",
        "r_prob": "1.0",
        "prob": "0.7238",
        "sentences": [
          "try to use tf.where and tf.equal instead of use if else statement."
        ]
      },
      {
        "title": "54345492",
        "h": "tf.where",
        "t": "tf.equal",
        "r": "S1",
        "h_prob": "0.95",
        "t_prob": "0.9",
        "r_prob": "0.72",
        "prob": "0.6155999999999999",
        "sentences": [
          "here is one possible solution with tf.equal , tf.where , tf.scater_nd_update , tf.gather_nd and tf.reverse_v2 : "
        ]
      },
      {
        "title": "52815622",
        "h": "tf.equal",
        "t": "tf.where",
        "r": "S1",
        "h_prob": "0.91",
        "t_prob": "0.94",
        "r_prob": "0.65",
        "prob": "0.55601",
        "sentences": [
          "you can just compare with tf.equal and then convert the boolean result to a number with tf.cast : "
        ]
      }
    ]
  },
  "matplotlib": {},
  "perceptron": {},
  "tf.nn.bidirectional_dynamic_rnn": {},
  "tf.contrib.rnn.stack_bidirectional_dynamic_rnn": {},
  "tf.random_gamma": {},
  "vgg": {},
  "tf.train.adamoptimizer": {},
  "tf.count_nonzero": {},
  "hyperparameter": {},
  "tf.bitcast": {},
  "tf.nn.rnn creates": {},
  "function": {},
  "tf.train.rmspropoptimizer": {},
  "tf.initializers.constant": {},
  "tf.contrib nam": {},
  "weight w": {},
  "tf.image.extract_glimpse": {},
  "tf.nn.conv1d": {},
  "tf.keras.utils.get_file": {},
  "tf.image.rot90": {},
  "tf.case": {},
  "pooling layers": {},
  "tf.matrix_inverse": {},
  "tf.linalg.inv": {},
  "strip": {},
  "names": {},
  "tf.train.summary_iterator": {},
  "tf.summary": {},
  "tf.keras.losses.sparse_categorical_crossentropy": {},
  "tf.keras.losses.mse": {},
  "tf.compat.v2.nn.softmax_cross_entropy_with_logits": {},
  "tf.math.reduce_sum": {},
  "tf.contrib.distribute": {},
  "tf.distribute": {},
  "validation set": {},
  "broadcasting": {
    "batch size": [
      {
        "title": "34220963",
        "h": "batch size",
        "t": "broadcasting",
        "r": "S2",
        "h_prob": "0.94",
        "t_prob": "0.71",
        "r_prob": "1.0",
        "prob": "0.6673999999999999",
        "sentences": [
          "the tf.mul () operator supports numpy - style broadcasting , which would allow you to simplify and optimize the code slightly.",
          "(* this would work equally if output_img were a 4 - d b x m x n x c ( for any number of channels c ) or 3 - d m x n x c tensor , due to the way broadcasting works .) "
        ]
      },
      {
        "title": "60531920",
        "h": "batch size",
        "t": "broadcasting",
        "r": "S2",
        "h_prob": "0.85",
        "t_prob": "0.8",
        "r_prob": "0.93",
        "prob": "0.6324000000000001",
        "sentences": [
          "notice that it ' s very important that you understand where your batch size is , and that a layer cannot have weights with sizes based on the batch size ( unless you define your inputs with batch_shape or batch_input_shape instead of shape -- this will force you to use a fixed batch size in the model ). ",
          "because of this , we will not use diagonals , but a broadcasting trick with elementwise multiplication : "
        ]
      },
      {
        "title": "48082967",
        "h": "batch size",
        "t": "broadcasting",
        "r": "S2",
        "h_prob": "0.9",
        "t_prob": "0.95",
        "r_prob": "0.68",
        "prob": "0.5814",
        "sentences": [
          "this is an invalid multiplication , since the dimensions don ' t agree and broadcasting can ' t be applied."
        ]
      }
    ]
  },
  "tf.multinomial": {},
  "dot": {},
  "tf.math": {},
  "tf.less": {},
  "tf.add_n": {},
  "accuracy": {},
  "tf.estim": {},
  "tf.image.grayscale_to_rgb": {},
  "round": {},
  "tf.nn": {},
  "tf jav": {},
  "tf.contrib.losses.add_loss": {},
  "tf.compat.v1": {},
  "tf.logging": {},
  "tf.contrib.seq2seq.helper": {},
  "tf.distribute.mirroredstrategy": {},
  "tf.contrib.learn.preprocessing": {},
  "bilinear": {},
  "tf.split_": {},
  "tf.saver": {},
  "tf.deserialize_many_sparse": {},
  "tf.image.resize_with_crop_or_pad": {},
  "fully connected neural network": {},
  "conv2d": {},
  "tf.scal": {},
  "max function": {},
  "tf.gradienttape": {},
  "tf.gra": {},
  "stream": {},
  "tf.layer.dense": {},
  "tf.nn.drouput": {},
  "gradient": {},
  "logits": {},
  "logits_": {},
  "tf.contrib.training.batch_sequences_with_states": {},
  "tf.zeros_initializer": {},
  "tf.model_variables": {},
  "tf.array": {},
  "loop": {},
  "tf.manip.roll": {},
  "recurrent": {},
  "returns": {},
  "tf.keras.backend.set_session": {},
  "tf.rank": {},
  "tf.decode_base64": {},
  "tf.keras.callbacks.earlystopping": {
    "l2 regularization": [
      {
        "title": "59246895",
        "h": "tf.keras.callbacks.earlystopping",
        "t": "l2 regularization",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.44",
        "r_prob": "0.96",
        "prob": "0.287232",
        "sentences": [
          "use early stopping.",
          "code is shown below ",
          "callback = tf.keras.callbacks.earlystopping ( monitor =' val_loss ', patience = 15 ) ",
          "code for regularization is shown below ( you can try l1 regularization or l1_l2 regularization as well ). "
        ]
      },
      {
        "title": "59172613",
        "h": "tf.keras.callbacks.earlystopping",
        "t": "l2 regularization",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.46",
        "r_prob": "0.72",
        "prob": "0.225216",
        "sentences": [
          "use early stopping.",
          "code is shown below ",
          "callback = tf.keras.callbacks.earlystopping ( monitor =' val_loss ', patience = 15 ) ",
          "code for regularization is shown below ( you can try l1 regularization or l1_l2 regularization as well ): "
        ]
      },
      {
        "title": "59171581",
        "h": "tf.keras.callbacks.earlystopping",
        "t": "l2 regularization",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.46",
        "r_prob": "0.72",
        "prob": "0.225216",
        "sentences": [
          "use early stopping.",
          "code is shown below ",
          "callback = tf.keras.callbacks.earlystopping ( monitor =' val_loss ', patience = 15 ) ",
          "code for regularization is shown below ( you can try l1 regularization or l1_l2 regularization as well ): "
        ]
      }
    ]
  },
  "cross entropy loss": {},
  "tf.contrib.estimator.clip_gradients_by_norm": {},
  "tf ' s estimator": {},
  "tf.learn.metr": {},
  "tf.foldr": {},
  "tf.image": {},
  "tf.name_": {},
  "op": {},
  "tf.version": {},
  "tf.compat.v1.interactivesession": {},
  "tf.operation": {
    "tf.tensor": [
      {
        "title": "50802716",
        "h": "tf.tensor",
        "t": "tf.operation",
        "r": "S1",
        "h_prob": "0.94",
        "t_prob": "0.82",
        "r_prob": "0.96",
        "prob": "0.739968",
        "sentences": [
          "this import statement imports the tensorflow.python.framework.ops module , which ( at present ) includes the implementations of classes like tf.graph , tf.tensor , and tf.operation."
        ]
      },
      {
        "title": "37955299",
        "h": "tf.tensor",
        "t": "tf.operation",
        "r": "S1",
        "h_prob": "0.87",
        "t_prob": "0.76",
        "r_prob": "1.0",
        "prob": "0.6612",
        "sentences": [
          "tensorflow primarily expects tf.tensor objects as the keys in the feed dictionary.",
          "\" data_val \" does not work because it is the name of a tf.operation ( viz.",
          "x.op ) and not the name of a tf.tensor , which is the output of a tf.operation."
        ]
      },
      {
        "title": "42493778",
        "h": "tf.operation",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.75",
        "t_prob": "0.78",
        "r_prob": "0.99",
        "prob": "0.5791499999999999",
        "sentences": [
          "in your sample code , c is a tf.tensor while c_operation is a tf.operation.",
          "a tf.operation represents a computation that produces 0 or more tf.tensors.",
          "calling run on a tf.operation executes all operations in the graph required to produce inputs for this operation , but doesn ' t return anything ( documentation ). ",
          "calling eval on a tf.tensor executes the operation that produces it and returns its value ( documentation ). ",
          "in general , if you ' re interested in the value , you ' d want to call eval on the tf.tensor."
        ]
      },
      {
        "title": "34399966",
        "h": "tf.tensor",
        "t": "tf.operation",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.95",
        "r_prob": "0.66",
        "prob": "0.55176",
        "sentences": [
          "if you want to \" rename \" an op , there is no way to do that directly , because a tf.operation ( or tf.tensor ) is immutable once it has been created."
        ]
      },
      {
        "title": "44903756",
        "h": "tf.operation",
        "t": "tf.tensor",
        "r": "S1",
        "h_prob": "0.76",
        "t_prob": "0.85",
        "r_prob": "0.83",
        "prob": "0.53618",
        "sentences": [
          "the tensorflow graph is an object which contains your various tf.tensor and tf.operation.",
          "work with the default graph ,. ",
          "create a fresh new graph ,. "
        ]
      }
    ],
    "tf.variable": [
      {
        "title": "44903756",
        "h": "tf.variable",
        "t": "tf.operation",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.76",
        "r_prob": "1.0",
        "prob": "0.7296",
        "sentences": [
          "the tensorflow graph is an object which contains your various tf.tensor and tf.operation.",
          "when you create these tensors ( e.g.",
          "using tf.variable or tf.constant ) or operations ( e.g.",
          "work with the default graph ,. ",
          "create a fresh new graph ,. "
        ]
      },
      {
        "title": "54977745",
        "h": "tf.operation",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.81",
        "r_prob": "0.98",
        "prob": "0.579474",
        "sentences": [
          "tf.operation represents a graph node and performs computation on tensors.",
          "tf.constant returns a special kind of tf.operation which takes 0 tensors as input and produces 0 tensors as output since it performs no computation.",
          "while tf.variable is in fact a nested operation ( or subgraph ) consists of 3 nodes.",
          "maybe this node is just like a pointer pointing to the corresponding device memory with its value , waiting for other callable nodes to find it .. "
        ]
      },
      {
        "title": "42861919",
        "h": "tf.operation",
        "t": "tf.variable",
        "r": "S1",
        "h_prob": "0.67",
        "t_prob": "0.83",
        "r_prob": "0.91",
        "prob": "0.506051",
        "sentences": [
          "the documentation for tf.variable.op is not particularly clear , but it does refer to the crucial tf.operation used in the implementation of a tf.variable : any op that depends on a tf.variable will be on a path from that operation."
        ]
      },
      {
        "title": "53906363",
        "h": "tf.variable",
        "t": "tf.operation",
        "r": "S1",
        "h_prob": "0.58",
        "t_prob": "0.68",
        "r_prob": "0.99",
        "prob": "0.390456",
        "sentences": [
          "a session may own resources , such as tf.variable and it is important to release these resources when they are no longer required.",
          "the fetches argument may be a single graph element , or an arbitrarily nested list , tuple , namedtuple , dict , or ordereddict containing graph elements at its leaves.",
          "a graph element can be one of the following types : ",
          "an tf.operation.",
          "the corresponding fetched value will be none .. "
        ]
      }
    ],
    "tf.graph": [
      {
        "title": "50802716",
        "h": "tf.operation",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.97",
        "r_prob": "0.89",
        "prob": "0.7079059999999999",
        "sentences": [
          "this import statement imports the tensorflow.python.framework.ops module , which ( at present ) includes the implementations of classes like tf.graph , tf.tensor , and tf.operation."
        ]
      },
      {
        "title": "54370971",
        "h": "tf.graph",
        "t": "tf.operation",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.68",
        "r_prob": "0.97",
        "prob": "0.646408",
        "sentences": [
          "for getting tf.operation by names , you can use tf.graph.get_operation_by_name "
        ]
      },
      {
        "title": "42600405",
        "h": "tf.operation",
        "t": "tf.graph",
        "r": "S1",
        "h_prob": "0.68",
        "t_prob": "0.93",
        "r_prob": "0.87",
        "prob": "0.550188",
        "sentences": [
          "the tf.graph.get_operation_by_name () method always returns a tf.operation object.",
          "when you pass a tf.operation object to tf.session.run (), tensorflow will execute that operation ( and everything on which it depends ) and discard its outputs ( if any ). ",
          "get a tf.tensor from the graph by calling tf.graph.get_tensor_by_name (), and appending \":< output index >\" to the operation ' s name : "
        ]
      }
    ]
  },
  "tf.train.list_variables": {},
  "tf.variables.": {},
  "dtype": {},
  "tf.applications": {},
  "tf.compat.v1.nn.rnn_cell.multirnncell": {},
  "tf.nn.rnn_cell.basicrnncell": {},
  "loop_var": {},
  "tf.not_equal": {},
  "tf.contrib.framework.local_variable": {
    "tf.graphkeys": [
      {
        "title": "49327230",
        "h": "tf.contrib.framework.local_variable",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.9",
        "t_prob": "0.95",
        "r_prob": "1.0",
        "prob": "0.855",
        "sentences": [
          "it appears that the moving mean and moving variance are stored within tf.graphkeys.global_variables and it looks like the reason nothing showed up in model_variables is because you need to use tf.contrib.framework.local_variable "
        ]
      },
      {
        "title": "43602899",
        "h": "tf.contrib.framework.local_variable",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.97",
        "r_prob": "0.89",
        "prob": "0.828768",
        "sentences": [
          "usually used for temporarily variables , like counters.",
          "note : use tf.contrib.framework.local_variable to add to this collection.",
          "so to create a local variable you need to do something like this."
        ]
      },
      {
        "title": "43602962",
        "h": "tf.contrib.framework.local_variable",
        "t": "tf.graphkeys",
        "r": "S1",
        "h_prob": "0.96",
        "t_prob": "0.97",
        "r_prob": "0.81",
        "prob": "0.7542719999999999",
        "sentences": [
          "a local variable in tf is any variable which was created with collections =[ tf.graphkeys.local_variables ]. ",
          "usually used for temporarily variables , like counters.",
          "note : use tf.contrib.framework.local_variable to add to this collection.",
          "a global variable is mostly every other variable initialized by you."
        ]
      }
    ]
  },
  "tf.is_nan": {},
  "tf.map": {},
  "tf.tensorshape": {},
  "tf.dimension": {},
  "tf.linalg.matmul": {},
  "batch normalisation": {},
  "tf.dtypes.cast": {},
  "variance": {},
  "keras.": {},
  "gan": {
    "discriminator": [
      {
        "title": "53593130",
        "h": "discriminator",
        "t": "gan",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.76",
        "r_prob": "0.92",
        "prob": "0.6432640000000001",
        "sentences": [
          "the same can be done for discriminator and gan."
        ]
      },
      {
        "title": "60300581",
        "h": "discriminator",
        "t": "gan",
        "r": "S1",
        "h_prob": "0.88",
        "t_prob": "0.72",
        "r_prob": "0.99",
        "prob": "0.6272639999999999",
        "sentences": [
          "in a gan nash equilibrium is reached when you converge i.e.",
          "when the loss of the discriminator does not get reduced at the expense of the generator and v.v.",
          "another way would be to implement some sort of early stopping ( or if you use tensorboard track the losses of the two networks to visualize convergence ). ",
          "this article seems to have an extensive explanation of the nash equilibrium for gans."
        ]
      },
      {
        "title": "42470757",
        "h": "discriminator",
        "t": "gan",
        "r": "S1",
        "h_prob": "0.82",
        "t_prob": "0.72",
        "r_prob": "1.0",
        "prob": "0.5903999999999999",
        "sentences": [
          "first , i would advise you to switch to the functional api models.",
          "the way i have found around that problem , is to define the layers and use them for both the discriminator and the gan models."
        ]
      },
      {
        "title": "60590207",
        "h": "discriminator",
        "t": "gan",
        "r": "S1",
        "h_prob": "0.89",
        "t_prob": "0.6",
        "r_prob": "0.8",
        "prob": "0.4272",
        "sentences": [
          "discriminator is compiled first and then comes the gan part ( self.combined ). "
        ]
      },
      {
        "title": "51070467",
        "h": "discriminator",
        "t": "gan",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.42",
        "r_prob": "0.68",
        "prob": "0.22848000000000004",
        "sentences": [
          "when you do gan there are few things that change from normal neural network training.",
          "the artificial images from the generator network change at each update of the weights in the network .. ",
          "it is pointless to train the discriminator on a lot of data , if you then update the generator."
        ]
      }
    ]
  },
  "tf.instead": {},
  "tf.keras.backend": {},
  "statistics": {},
  "class": {},
  "tf.keras.optimizers.adam": {},
  "tf.custom_gradient": {},
  "tf._train.shuffle_batch": {},
  "tf.compat.v1.keras.backend.get_session": {},
  "tf.keras.layers.gru": {},
  "tf.compat.v1.numpy_function": {},
  "broadcasts": {},
  "tf.summary.audio": {},
  "tf.nn.rnn cre": {},
  "tf.contrib.data.csvdataset": {},
  "tf.compat.v1.get_default_graph": {},
  "tf.saved_model.loader.load": {},
  "tf.colocate_with": {},
  "sub": {},
  "tf.contrib.framework.get_variables_by_suffix": {},
  "tf.greater": {},
  "vram": {},
  "tf.sparse.sparsetensors": {},
  "tf.estimator.export.servinginputreceiver": {},
  "tf.neg": {},
  "tf.sub": {},
  "tf.while": {},
  "tf.debugging.set_log_device_placement": {},
  "tf.contrib.graph_editor.swap_inputs": {},
  "random forests": {},
  "tf.keras.backend.conv2d_transpose": {},
  "tf.estimator.add_metrics": {},
  "tf.nn.ctc_loss": {},
  "graph": {},
  "border": {},
  "tf.contrib.seq2seq.dynamic_decode": {},
  "dataframe": {},
  "weight": {},
  "tf.nn.rnn_cell.basiclstmcell": {},
  "tf.metrics.true_positives": {},
  "scalar": {},
  "tf.data.experimental.copy_to_device": {},
  "tf.pow": {},
  "ger": {},
  "view": {},
  "tf.comm": {},
  "convolutional network": {},
  "dense layers": {},
  "class problems": {},
  "tf vers": {},
  "dropout": {},
  "logistic regression": {},
  "tf.estimator.multiclasshead": {},
  "tf.keras.activations.relu": {},
  "tf.negative": {},
  "tf.saved_model": {},
  "tf.train.saverdef": {},
  "identity": {},
  "tf.glorot_uniform_initializer": {},
  "tf.keras.layers.conv2dtranspose": {},
  "tf.confusion_matrix": {},
  "tf.nn.sparse_softmax_": {},
  "nois": {},
  "tanh": {},
  "learning process": {},
  "tf.train.queuerunner": {},
  "hidden layer": {},
  "classes": {},
  "tf.example": {},
  "checkpoint": {},
  "fully connected layer": {},
  "deep learning": {
    "batch size": [
      {
        "title": "50784097",
        "h": "batch size",
        "t": "deep learning",
        "r": "S2",
        "h_prob": "0.83",
        "t_prob": "0.92",
        "r_prob": "0.99",
        "prob": "0.755964",
        "sentences": [
          "batch size , in general , represents the size of the mini - batches constructed from the experimental dataset."
        ]
      },
      {
        "title": "47240662",
        "h": "batch size",
        "t": "deep learning",
        "r": "S2",
        "h_prob": "0.81",
        "t_prob": "0.66",
        "r_prob": "0.99",
        "prob": "0.5292540000000001",
        "sentences": [
          "i have a deep learning fundamentals youtube playlist that you may find helpful.",
          "additionally , this deep learning with keras playlist may also be beneficial if you ' re wanting to focus more on coding after getting the basic concepts down."
        ]
      },
      {
        "title": "43429424",
        "h": "batch size",
        "t": "deep learning",
        "r": "S2",
        "h_prob": "0.91",
        "t_prob": "0.9",
        "r_prob": "0.6",
        "prob": "0.4914",
        "sentences": [
          "2 ) increase your batch size , at least as same as classes number , "
        ]
      }
    ],
    "machine learning": [
      {
        "title": "45979913",
        "h": "machine learning",
        "t": "deep learning",
        "r": "S1",
        "h_prob": "0.8",
        "t_prob": "0.82",
        "r_prob": "0.82",
        "prob": "0.53792",
        "sentences": [
          "image encoding using a cnn model ( openface for instance ).",
          "image search using a voisinage algorithm ( knn , lshforest , etc ).",
          "here ' s a blog article that draws a nice walk through the steps required to do face - based user identification : ",
          "machine learning is fun part 4 modern face recognition with deep learning."
        ]
      },
      {
        "title": "56208907",
        "h": "machine learning",
        "t": "deep learning",
        "r": "S1",
        "h_prob": "0.92",
        "t_prob": "0.9",
        "r_prob": "0.61",
        "prob": "0.5050800000000001",
        "sentences": [
          "gal , yarin , and zoubin ghahramani.",
          "\" dropout as a bayesian approximation : representing model uncertainty in deep learning .\" ",
          "international conference on machine learning.",
          "2016."
        ]
      },
      {
        "title": "61549702",
        "h": "machine learning",
        "t": "deep learning",
        "r": "S1",
        "h_prob": "0.66",
        "t_prob": "0.61",
        "r_prob": "0.95",
        "prob": "0.38247",
        "sentences": [
          "machine learning - based approach : https :// www.researchgate.net / publication / 266672947_estimating_smile_intensity_a_better_way ",
          "deep learning ( cnn ): https :// arxiv.org / pdf / 1602.00172.pdf ",
          "if you have the label for smiling intensity , you can just solve it as a regression problem , the cnn will have one output , will try to regress the smile intensity ( the normalized smile intensity with sigmoid in this case ). "
        ]
      },
      {
        "title": "38875433",
        "h": "machine learning",
        "t": "deep learning",
        "r": "S1",
        "h_prob": "0.93",
        "t_prob": "0.7",
        "r_prob": "0.55",
        "prob": "0.35805000000000003",
        "sentences": [
          "it ' s originally called skflow ( scikit flow ). ",
          "it is for both deep learning as well as general machine learning."
        ]
      },
      {
        "title": "59779253",
        "h": "machine learning",
        "t": "deep learning",
        "r": "S1",
        "h_prob": "0.73",
        "t_prob": "0.89",
        "r_prob": "0.55",
        "prob": "0.357335",
        "sentences": [
          ". ",
          "your dataset size is obviously too small for deep learning.",
          "adding more training data or try traditional machine learning like svm , lr .. "
        ]
      }
    ]
  },
  "fully connected layers": {},
  "tf.cumsum": {},
  "tf.summary.create_file_writer": {},
  "tf.make_template": {},
  "ops": {},
  "any": {},
  "tf.expanddims": {},
  "adam optimizer": {},
  "tf.sparsetens": {},
  "mu": {},
  "tf.contrib.layers.scale_gradient": {},
  "backward": {},
  "tf.nn.max_pool_with_argmax": {},
  "max pooling": {},
  "fn": {},
  "tf.eye": {},
  "logitsop": {},
  "tf.ass": {},
  "tf.keras.metrics.accuracy": {},
  "tf.nn.rnn_cell.grucell": {},
  "tf.contrib.rnn": {},
  "tf.nn.rnn_cell": {},
  "tf.nn.rnn_cell.rnncell": {},
  "future": {},
  "tf.math.equal": {},
  "tf.math.logical_not": {},
  "loss": {},
  "tf.subtract": {},
  "tf.keras.metrics.sparsecategoricalaccuracy": {},
  "accuracy function": {},
  "reinforcement learning": {},
  "statistical efficiency": {},
  "tf.train.exponential_decay": {},
  "sample": {},
  "tf.app.run": {},
  "tf.compat.v1.app.run": {},
  "tf.data.experimental.parallel_interleave": {},
  "tf.tf while loop": {},
  "tf.newa": {},
  "computational ability": {},
  "tf.offici": {},
  "data": {},
  "tf.compat.v1.flags": {},
  "tf.app": {},
  "tf.log_sigmoid": {},
  "logit": {},
  "k.clear_": {},
  "tf.regex_replace": {},
  "tf.train.summarysaverhook": {},
  "tf.contrib.layers.convolution2d": {},
  "tf.lookup.staticvocabularytable": {},
  "tf.lookup.textfileinitializer": {},
  "tf.slim": {},
  "tf.losses.absolut_difference": {},
  "validation loss": {},
  "tf.layers.max_pooling1d": {},
  "tf.runoptions": {},
  "tf.feature_column.categorical_column_with_vocabulary_list": {},
  "tf.keras.layers.densefeatures": {},
  "complex": {},
  "tf.div": {},
  "tensorflow": {},
  "tf.contrib.layers.batch_norm": {},
  "feed forward network": {},
  "ground truth": {},
  "tf.train.load_variable": {},
  "tf.quantization.quantize": {},
  "tf.qint": {},
  "tf.saved_model.save": {},
  "tf.random.normal": {},
  "batch gradient de": {},
  "tf.typespec": {},
  "gamma": {},
  "tf.diag": {},
  "optimizer": {},
  "fusion": {},
  "output layer": {},
  "tf.train.": {},
  "tf.io.fixedlenfeature": {},
  "sgd": {},
  "derivative": {},
  "tf.contrib re": {},
  "tf.lookup.statichashtable": {},
  "tf.train.get_global_step": {},
  "tf.logical_or": {},
  "garden": {},
  "vector": {},
  "k.clear_session": {},
  "actions": {},
  "tf.batch": {},
  "tf.train.* optimizer": {},
  "tf.keras.regularizers.l2": {},
  "tf.matching_files": {},
  "tf.train.checkpoint": {},
  "tf.train": {},
  "gradient computation": {},
  "relu activation function": {},
  "then": {},
  "gradient updates": {},
  "vx": {
    "instructions": [
      {
        "title": "56410843",
        "h": "vx",
        "t": "instructions",
        "r": "S1",
        "h_prob": "0.57",
        "t_prob": "0.51",
        "r_prob": "0.99",
        "prob": "0.28779299999999997",
        "sentences": [
          "probably , your cpu does not have the instructions for avx ( needed for all tensorflow binaries since 1.6 ). ",
          "tensorflow - windows - wheel "
        ]
      },
      {
        "title": "49169125",
        "h": "vx",
        "t": "instructions",
        "r": "S1",
        "h_prob": "0.48",
        "t_prob": "0.5",
        "r_prob": "0.87",
        "prob": "0.20879999999999999",
        "sentences": [
          "the precompiled binaries of versions > 1.5 use avx instructions that are not supported by older cpus "
        ]
      },
      {
        "title": "51926047",
        "h": "vx",
        "t": "instructions",
        "r": "S1",
        "h_prob": "0.55",
        "t_prob": "0.55",
        "r_prob": "0.61",
        "prob": "0.18452500000000002",
        "sentences": [
          "it is just an information message that says that your version of tensorflow doesn ' t use some instructions ( avx ) that are supported by the cpu.",
          "it means that you can get better performance if you rebuild tensorflow with enabled support for those instructions."
        ]
      }
    ],
    "sets": [
      {
        "title": "58207944",
        "h": "vx",
        "t": "sets",
        "r": "S2",
        "h_prob": "0.5",
        "t_prob": "0.29",
        "r_prob": "0.65",
        "prob": "0.09425",
        "sentences": [
          "tensorflow release binaries version 1.6 and higher are prebuilt with avx instruction sets.",
          "apparently , your cpu model does not support avx instruction sets."
        ]
      },
      {
        "title": "57341291",
        "h": "vx",
        "t": "sets",
        "r": "S2",
        "h_prob": "0.48",
        "t_prob": "0.29",
        "r_prob": "0.65",
        "prob": "0.09047999999999999",
        "sentences": [
          "tensorflow release binaries version 1.6 and higher are prebuilt with avx instruction sets.",
          "apparently , your cpu model does not support avx instruction sets."
        ]
      },
      {
        "title": "57516724",
        "h": "vx",
        "t": "sets",
        "r": "S2",
        "h_prob": "0.48",
        "t_prob": "0.29",
        "r_prob": "0.64",
        "prob": "0.089088",
        "sentences": [
          "tensorflow release binaries version 1.6 and higher are prebuilt with avx instruction sets.",
          "apparently , your cpu model does not support avx instruction sets."
        ]
      }
    ]
  },
  "instructions": {
    "vx": [
      {
        "title": "56410843",
        "h": "vx",
        "t": "instructions",
        "r": "S1",
        "h_prob": "0.57",
        "t_prob": "0.51",
        "r_prob": "0.99",
        "prob": "0.28779299999999997",
        "sentences": [
          "probably , your cpu does not have the instructions for avx ( needed for all tensorflow binaries since 1.6 ). ",
          "tensorflow - windows - wheel "
        ]
      },
      {
        "title": "49169125",
        "h": "vx",
        "t": "instructions",
        "r": "S1",
        "h_prob": "0.48",
        "t_prob": "0.5",
        "r_prob": "0.87",
        "prob": "0.20879999999999999",
        "sentences": [
          "the precompiled binaries of versions > 1.5 use avx instructions that are not supported by older cpus "
        ]
      },
      {
        "title": "51926047",
        "h": "vx",
        "t": "instructions",
        "r": "S1",
        "h_prob": "0.55",
        "t_prob": "0.55",
        "r_prob": "0.61",
        "prob": "0.18452500000000002",
        "sentences": [
          "it is just an information message that says that your version of tensorflow doesn ' t use some instructions ( avx ) that are supported by the cpu.",
          "it means that you can get better performance if you rebuild tensorflow with enabled support for those instructions."
        ]
      }
    ]
  },
  "epochs times": {},
  "squared_error": {},
  "tf.broadcast_to": {},
  "name": {},
  "tf.train.add_queue_runner": {},
  "tf.math.multiply": {},
  "tf session": {},
  "tf.train.feature": {},
  "tf.train.byteslist": {},
  "tf.embedd": {},
  "identity matrix": {},
  "tf.compat.v1.py_func": {},
  "pooling on": {},
  "tf.unstack": {},
  "tf.fixedlengthrecordreader": {},
  "tf.data.filter": {},
  "tf.compat": {},
  "tf.random_normal_initiali": {},
  "script": {
    "tf.session": [
      {
        "title": "38853802",
        "h": "script",
        "t": "tf.session",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.92",
        "r_prob": "0.82",
        "prob": "0.648784",
        "sentences": [
          "assuming sess is your tf.session () and \" output \" is the name of your prediction node , the following code will serialize your minimal graph both into textual and binary protobuf."
        ]
      },
      {
        "title": "58603627",
        "h": "tf.session",
        "t": "script",
        "r": "S1",
        "h_prob": "0.98",
        "t_prob": "0.62",
        "r_prob": "0.75",
        "prob": "0.4557",
        "sentences": [
          "using tf.enable_eager_execution () or evaluating a tensor within a tf.session () context ",
          "the following script doesn ' t throw an error when using either pickle or dill : "
        ]
      },
      {
        "title": "51653363",
        "h": "tf.session",
        "t": "script",
        "r": "S1",
        "h_prob": "0.86",
        "t_prob": "0.79",
        "r_prob": "0.51",
        "prob": "0.346494",
        "sentences": [
          "device memory is reserved when you instantiate a tf.session.",
          "although it is common in \" raw \" tensorflow to create the graph first then instantiate a session , it is nonetheless allowed to proceed differently , and it is actually common in the keras world where one would often start a script with "
        ]
      }
    ]
  },
  "tf.errors.notfounderror": {},
  "tf.keras.optimizers.sgd": {},
  "tf.global_variable_initializers": {},
  "abs": {},
  "pad": {},
  "random.": {},
  "normal array": {},
  "partial derivatives": {},
  "mul": {},
  "tf.histogram_fixed_width": {},
  "optimization algorithm": {},
  "convolutional networks": {},
  "tf.contrib.data.unbatch": {},
  "deep reinforcement learning": {},
  "tf.registergradient": {},
  "tf.squ": {},
  "tf.summary namespac": {},
  "dataset.from_": {},
  "version": {},
  "tf.train.sessionrunhook": {},
  "short": {},
  "vx inst": {},
  "minority class": {},
  "tf.keras.preprocessing.image.load_img": {},
  "output": {},
  "tf.p": {},
  "convergions": {},
  "tf.compat.v1.train.adamoptimizer": {},
  "tf.optimizers.adam": {},
  "tf.keras.layer": {},
  "tf.initializers.glorot_uniform": {},
  "instruct": {},
  "supervised learning": {},
  "fully - connected": {},
  "batch_norm": {},
  "exploding gradients": {},
  "quantization": {},
  "operations": {},
  "tf.saved_model.load": {},
  "tf.local_variables": {},
  "tf.train.featurelists": {},
  "conv3d": {},
  "word ser": {},
  "sort": {},
  "tf.calls": {},
  "tf.gradient": {},
  "tf.feature_column.embedding_column": {},
  "tf.keras.preprocessing.image.imagedatagenerator": {},
  "tf.variable_initializers": {},
  "tf.summary.value": {},
  "tf.reverse": {},
  "tf.scalar_mul": {},
  "tf.data programs": {},
  "ignore": {},
  "tf.setdiff1d": {},
  "tf.dynamic_stitch": {},
  "tf.sparse_reduce_sum": {},
  "objective": {},
  "argsort": {},
  "maximum likelihood": {},
  "tation": {},
  "normalise": {},
  "new": {},
  "tf.contrib.rnn.static_rnn": {},
  "tf.contrib.*": {},
  "tf.global_": {},
  "tf.summary nam": {},
  "state": {},
  "functional": {},
  "tf.on": {},
  "tf.j": {},
  "tf.summary.filewriter": {},
  "tf.sqrt": {},
  "tf.train.metagraphde": {},
  "singular value decomposition": {},
  "tf.svd": {},
  "recurrent neural network": {},
  "convolutions": {},
  "tf.io": {},
  "unsupervised learning": {},
  "gradients": {},
  "tf.linalg.norm": {},
  "tf.norm": {},
  "parameter servi": {},
  "tf.unsorted_segment_max": {},
  "clipping": {},
  "augmentation": {},
  "tf.train.polynomial_decay": {},
  "conv_": {},
  "tf.place": {},
  "tf.* namespac": {},
  "tf.enqu": {},
  "kernels": {},
  "tf.some_function_that_create": {},
  "results": {},
  "kernel": {},
  "tf.train.adagradoptimizer": {},
  "tf.python": {},
  "transform": {},
  "grid search": {},
  "statistical learning theory": {},
  "deep neural networks": {},
  "tf.layers.conv * functions": {},
  "probability": {},
  "keras": {},
  "tf.train.init_from_checkpoint": {},
  "hidden layers": {},
  "tf.train.piecewise_constant": {},
  "tf.keras.layers.conv": {},
  "tf.contrib.data.map_and_batch": {},
  "numpy": {},
  "tf.su": {},
  "tf.ti": {},
  "downsamp": {},
  "tf.keras.layers.lstmcell": {},
  "tf.keras.layers.": {},
  "string": {},
  "tf.string_input_produ": {},
  "tf.logical_not": {},
  "complete": {},
  "seed": {},
  "var": {},
  "argmin": {},
  "tf.data works": {},
  "tf.feature_column.categorical_column_with_hash_bucket": {},
  "independent": {},
  "input_dim": {},
  "word embeddings": {},
  "tf.signal.rfft2d": {},
  "dropout layer": {},
  "tf.train.features": {},
  "number of parameters": {},
  "logar": {},
  "logre": {},
  "convex function": {},
  "conve": {},
  "logical operator": {},
  "tf.contrib.losses.get_total_loss": {},
  "full": {},
  "labelbin": {},
  "tf.keras.layers.relu": {},
  "tf.nn.rnn_cell.lstmcell": {},
  "shorter": {},
  "computational graph": {},
  "tf.keras.utils.multi_gpu_model": {},
  "multilayer perceptron": {},
  "fp": {},
  "output layers": {},
  "recall": {},
  "less": {},
  "instance": {},
  "double": {},
  "tf.sparsetensorvalue": {},
  "vpn": {},
  "artificial neural networks": {},
  "convolution layers": {},
  "squeeze": {},
  "tf.math.log": {},
  "linear": {},
  "feature map": {},
  "ger execution": {},
  "mixing": {},
  "normal": {},
  "layers arn": {},
  "tf.contrib.saved_model.save_keras_model": {},
  "tf.math.floordiv": {},
  "tf.di": {},
  "superviso": {},
  "learning": {},
  "tf.qui": {},
  "trace": {},
  "tf.math.mod": {},
  "random.seed": {},
  "categorical": {},
  "multi - class": {},
  "word ids": {},
  "_init_from_arg": {},
  "tf.expand": {},
  "tf.dynnam": {},
  "datasets function": {},
  "logisti": {},
  "early stopping": {},
  "tf.data guide": {},
  "tf.compat.v1.layers.dense": {},
  "linear regression": {},
  "scaling high": {},
  "graphs": {},
  "tf.saved_model.builder.savedmodelbuilder": {},
  "tf.zero_initializer": {},
  "batch process": {},
  "activation functions": {},
  "tf probably": {},
  "tf.data nodule": {},
  "tf.random.categorical": {},
  "threshold": {},
  "tf.python.": {},
  "squar": {
    "loss function": [
      {
        "title": "51576032",
        "h": "squar",
        "t": "loss function",
        "r": "S1",
        "h_prob": "0.57",
        "t_prob": "0.91",
        "r_prob": "0.8",
        "prob": "0.41496",
        "sentences": [
          "you ' ll want to use ' mean_squared_error ' as a loss function , and track mse as a metric instead of accuracy."
        ]
      },
      {
        "title": "51094115",
        "h": "squar",
        "t": "loss function",
        "r": "S1",
        "h_prob": "0.55",
        "t_prob": "0.67",
        "r_prob": "0.84",
        "prob": "0.30954000000000004",
        "sentences": [
          "the ' mean_squared_error ' loss function is probably expecting to receive a ( batch_sz x n_labels ) matrix of labels , but you are passing a ( batch_sz x 1 x n_labels ) matrix of labels , specifically with labels.shape =( 32 , 1 , 4 ). "
        ]
      },
      {
        "title": "47061112",
        "h": "squar",
        "t": "loss function",
        "r": "S1",
        "h_prob": "0.48",
        "t_prob": "0.79",
        "r_prob": "0.71",
        "prob": "0.26923199999999997",
        "sentences": [
          "you have used mean_squared_error ( mse ) loss function.",
          "formula for mse ",
          "mse must be low for a good model.",
          "which is pretty good.",
          "in keras there is another loss function named mean_absolute_percentage_error.",
          "you can compile the model with mean_absolute_percentage_error as loss function if you want to know the percentage error of the model with train and test."
        ]
      }
    ]
  },
  "squared_": {},
  "tf.layers interfa": {},
  "invalidargum": {},
  "add": {},
  "id": {},
  "close": {},
  "algorithms": {},
  "flatten": {},
  "real": {},
  "squared": {},
  "tf.get_global_variables": {},
  "tf.get_trainable_variables": {},
  "logarithm": {},
  "timesteps": {},
  "derivatives": {},
  "tf.layers.max_": {},
  "sign": {},
  "scaling": {},
  "tf.sa": {},
  "tf.gpuoptions": {},
  "tf.clu": {},
  "reward": {},
  "keras_model": {},
  "random cropping": {},
  "classification": {},
  "softmax": {},
  "tf.math.argmax": {},
  "batch - norm": {},
  "vt": {},
  "supervi": {},
  "lstmcell": {},
  "selu": {},
  "binary crossentropy": {},
  "label": {},
  "logical": {},
  "tf.trans": {},
  "stack": {},
  "forward": {},
  "tf.get_": {},
  "tf.estimator.trainspec": {},
  "reality": {},
  "tf.keras.metrics": {},
  "tf.get_collection_ref": {},
  "l2 loss": {},
  "logistic": {},
  "tf.relu": {},
  "modules": {},
  "tf best pract": {},
  "total derivative": {},
  "tf.floatx": {},
  "tf.image.resize_bicubic": {},
  "op registration": {},
  "greater": {},
  "images": {},
  "code": {},
  "tf.train.limit_epochs": {},
  "vstack": {},
  "beta": {},
  "kl": {},
  "tf te": {},
  "convolutional": {},
  "tf.argsort": {},
  "subset": {},
  "exploding gradient": {},
  "deep - reinforcement": {},
  "logical rules": {},
  "structure": {},
  "lstm cell": {},
  "tf.sparse.sparsetensor": {},
  "eval": {},
  "tf.initializer": {},
  "add runoptions": {},
  "log_devi": {},
  "range": {},
  "fragment": {},
  "tf.mod": {},
  "showing": {},
  "tf.train.monitoredsession": {},
  "timestep": {},
  "le": {},
  "le_devi": {},
  "ional matrices": {},
  "idth": {},
  "size": {},
  "statistical cur": {},
  "striding what": {},
  "striding": {},
  "action sp": {},
  "tf.space_to_depth": {},
  "sort_indices": {},
  "tf.sparse.reorder": {},
  "machine translation": {},
  "tf.un": {},
  "tf.summ": {},
  "services": {},
  "mean squared error": {},
  "broadcast": {},
  "tf beam": {},
  "representations": {},
  "emit_structure": {},
  "pandas": {},
  "tf.enable_resource_variables": {},
  "enabled": {},
  "parallel lines": {},
  "convolution": {},
  "multinomial": {},
  "soul": {},
  "tf.keras.optimizers": {},
  "vp": {},
  "epochs": {},
  "class aga": {},
  "tf.data.experimental.get_structure": {},
  "svd": {},
  "gradient w": {},
  "random rotation": {},
  "tf.feature_column.categorical_column_with_vocabulary_file": {},
  "tf v": {},
  "shorth": {},
  "gaussi": {},
  "bias b": {},
  "logdir": {},
  "shape =": {},
  "v1": {},
  "tf.v": {},
  "deep probab": {},
  "logging": {},
  "hyper": {},
  "predict": {},
  "tf.tuple": {},
  "singular values": {},
  "ml servi": {},
  "max": {},
  "bool": {},
  "logis": {},
  "lstm layers": {},
  "tf.contrib.tfprof": {},
  "quantized": {},
  "calculations": {},
  "squared loss": {},
  "gpu devi": {},
  "deep - learning framework": {},
  "hiding": {},
  "categorical variable": {},
  "continuous variables": {},
  "tf.signal": {},
  "tf.signal.stft": {},
  "tf.nn.static_bidirectional_rnn": {},
  "tf.nn.dropu": {},
  "runni": {},
  "loops": {},
  "random transform": {},
  "tf.tanh": {},
  "mlp": {},
  "logical relat": {},
  "ids": {},
  "forward propagation": {},
  "parameters": {},
  "minibatch": {},
  "artificial neural network": {},
  "tensor": {},
  "iid": {},
  "vgg16": {},
  "supervised machine learning": {},
  "tf.train.int64list": {},
  "tf.strings.to_number": {},
  "sigmoid": {},
  "keras.applications": {},
  "correlation": {},
  "computational resour": {},
  "parameter sp": {},
  "tf com": {},
  "discriminative": {},
  "dice": {},
  "kl divergence": {},
  "operation": {},
  "son": {},
  "table": {},
  "tf.example proto": {},
  "_indices": {},
  "tf.brow": {},
  "tf.no": {},
  "vmaf": {},
  "support vector machine": {},
  "fragm": {},
  "fully util": {},
  "tf.get_variable_scope": {},
  "dl": {},
  "flush": {},
  "tf.fro": {},
  "pooling1d": {},
  "examples <PAD>": {},
  "tau": {},
  "regression model": {},
  "tf.contrib.distributions.categorical": {},
  "sigmoid function": {},
  "tolist": {},
  "broadca": {},
  "gradient calculation": {},
  "tf.f": {},
  "tf.contrib.layers.embedding_column": {},
  "object recognition": {},
  "all": {},
  "tf.losses.get_regularization_losses": {},
  "stride": {},
  "noise": {},
  "vbo": {},
  "tf -": {},
  "dimension": {},
  "tf.keras.in": {},
  "parameter": {},
  "false negatives": {},
  "true negatives": {},
  "states ().": {},
  "mse does": {},
  "tf.keras.utils.sequence": {},
  "exception": {},
  "models": {},
  "comparis": {},
  "float32": {},
  "feature": {},
  "dilation": {},
  "tf.contrib.deprecated.histogram_summary": {},
  "diagonal": {},
  "help": {},
  "car": {},
  "cross_entropy_with_logits function": {},
  "tf.image.random_saturation": {},
  "tf.resha": {},
  "sor": {},
  "tf.compat.v2.clip_by_value": {},
  "vanishing gradients": {},
  "serving": {},
  "int": {},
  "tf.train.floatlist": {},
  "mini batch": {},
  "tf.saved_model.signature_def_utils.build_signature_def": {},
  "layers": {},
  "dou": {},
  "tf.distributions.categorical": {},
  "vmware": {},
  "tf.image_": {},
  "request": {},
  "tf.test.testcase": {},
  "hstack": {},
  "true_divide": {},
  "division": {},
  "take": {},
  "tf.__vers": {},
  "fully connected network": {},
  "vggnet": {},
  "weights": {},
  "tf.train.load_checkpoint": {},
  "tf.sessi": {},
  "tf.keras.layers.concatenate": {},
  "perspective": {},
  "tf.get_shape": {},
  "hogwil": {},
  "fully connected": {},
  "torch.onnx.export": {},
  "momentum optimizer": {},
  "conv1d": {},
  "tf.get_session_tensor": {},
  "triu_indices": {},
  "relu": {},
  "amoptim": {},
  "tf.keras.sa": {},
  "tf side": {},
  "linear combinations": {},
  "logical functions": {},
  "tf.less_equal": {},
  "batch dimension": {},
  "1d te": {},
  "strin": {},
  "tf.load": {},
  "get": {},
  "classification layers": {},
  "image recognition": {},
  "tf.compat.forward_compatible": {},
  "check": {},
  "update rule": {},
  "tf.greater_equal": {},
  "fragmentation": {},
  "transfer learning": {},
  "resnet50": {},
  "expressed": {},
  "kern": {},
  "tf.squared_di": {},
  "logdi": {},
  "bash": {},
  "mae": {},
  "correlation coefficient": {},
  "tf.contrib.metrics.streaming_pearson_correlation": {},
  "tf.lgamma": {},
  "gpu": {},
  "random sampler": {},
  "support": {},
  "perm": {},
  "sets": {
    "vx": [
      {
        "title": "58207944",
        "h": "vx",
        "t": "sets",
        "r": "S2",
        "h_prob": "0.5",
        "t_prob": "0.29",
        "r_prob": "0.65",
        "prob": "0.09425",
        "sentences": [
          "tensorflow release binaries version 1.6 and higher are prebuilt with avx instruction sets.",
          "apparently , your cpu model does not support avx instruction sets."
        ]
      },
      {
        "title": "57341291",
        "h": "vx",
        "t": "sets",
        "r": "S2",
        "h_prob": "0.48",
        "t_prob": "0.29",
        "r_prob": "0.65",
        "prob": "0.09047999999999999",
        "sentences": [
          "tensorflow release binaries version 1.6 and higher are prebuilt with avx instruction sets.",
          "apparently , your cpu model does not support avx instruction sets."
        ]
      },
      {
        "title": "57516724",
        "h": "vx",
        "t": "sets",
        "r": "S2",
        "h_prob": "0.48",
        "t_prob": "0.29",
        "r_prob": "0.64",
        "prob": "0.089088",
        "sentences": [
          "tensorflow release binaries version 1.6 and higher are prebuilt with avx instruction sets.",
          "apparently , your cpu model does not support avx instruction sets."
        ]
      }
    ]
  },
  "ilsvrc": {},
  "tf.ser": {},
  "update": {},
  "tf.train.loggingtensorhook": {},
  "data object": {},
  "tf.se": {},
  "tf.metrics.mean_cosine_distance": {},
  "minval": {},
  "nor": {},
  "batch sp": {},
  "cifar10": {},
  "closin": {},
  "tensor objects": {},
  "computations": {},
  "diagra": {},
  "tf.losses.sparse_categorical_crossentropy": {},
  "dorg": {},
  "jth": {},
  "tf functions": {},
  "tf.contrib.learn.read_batch_examples": {},
  "scrip": {},
  "insert": {},
  "load": {},
  "tf.scalar": {},
  "resour": {},
  "regular calculations": {},
  "convolutional bayesi": {},
  "recurrent networks": {},
  "tf.keras.models": {},
  "tf.stride_slice": {},
  "tf.math.truediv": {},
  "tf.qint32": {},
  "device": {},
  "labels": {},
  "accuracy calcul": {},
  "multiclass logistic regression": {},
  "locations": {},
  "environ": {},
  "risk": {},
  "environments": {},
  "tf.distribute.cluster_resolver": {},
  "unit norm": {},
  "abstr": {},
  "classifications": {},
  "divi": {},
  "equal": {},
  "defaul": {},
  "tf.initializers.identity": {},
  "algorithm": {},
  "minimization": {},
  "tensorboard": {},
  "image size": {},
  "frame": {},
  "scre": {},
  "tf.contrib.data.group_by_window": {},
  "tf.math.sin": {},
  "tf.tra": {},
  "minority classes": {},
  "tf.data.experimental.make_csv_dataset": {},
  "categorical data": {},
  "convergence rate": {},
  "step function": {},
  "tf.mul ().": {},
  "matrix": {},
  "tf.imag": {},
  "tf.real": {},
  "isclose": {},
  "vrride": {},
  "tf.nn_": {},
  "gradientdesc": {},
  "gpu devices": {},
  "layer": {},
  "map": {},
  "shot": {},
  "tf.switch_case": {},
  "tf.train.sessionmanager": {},
  "ge loss function": {},
  "store": {},
  "values": {},
  "convolutional neural": {},
  "exponential": {},
  "tf.shuffle_batch": {},
  "weight parameter": {},
  "shor": {},
  "computational no": {},
  "logs": {},
  "cost": {},
  "gman": {},
  "np.float32": {},
  "self.gen_": {}
}